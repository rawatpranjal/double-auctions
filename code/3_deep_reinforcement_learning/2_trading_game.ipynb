{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2298e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf1e1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_strategies = ['Honest']\n",
    "seller_strategies = ['Honest','Honest','Honest','Honest','Honest','Honest','Honest','Honest']\n",
    "nbuyers, nsellers = len(buyer_strategies), len(seller_strategies)\n",
    "nrounds, nperiods, ntokens, nsteps, gametype, nbuyers, nsellers = 1, 10000, 10, 10, '1234', len(buyer_strategies), len(seller_strategies)\n",
    "R1, R2, R3, R4 = gametype_to_ran(gametype)\n",
    "game_metadata = [nrounds, nperiods, ntokens, nbuyers, nsellers, nsteps, R1, R2, R3, R4]\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "rnd = 0\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "period = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "304478f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, db):\n",
    "        # Define your environment parameters here\n",
    "        self.db = db\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Discrete(10)\n",
    "        self.state = 0 \n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state and return the initial observation\n",
    "        self.db.reset_period(rnd)\n",
    "        self.state = 0\n",
    "        observation = self.state  # Replace this with your actual observation\n",
    "        reset_info = None  # Replace this with any reset information you want to return\n",
    "        return observation, reset_info\n",
    "\n",
    "    def step(self, action):\n",
    "        # convert action to bid\n",
    "        self.db.buyers[0].next_token()\n",
    "        min_bid = self.db.buyers[0].value*0.1\n",
    "        max_bid = self.db.buyers[0].value*1.9\n",
    "        bid = min_bid * action.item() + (1-action.item())*max_bid\n",
    "        \n",
    "        # simulate market\n",
    "        bids = [bid]\n",
    "        asks = [seller.ask(self.db) for seller in self.db.sellers]\n",
    "        current_ask, current_ask_idx, current_bid, current_bid_idx = current_bid_ask(bids, asks) \n",
    "        sale, price, bprofit, sprofit, buy, sell = buy_sell(db, current_bid, current_bid_idx, current_ask, current_ask_idx)\n",
    "        step_data = [rnd,period,self.state,bids,asks,current_bid,current_bid_idx,current_ask,current_ask_idx,buy,sell,price,sale,bprofit,sprofit]\n",
    "        self.db.add_step(step_data)\n",
    "        \n",
    "        # compute reward, new state\n",
    "        reward = 0\n",
    "        if (sale == 1) and (current_bid_idx == 0):\n",
    "            reward = bprofit\n",
    "        new_state = self.state + 1\n",
    "        \n",
    "        # check termination \n",
    "        if self.state == nsteps-1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = None\n",
    "        self.state = new_state  # Update the current state\n",
    "        reset_info = None\n",
    "        return new_state, reward, done, info, reset_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2655b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 0, State: 1, Action:0.1, Reward: 4.8, New State: 1, Period End: False\n",
      "Rnd: 0, Period: 0, State: 2, Action:0.7, Reward: 50.0, New State: 2, Period End: False\n",
      "Rnd: 0, Period: 0, State: 3, Action:0.6, Reward: 33.7, New State: 3, Period End: False\n",
      "Rnd: 0, Period: 0, State: 4, Action:0.8, Reward: 40.8, New State: 4, Period End: False\n",
      "Rnd: 0, Period: 0, State: 5, Action:0.8, Reward: 19.7, New State: 5, Period End: False\n",
      "Rnd: 0, Period: 0, State: 6, Action:0.4, Reward: 5.2, New State: 6, Period End: False\n",
      "Rnd: 0, Period: 0, State: 7, Action:0.7, Reward: 13.2, New State: 7, Period End: False\n",
      "Rnd: 0, Period: 0, State: 8, Action:0.6, Reward: 7.0, New State: 8, Period End: False\n",
      "Rnd: 0, Period: 0, State: 9, Action:0.1, Reward: -11.2, New State: 9, Period End: False\n",
      "Rnd: 0, Period: 0, State: 10, Action:0.3, Reward: 0, New State: 10, Period End: True\n",
      "done\n",
      "Rnd: 0, Period: 0, State: 1, Action:0.8, Reward: 75.9, New State: 1, Period End: False\n",
      "Rnd: 0, Period: 0, State: 2, Action:0.8, Reward: 60.9, New State: 2, Period End: False\n",
      "Rnd: 0, Period: 0, State: 3, Action:0.2, Reward: 7.1, New State: 3, Period End: False\n",
      "Rnd: 0, Period: 0, State: 4, Action:0.3, Reward: 11.5, New State: 4, Period End: False\n",
      "Rnd: 0, Period: 0, State: 5, Action:0.5, Reward: 8.0, New State: 5, Period End: False\n",
      "Rnd: 0, Period: 0, State: 6, Action:0.9, Reward: 15.5, New State: 6, Period End: False\n",
      "Rnd: 0, Period: 0, State: 7, Action:0.7, Reward: 13.2, New State: 7, Period End: False\n",
      "Rnd: 0, Period: 0, State: 8, Action:0.9, Reward: 7.0, New State: 8, Period End: False\n",
      "Rnd: 0, Period: 0, State: 9, Action:0.3, Reward: -6.0, New State: 9, Period End: False\n",
      "Rnd: 0, Period: 0, State: 10, Action:1.0, Reward: 0, New State: 10, Period End: True\n",
      "done\n",
      "Rnd: 0, Period: 0, State: 1, Action:0.9, Reward: 85.8, New State: 1, Period End: False\n",
      "Rnd: 0, Period: 0, State: 2, Action:1.0, Reward: 72.5, New State: 2, Period End: False\n",
      "Rnd: 0, Period: 0, State: 3, Action:0.5, Reward: 26.9, New State: 3, Period End: False\n",
      "Rnd: 0, Period: 0, State: 4, Action:0.0, Reward: -9.1, New State: 4, Period End: False\n",
      "Rnd: 0, Period: 0, State: 5, Action:0.5, Reward: 8.8, New State: 5, Period End: False\n",
      "Rnd: 0, Period: 0, State: 6, Action:0.9, Reward: 15.5, New State: 6, Period End: False\n",
      "Rnd: 0, Period: 0, State: 7, Action:0.1, Reward: -8.2, New State: 7, Period End: False\n",
      "Rnd: 0, Period: 0, State: 8, Action:0.4, Reward: -1.6, New State: 8, Period End: False\n",
      "Rnd: 0, Period: 0, State: 9, Action:0.8, Reward: 2.9, New State: 9, Period End: False\n",
      "Rnd: 0, Period: 0, State: 10, Action:0.2, Reward: 0, New State: 10, Period End: True\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of your custom environment\n",
    "env = TradingEnv(db)\n",
    "\n",
    "# Reset the environment to its initial state\n",
    "observation = env.reset()\n",
    "\n",
    "# Test some steps in the environment\n",
    "for _ in range(30):  # You can adjust the number of steps\n",
    "    # Replace this with your action selection logic\n",
    "    action = env.action_space.sample()  # Random action for testing\n",
    "\n",
    "    # Take a step in the environment\n",
    "    new_observation, reward, done, info, reset_info = env.step(action)\n",
    "\n",
    "    # Print the current observation, reward, and whether the episode is done\n",
    "    print(f\"Rnd: {rnd}, Period: {period}, State: {env.state}, Action:{np.round(action.item(),1)}, Reward: {np.round(reward,1)}, New State: {new_observation}, Period End: {done}\")\n",
    "\n",
    "    if done:\n",
    "        # If the episode is done, reset the environment\n",
    "        print('done')\n",
    "        observation = env.reset()\n",
    "        pass\n",
    "\n",
    "# Close the environment when done\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44f7ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "import time\n",
    "# Saving logs to visulise in Tensorboard, saving models\n",
    "models_dir = f\"models/Mountain-{time.time()}\"\n",
    "logdir = f\"logs/Mountain-{time.time()}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1)\n",
    "\n",
    "# The learning agent and hyperparameters\n",
    "model = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=256,\n",
    "    ent_coef=0.00429,\n",
    "    learning_rate=7.77e-05,\n",
    "    n_epochs=10,\n",
    "    n_steps=8,\n",
    "    gae_lambda=0.9,\n",
    "    gamma=0.9999,\n",
    "    clip_range=0.1,\n",
    "    max_grad_norm =5,\n",
    "    vf_coef=0.19,\n",
    "    use_sde=True,\n",
    "    policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
