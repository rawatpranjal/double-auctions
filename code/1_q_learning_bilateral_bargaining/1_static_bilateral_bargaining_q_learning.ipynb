{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12219459",
   "metadata": {},
   "source": [
    "### Simple Q-learning buyer making bids to a seller with unknown but fixed reservation price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f61f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Buyer: 1\n",
      "Value Seller: 0.2\n",
      "Bid: 0.0 Reward: 0 Epsilon: 0.99\n",
      "Bid: 0.7 Reward: 0.3 Epsilon: 0.6\n",
      "Bid: 0.35 Reward: 0.65 Epsilon: 0.36\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.22\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.13\n",
      "Bid: 0.15 Reward: 0 Epsilon: 0.08\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.05\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.03\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.02\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "value_buyer = 1\n",
    "value_seller = 0.2\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 5000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.999  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Q-table initialization\n",
    "q_table = np.zeros((num_actions,))\n",
    "bid2action = np.linspace(0, 1, num_actions)  # Mapping of action index to bid value\n",
    "\n",
    "print('Value Buyer:', value_buyer)\n",
    "print('Value Seller:', value_seller)\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, num_actions)  # Exploration: Random action\n",
    "    else:\n",
    "        action = np.argmax(q_table)  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = bid2action[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = 0  # No reward if bid is below seller's value\n",
    "\n",
    "    # Q-value update using Q-learning equation\n",
    "    q_table[action] += alpha * (reward + gamma * np.max(q_table) - q_table[action])\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print relevant information for each episode\n",
    "    if episode % 500 == 0:\n",
    "        print(\"Bid:\", round(bid, 2), \"Reward:\", round(reward, 2), \"Epsilon:\", round(epsilon, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dabd4372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.8        0.74718217\n",
      " 0.69161924 0.63682089 0.59999991 0.54341512 0.49910149 0.44563602\n",
      " 0.39408765 0.34534402 0.29860848 0.24871156 0.19858607 0.1490456\n",
      " 0.09903023 0.04964652 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1da5f",
   "metadata": {},
   "source": [
    "### Q-learning against seller with unknown and random reservation price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce689b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Buyer: 1\n",
      "Avg Value Seller: 0.2\n",
      "Bid: 0.9 Seller value: 0.13 0.9 Reward: 0.1 Epsilon: 0.99\n",
      "Bid: 0.35 Seller value: 0.19 0.35 Reward: 0.65 Epsilon: 0.08\n",
      "Bid: 0.35 Seller value: 0.17 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.23 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.12 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.16 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.16 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.2 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.18 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.14 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.21 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.18 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.26 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.21 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.22 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.26 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.22 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.18 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.25 0.35 Reward: 0.65 Epsilon: 0.01\n",
      "Bid: 0.35 Seller value: 0.18 0.35 Reward: 0.65 Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "value_buyer = 1\n",
    "avg_value_seller = 0.2\n",
    "std_value_seller = 0.05\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 10000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.995  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Q-table initialization\n",
    "q_table = np.zeros((num_actions,))\n",
    "bid2action = np.linspace(0, 1, num_actions)  # Mapping of action index to bid value\n",
    "\n",
    "print('Value Buyer:', value_buyer)\n",
    "print('Avg Value Seller:', avg_value_seller)\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Seller realizes value\n",
    "    value_seller = np.random.normal(avg_value_seller,std_value_seller,1)[0]\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, num_actions)  # Exploration: Random action\n",
    "    else:\n",
    "        action = np.argmax(q_table)  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = bid2action[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = 0  # No reward if bid is below seller's value\n",
    "\n",
    "    # Q-value update using Q-learning equation\n",
    "    q_table[action] += alpha * (reward + gamma * np.max(q_table) - q_table[action])\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print relevant information for each episode\n",
    "    if episode % 500 == 0:\n",
    "        print(\"Bid:\",round(bid,2), \"Seller value:\",round(value_seller,2),round(bid, 2), \"Reward:\", round(reward, 2), \"Epsilon:\", round(epsilon, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc15bb",
   "metadata": {},
   "source": [
    "### Q-learning when buyer value changes but is known, but seller value is unknown and random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "835e455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seller value: 0.6 Buyer value: 0.92 Bid: 0.65 Reward: 0.28 Epsilon: 0.99\n",
      "Seller value: 0.59 Buyer value: 0.62 Bid: 0.15 Reward: 0 Epsilon: 0.81\n",
      "Seller value: 0.75 Buyer value: 0.88 Bid: 0.3 Reward: 0 Epsilon: 0.66\n",
      "Seller value: 0.69 Buyer value: 0.68 Bid: 0.65 Reward: 0 Epsilon: 0.54\n",
      "Seller value: 0.64 Buyer value: 0.62 Bid: 0.6 Reward: 0 Epsilon: 0.44\n",
      "Seller value: 0.67 Buyer value: 1.0 Bid: 0.75 Reward: 0.25 Epsilon: 0.36\n",
      "Seller value: 0.72 Buyer value: 0.52 Bid: 0.0 Reward: 0 Epsilon: 0.3\n",
      "Seller value: 0.65 Buyer value: 0.82 Bid: 0.9 Reward: -0.08 Epsilon: 0.24\n",
      "Seller value: 0.62 Buyer value: 0.92 Bid: 0.75 Reward: 0.18 Epsilon: 0.2\n",
      "Seller value: 0.65 Buyer value: 0.95 Bid: 0.75 Reward: 0.2 Epsilon: 0.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "avg_value_seller = 0.7\n",
    "std_value_seller = 0.05\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 200000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.99999  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Divide the buyer value range into 11 divisions\n",
    "buyer_value_divisions = np.linspace(0.5, 1.0, num_actions)\n",
    "\n",
    "# Q-table initialization\n",
    "num_states = num_actions\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "action2bid = np.linspace(0, 1, num_actions)  # Mapping of action index to bid value\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Seller realizes value\n",
    "    value_seller = np.random.normal(avg_value_seller, std_value_seller, 1)[0]\n",
    "    \n",
    "    # Buyer realizes value \n",
    "    value_buyer = np.random.choice(buyer_value_divisions)\n",
    "    \n",
    "    # Find the index of the buyer value division\n",
    "    state = np.argmin(np.abs(buyer_value_divisions - value_buyer))\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, num_actions)  # Exploration: Random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = action2bid[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = 0  # No reward if bid is below seller's value\n",
    "\n",
    "    # Q-value update using Q-learning equation\n",
    "    q_table[state][action] += alpha * (reward + gamma * np.max(q_table[state]) - q_table[state][action])\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print relevant information for each episode\n",
    "    if episode % 20000 == 0:\n",
    "        print(\"Seller value:\", round(value_seller, 2), \"Buyer value:\", round(value_buyer, 2), \"Bid:\", round(bid, 2), \"Reward:\", round(reward, 2), \"Epsilon:\", round(epsilon, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "44225a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5   0.525 0.55  0.575 0.6   0.625 0.65  0.675 0.7   0.725 0.75  0.775\n",
      " 0.8   0.825 0.85  0.875 0.9   0.925 0.95  0.975 1.   ]\n",
      "[0.   0.   0.   0.   0.   0.6  0.6  0.65 0.65 0.7  0.7  0.7  0.75 0.75\n",
      " 0.75 0.75 0.75 0.75 0.8  0.8  0.75]\n"
     ]
    }
   ],
   "source": [
    "print(buyer_value_divisions)\n",
    "print(action2bid[np.argmax(q_table, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f79981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
