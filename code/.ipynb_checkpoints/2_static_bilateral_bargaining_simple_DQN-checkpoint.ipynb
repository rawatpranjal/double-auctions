{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12219459",
   "metadata": {},
   "source": [
    "### Simple Q-learning buyer making bids to a seller with unknown but fixed reservation price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f61f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Buyer: 1\n",
      "Value Seller: 0.2\n",
      "Bid: 0.95 Reward: 0.05 Epsilon: 0.99\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.6\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.36\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.22\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.13\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.08\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.05\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.03\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.02\n",
      "Bid: 0.2 Reward: 0.8 Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "value_buyer = 1\n",
    "value_seller = 0.2\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 5000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.999  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Q-table initialization\n",
    "q_table = np.zeros((num_actions,))\n",
    "bid2action = np.linspace(0, 1, num_actions)  # Mapping of action index to bid value\n",
    "\n",
    "print('Value Buyer:', value_buyer)\n",
    "print('Value Seller:', value_seller)\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, num_actions)  # Exploration: Random action\n",
    "    else:\n",
    "        action = np.argmax(q_table)  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = bid2action[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = 0  # No reward if bid is below seller's value\n",
    "\n",
    "    # Q-value update using Q-learning equation\n",
    "    q_table[action] += alpha * (reward + gamma * np.max(q_table) - q_table[action])\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print relevant information for each episode\n",
    "    if episode % 500 == 0:\n",
    "        print(\"Bid:\", round(bid, 2), \"Reward:\", round(reward, 2), \"Epsilon:\", round(epsilon, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe95e7d",
   "metadata": {},
   "source": [
    "### Q-learning Network to approximate Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba26812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Buyer: 1\n",
      "Value Seller: 0.2\n",
      "Episode: 0 Epsilon: 0.98 Bid: 0.3 Reward: 0.7\n",
      "Episode: 500 Epsilon: 0.01 Bid: 0.2 Reward: 0.8\n",
      "Episode: 1000 Epsilon: 0.01 Bid: 0.2 Reward: 0.8\n",
      "Episode: 1500 Epsilon: 0.01 Bid: 0.2 Reward: 0.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameters\n",
    "value_buyer = 1\n",
    "value_seller = 0.2\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 2000\n",
    "batch_size = 32\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.99  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "\n",
    "# Q-network definition\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(num_actions, num_actions)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "q_network = QNetwork()\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=alpha)\n",
    "\n",
    "# Bid to action mapping\n",
    "bid2action = torch.linspace(0, 1, num_actions)\n",
    "\n",
    "print('Value Buyer:', value_buyer)\n",
    "print('Value Seller:', value_seller)\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    state = torch.zeros(num_actions)\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, num_actions-1)  # Exploration: Random action\n",
    "    else:\n",
    "        q_values = q_network(state)\n",
    "        action = torch.argmax(q_values).item()  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = bid2action[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = bid*0  # No reward if bid is below seller's value\n",
    "\n",
    "    # Q-value update using Q-learning equation\n",
    "    q_values = q_network(state)\n",
    "    next_q_value = torch.max(q_values)\n",
    "    target_q = reward + gamma * next_q_value\n",
    "    loss = nn.MSELoss()(q_values[action], target_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print relevant information for each episode\n",
    "    if episode % 500 == 0:\n",
    "        print(\"Episode:\", episode, \"Epsilon:\", round(epsilon, 2), \"Bid:\", round(bid.item(), 2), \"Reward:\", round(reward.item(), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32459d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-8.3231e-02,  5.5671e-04, -7.2437e-01, -2.9054e-01,  8.0000e-01,\n",
      "         7.4481e-01,  5.7152e-01, -3.7087e-01,  6.8265e-01,  6.0429e-01,\n",
      "         4.9629e-01,  5.1394e-01,  1.4285e-01,  5.9004e-01, -1.5994e-02,\n",
      "        -5.7252e-01, -9.1316e-01, -3.4263e-01, -3.4104e-01, -3.6717e-01,\n",
      "         3.3541e-01], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(q_network(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033396b",
   "metadata": {},
   "source": [
    "### Q-Network with Experience Replay to stabilize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Buyer: 1\n",
      "Value Seller: 0.2\n",
      "Episode: 0 Epsilon: 0.99 Bid: 0.95 Reward: 0.05\n",
      "Episode: 200 Epsilon: 0.81 Bid: 0.9 Reward: 0.35\n",
      "Episode: 400 Epsilon: 0.66 Bid: 0.2 Reward: 0.0\n",
      "Episode: 600 Epsilon: 0.54 Bid: 0.45 Reward: 0.1\n",
      "Episode: 800 Epsilon: 0.44 Bid: 0.2 Reward: 0.8\n",
      "Episode: 1000 Epsilon: 0.36 Bid: 0.25 Reward: 0.6\n",
      "Episode: 1200 Epsilon: 0.3 Bid: 0.2 Reward: 0.55\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 21\n",
    "num_episodes = 2000\n",
    "batch_size = 32\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0  # No discount for immediate rewards\n",
    "initial_epsilon = 0.99\n",
    "epsilon_decay = 0.999  # Decay factor for epsilon\n",
    "min_epsilon = 0.01\n",
    "memory_capacity = 1000\n",
    "\n",
    "# Q-network definition\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(num_actions, num_actions)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "q_network = QNetwork()\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=alpha)\n",
    "\n",
    "# Bid to action mapping\n",
    "bid2action = torch.linspace(0, 1, num_actions)\n",
    "\n",
    "print('Value Buyer:', value_buyer)\n",
    "print('Value Seller:', value_seller)\n",
    "\n",
    "# Experience replay buffer\n",
    "memory = deque(maxlen=memory_capacity)\n",
    "\n",
    "# Training loop\n",
    "epsilon = initial_epsilon\n",
    "for episode in range(num_episodes):\n",
    "    state = torch.zeros(num_actions)\n",
    "    \n",
    "    # Select action using epsilon-greedy strategy\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, num_actions-1)  # Exploration: Random action\n",
    "    else:\n",
    "        q_values = q_network(state)\n",
    "        action = torch.argmax(q_values).item()  # Exploitation: Choose best action based on Q-values\n",
    "\n",
    "    bid = bid2action[action]  # Convert action index to bid value\n",
    "    \n",
    "    if bid >= value_seller:\n",
    "        reward = value_buyer - bid  # Calculate reward based on bid and buyer's value\n",
    "    else:\n",
    "        reward = bid*0  # No reward if bid is below seller's value\n",
    "    \n",
    "    # Store experience in memory\n",
    "    memory.append((state, action, reward))\n",
    "\n",
    "    # Sample a batch from memory for training\n",
    "    if len(memory) >= batch_size:\n",
    "        batch = random.sample(memory, batch_size)\n",
    "        for state, action, reward in batch:\n",
    "            q_values = q_network(state)\n",
    "            next_q_value = torch.max(q_network(state))\n",
    "            target_q = reward + gamma * next_q_value\n",
    "            loss = nn.MSELoss()(q_values[action], target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "          \n",
    "    if episode % 200 == 0:\n",
    "        print(\"Episode:\", episode, \"Epsilon:\", round(epsilon, 2), \"Bid:\", round(bid.item(), 2), \"Reward:\", round(reward.item(), 2))\n",
    "    \n",
    "# Print the optimal action given state at the end of training\n",
    "optimal_action = torch.argmax(q_network(state)).item()\n",
    "optimal_bid = bid2action[optimal_action]\n",
    "print(\"Optimal Action:\", optimal_action, \"Optimal Bid:\", optimal_bid.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimal action given state at the end of training\n",
    "optimal_action = torch.argmax(q_network(state)).item()\n",
    "optimal_bid = bid2action[optimal_action]\n",
    "print(\"Optimal Action:\", optimal_action, \"Optimal Bid:\", optimal_bid.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
