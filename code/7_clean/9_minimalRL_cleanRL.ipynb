{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32155ed6-0a96-4d56-a031-20527a80d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/econ_share/home/pp712/myenv/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_32245/2811573924.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : -1422.5 alpha:0.0075\n",
      "# of episode :40, avg score : -1609.2 alpha:0.0057\n",
      "# of episode :60, avg score : -1366.3 alpha:0.0060\n",
      "# of episode :80, avg score : -1144.0 alpha:0.0078\n",
      "# of episode :100, avg score : -798.3 alpha:0.0103\n",
      "# of episode :120, avg score : -787.0 alpha:0.0116\n",
      "# of episode :140, avg score : -692.5 alpha:0.0118\n",
      "# of episode :160, avg score : -614.8 alpha:0.0133\n",
      "# of episode :180, avg score : -775.4 alpha:0.0214\n",
      "# of episode :200, avg score : -467.4 alpha:0.0286\n",
      "# of episode :220, avg score : -268.0 alpha:0.0329\n",
      "# of episode :240, avg score : -126.3 alpha:0.0368\n",
      "# of episode :260, avg score : -176.9 alpha:0.0361\n",
      "# of episode :280, avg score : -181.5 alpha:0.0359\n",
      "# of episode :300, avg score : -178.9 alpha:0.0375\n",
      "# of episode :320, avg score : -150.0 alpha:0.0396\n",
      "# of episode :340, avg score : -159.9 alpha:0.0387\n",
      "# of episode :360, avg score : -152.7 alpha:0.0368\n",
      "# of episode :380, avg score : -134.0 alpha:0.0313\n",
      "# of episode :400, avg score : -125.2 alpha:0.0267\n",
      "# of episode :420, avg score : -165.4 alpha:0.0233\n",
      "# of episode :440, avg score : -163.4 alpha:0.0188\n",
      "# of episode :460, avg score : -144.8 alpha:0.0146\n",
      "# of episode :480, avg score : -187.5 alpha:0.0113\n",
      "# of episode :500, avg score : -148.9 alpha:0.0093\n",
      "# of episode :520, avg score : -190.7 alpha:0.0079\n",
      "# of episode :540, avg score : -146.1 alpha:0.0064\n",
      "# of episode :560, avg score : -152.6 alpha:0.0054\n",
      "# of episode :580, avg score : -193.4 alpha:0.0047\n",
      "# of episode :600, avg score : -181.0 alpha:0.0043\n",
      "# of episode :620, avg score : -145.6 alpha:0.0038\n",
      "# of episode :640, avg score : -163.0 alpha:0.0033\n",
      "# of episode :660, avg score : -176.8 alpha:0.0032\n",
      "# of episode :680, avg score : -165.1 alpha:0.0030\n",
      "# of episode :700, avg score : -160.6 alpha:0.0030\n",
      "# of episode :720, avg score : -159.3 alpha:0.0036\n",
      "# of episode :740, avg score : -144.6 alpha:0.0036\n",
      "# of episode :760, avg score : -142.6 alpha:0.0035\n",
      "# of episode :780, avg score : -174.4 alpha:0.0030\n",
      "# of episode :800, avg score : -132.8 alpha:0.0026\n",
      "# of episode :820, avg score : -143.7 alpha:0.0023\n",
      "# of episode :840, avg score : -164.4 alpha:0.0033\n",
      "# of episode :860, avg score : -131.1 alpha:0.0029\n",
      "# of episode :880, avg score : -177.9 alpha:0.0025\n",
      "# of episode :900, avg score : -161.2 alpha:0.0021\n",
      "# of episode :920, avg score : -172.4 alpha:0.0018\n",
      "# of episode :940, avg score : -154.8 alpha:0.0020\n",
      "# of episode :960, avg score : -162.4 alpha:0.0018\n",
      "# of episode :980, avg score : -150.1 alpha:0.0015\n",
      "# of episode :1000, avg score : -150.2 alpha:0.0014\n",
      "# of episode :1020, avg score : -172.4 alpha:0.0015\n",
      "# of episode :1040, avg score : -153.9 alpha:0.0016\n",
      "# of episode :1060, avg score : -136.5 alpha:0.0024\n",
      "# of episode :1080, avg score : -148.0 alpha:0.0031\n",
      "# of episode :1100, avg score : -143.2 alpha:0.0040\n",
      "# of episode :1120, avg score : -191.4 alpha:0.0035\n",
      "# of episode :1140, avg score : -187.5 alpha:0.0033\n",
      "# of episode :1160, avg score : -197.3 alpha:0.0024\n",
      "# of episode :1180, avg score : -146.0 alpha:0.0019\n",
      "# of episode :1200, avg score : -184.9 alpha:0.0017\n",
      "# of episode :1220, avg score : -202.8 alpha:0.0015\n",
      "# of episode :1240, avg score : -177.5 alpha:0.0022\n",
      "# of episode :1260, avg score : -166.7 alpha:0.0026\n",
      "# of episode :1280, avg score : -140.2 alpha:0.0029\n",
      "# of episode :1300, avg score : -145.5 alpha:0.0027\n",
      "# of episode :1320, avg score : -125.7 alpha:0.0021\n",
      "# of episode :1340, avg score : -163.0 alpha:0.0016\n",
      "# of episode :1360, avg score : -186.8 alpha:0.0011\n",
      "# of episode :1380, avg score : -164.1 alpha:0.0009\n",
      "# of episode :1400, avg score : -143.9 alpha:0.0007\n",
      "# of episode :1420, avg score : -154.6 alpha:0.0006\n",
      "# of episode :1440, avg score : -173.7 alpha:0.0005\n",
      "# of episode :1460, avg score : -187.3 alpha:0.0004\n",
      "# of episode :1480, avg score : -123.0 alpha:0.0005\n",
      "# of episode :1500, avg score : -152.9 alpha:0.0008\n",
      "# of episode :1520, avg score : -170.6 alpha:0.0015\n",
      "# of episode :1540, avg score : -148.1 alpha:0.0023\n",
      "# of episode :1560, avg score : -138.6 alpha:0.0037\n",
      "# of episode :1580, avg score : -144.0 alpha:0.0059\n",
      "# of episode :1600, avg score : -220.5 alpha:0.0064\n",
      "# of episode :1620, avg score : -158.1 alpha:0.0085\n",
      "# of episode :1640, avg score : -178.3 alpha:0.0080\n",
      "# of episode :1660, avg score : -201.9 alpha:0.0049\n",
      "# of episode :1680, avg score : -117.9 alpha:0.0037\n",
      "# of episode :1700, avg score : -174.7 alpha:0.0031\n",
      "# of episode :1720, avg score : -126.8 alpha:0.0025\n",
      "# of episode :1740, avg score : -159.3 alpha:0.0021\n",
      "# of episode :1760, avg score : -190.8 alpha:0.0018\n",
      "# of episode :1780, avg score : -128.4 alpha:0.0018\n",
      "# of episode :1800, avg score : -163.5 alpha:0.0016\n",
      "# of episode :1820, avg score : -158.3 alpha:0.0013\n",
      "# of episode :1840, avg score : -128.4 alpha:0.0010\n",
      "# of episode :1860, avg score : -197.0 alpha:0.0011\n",
      "# of episode :1880, avg score : -141.4 alpha:0.0010\n",
      "# of episode :1900, avg score : -152.8 alpha:0.0015\n",
      "# of episode :1920, avg score : -152.9 alpha:0.0017\n",
      "# of episode :1940, avg score : -168.7 alpha:0.0023\n",
      "# of episode :1960, avg score : -145.9 alpha:0.0029\n",
      "# of episode :1980, avg score : -146.1 alpha:0.0029\n",
      "# of episode :2000, avg score : -136.0 alpha:0.0030\n",
      "# of episode :2020, avg score : -170.5 alpha:0.0022\n",
      "# of episode :2040, avg score : -169.5 alpha:0.0017\n",
      "# of episode :2060, avg score : -147.6 alpha:0.0014\n",
      "# of episode :2080, avg score : -170.4 alpha:0.0013\n",
      "# of episode :2100, avg score : -136.0 alpha:0.0013\n",
      "# of episode :2120, avg score : -148.3 alpha:0.0010\n",
      "# of episode :2140, avg score : -160.7 alpha:0.0009\n",
      "# of episode :2160, avg score : -153.4 alpha:0.0010\n",
      "# of episode :2180, avg score : -156.3 alpha:0.0011\n",
      "# of episode :2200, avg score : -114.6 alpha:0.0011\n",
      "# of episode :2220, avg score : -146.0 alpha:0.0016\n",
      "# of episode :2240, avg score : -127.5 alpha:0.0014\n",
      "# of episode :2260, avg score : -167.4 alpha:0.0011\n",
      "# of episode :2280, avg score : -149.8 alpha:0.0009\n",
      "# of episode :2300, avg score : -167.1 alpha:0.0008\n",
      "# of episode :2320, avg score : -149.5 alpha:0.0007\n",
      "# of episode :2340, avg score : -162.8 alpha:0.0007\n",
      "# of episode :2360, avg score : -161.2 alpha:0.0007\n",
      "# of episode :2380, avg score : -140.6 alpha:0.0008\n",
      "# of episode :2400, avg score : -132.6 alpha:0.0013\n",
      "# of episode :2420, avg score : -135.5 alpha:0.0015\n",
      "# of episode :2440, avg score : -139.9 alpha:0.0021\n",
      "# of episode :2460, avg score : -159.7 alpha:0.0023\n",
      "# of episode :2480, avg score : -174.7 alpha:0.0018\n",
      "# of episode :2500, avg score : -145.2 alpha:0.0016\n",
      "# of episode :2520, avg score : -152.5 alpha:0.0023\n",
      "# of episode :2540, avg score : -145.6 alpha:0.0023\n",
      "# of episode :2560, avg score : -124.0 alpha:0.0019\n",
      "# of episode :2580, avg score : -181.9 alpha:0.0017\n",
      "# of episode :2600, avg score : -181.0 alpha:0.0015\n",
      "# of episode :2620, avg score : -171.6 alpha:0.0015\n",
      "# of episode :2640, avg score : -152.3 alpha:0.0014\n",
      "# of episode :2660, avg score : -151.2 alpha:0.0012\n",
      "# of episode :2680, avg score : -163.9 alpha:0.0011\n",
      "# of episode :2700, avg score : -154.7 alpha:0.0011\n",
      "# of episode :2720, avg score : -195.9 alpha:0.0010\n",
      "# of episode :2740, avg score : -128.2 alpha:0.0009\n",
      "# of episode :2760, avg score : -135.7 alpha:0.0008\n",
      "# of episode :2780, avg score : -155.9 alpha:0.0011\n",
      "# of episode :2800, avg score : -157.4 alpha:0.0012\n",
      "# of episode :2820, avg score : -133.3 alpha:0.0010\n",
      "# of episode :2840, avg score : -153.7 alpha:0.0008\n",
      "# of episode :2860, avg score : -161.9 alpha:0.0007\n",
      "# of episode :2880, avg score : -124.0 alpha:0.0006\n",
      "# of episode :2900, avg score : -120.7 alpha:0.0005\n",
      "# of episode :2920, avg score : -156.5 alpha:0.0005\n",
      "# of episode :2940, avg score : -144.0 alpha:0.0005\n",
      "# of episode :2960, avg score : -163.2 alpha:0.0005\n",
      "# of episode :2980, avg score : -130.5 alpha:0.0005\n",
      "# of episode :3000, avg score : -162.6 alpha:0.0005\n",
      "# of episode :3020, avg score : -138.7 alpha:0.0006\n",
      "# of episode :3040, avg score : -157.1 alpha:0.0007\n",
      "# of episode :3060, avg score : -121.4 alpha:0.0006\n",
      "# of episode :3080, avg score : -129.9 alpha:0.0010\n",
      "# of episode :3100, avg score : -153.2 alpha:0.0014\n",
      "# of episode :3120, avg score : -144.3 alpha:0.0014\n",
      "# of episode :3140, avg score : -156.6 alpha:0.0014\n",
      "# of episode :3160, avg score : -156.5 alpha:0.0015\n",
      "# of episode :3180, avg score : -135.7 alpha:0.0016\n",
      "# of episode :3200, avg score : -163.9 alpha:0.0014\n",
      "# of episode :3220, avg score : -149.6 alpha:0.0015\n",
      "# of episode :3240, avg score : -142.2 alpha:0.0014\n",
      "# of episode :3260, avg score : -200.2 alpha:0.0015\n",
      "# of episode :3280, avg score : -115.5 alpha:0.0017\n",
      "# of episode :3300, avg score : -126.7 alpha:0.0020\n",
      "# of episode :3320, avg score : -137.7 alpha:0.0018\n",
      "# of episode :3340, avg score : -142.8 alpha:0.0015\n",
      "# of episode :3360, avg score : -164.0 alpha:0.0014\n",
      "# of episode :3380, avg score : -107.6 alpha:0.0012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 173\u001b[0m\n\u001b[1;32m    170\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 159\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m    158\u001b[0m     mini_batch \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m--> 159\u001b[0m     td_target \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq1_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     q1\u001b[38;5;241m.\u001b[39mtrain_net(td_target, mini_batch)\n\u001b[1;32m    161\u001b[0m     q2\u001b[38;5;241m.\u001b[39mtrain_net(td_target, mini_batch)\n",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m, in \u001b[0;36mcalc_target\u001b[0;34m(pi, q1, q2, mini_batch)\u001b[0m\n\u001b[1;32m    123\u001b[0m entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mpi\u001b[38;5;241m.\u001b[39mlog_alpha\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m*\u001b[39m log_prob\n\u001b[1;32m    124\u001b[0m q1_val, q2_val \u001b[38;5;241m=\u001b[39m q1(s_prime,a_prime), q2(s_prime,a_prime)\n\u001b[0;32m--> 125\u001b[0m q1_q2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mq1_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m min_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1_q2, \u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    127\u001b[0m target \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m done \u001b[38;5;241m*\u001b[39m (min_q \u001b[38;5;241m+\u001b[39m entropy)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import collections, random\n",
    "\n",
    "#Hyperparameters\n",
    "lr_pi           = 0.0005\n",
    "lr_q            = 0.001\n",
    "init_alpha      = 0.01\n",
    "gamma           = 0.98\n",
    "batch_size      = 32\n",
    "buffer_limit    = 50000\n",
    "tau             = 0.01 # for target network soft update\n",
    "target_entropy  = -1.0 # for automated alpha update\n",
    "lr_alpha        = 0.001  # for automated alpha update\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.float), \\\n",
    "                torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                torch.tensor(done_mask_lst, dtype=torch.float)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=lr_alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.rsample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_net(self, q1, q2, mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        loss = -min_q - entropy # for gradient ascent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc_s = nn.Linear(3, 64)\n",
    "        self.fc_a = nn.Linear(1,64)\n",
    "        self.fc_cat = nn.Linear(128,32)\n",
    "        self.fc_out = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        h1 = F.relu(self.fc_s(x))\n",
    "        h2 = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([h1,h2], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q = self.fc_out(q)\n",
    "        return q\n",
    "\n",
    "    def train_net(self, target, mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s, a) , target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def calc_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy)\n",
    "\n",
    "    return target\n",
    "    \n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    memory = ReplayBuffer()\n",
    "    q1, q2, q1_target, q2_target = QNet(lr_q), QNet(lr_q), QNet(lr_q), QNet(lr_q)\n",
    "    pi = PolicyNet(lr_pi)\n",
    "\n",
    "    q1_target.load_state_dict(q1.state_dict())\n",
    "    q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "        while count < 200 and not done:\n",
    "            a, log_prob= pi(torch.from_numpy(s).float())\n",
    "            s_prime, r, done, truncated, info = env.step([2.0*a.item()])\n",
    "            memory.put((s, a.item(), r/10.0, s_prime, done))\n",
    "            score +=r\n",
    "            s = s_prime\n",
    "            count += 1\n",
    "                \n",
    "        if memory.size()>1000:\n",
    "            for i in range(20):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                td_target = calc_target(pi, q1_target, q2_target, mini_batch)\n",
    "                q1.train_net(td_target, mini_batch)\n",
    "                q2.train_net(td_target, mini_batch)\n",
    "                entropy = pi.train_net(q1, q2, mini_batch)\n",
    "                q1.soft_update(q1_target)\n",
    "                q2.soft_update(q2_target)\n",
    "                \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f} alpha:{:.4f}\".format(n_epi, score/print_interval, pi.log_alpha.exp()))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89acabc-f9d5-4da5-ab2e-32b658556054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : -1589.3, optmization step: 400\n",
      "# of episode :40, avg score : -1425.8, optmization step: 800\n",
      "# of episode :60, avg score : -1256.5, optmization step: 1200\n",
      "# of episode :80, avg score : -1240.3, optmization step: 1600\n",
      "# of episode :100, avg score : -1131.2, optmization step: 2100\n",
      "# of episode :120, avg score : -1139.1, optmization step: 2500\n",
      "# of episode :140, avg score : -1074.8, optmization step: 2900\n",
      "# of episode :160, avg score : -1037.2, optmization step: 3300\n",
      "# of episode :180, avg score : -1189.4, optmization step: 3700\n",
      "# of episode :200, avg score : -1228.3, optmization step: 4200\n",
      "# of episode :220, avg score : -1103.8, optmization step: 4600\n",
      "# of episode :240, avg score : -1219.9, optmization step: 5000\n",
      "# of episode :260, avg score : -1117.3, optmization step: 5400\n",
      "# of episode :280, avg score : -1012.5, optmization step: 5800\n",
      "# of episode :300, avg score : -1032.9, optmization step: 6300\n",
      "# of episode :320, avg score : -1016.9, optmization step: 6700\n",
      "# of episode :340, avg score : -1052.0, optmization step: 7100\n",
      "# of episode :360, avg score : -982.3, optmization step: 7500\n",
      "# of episode :380, avg score : -934.4, optmization step: 7900\n",
      "# of episode :400, avg score : -857.3, optmization step: 8300\n",
      "# of episode :420, avg score : -807.5, optmization step: 8800\n",
      "# of episode :440, avg score : -772.9, optmization step: 9200\n",
      "# of episode :460, avg score : -781.8, optmization step: 9600\n",
      "# of episode :480, avg score : -734.8, optmization step: 10000\n",
      "# of episode :500, avg score : -726.1, optmization step: 10400\n",
      "# of episode :520, avg score : -705.3, optmization step: 10900\n",
      "# of episode :540, avg score : -682.9, optmization step: 11300\n",
      "# of episode :560, avg score : -699.9, optmization step: 11700\n",
      "# of episode :580, avg score : -672.0, optmization step: 12100\n",
      "# of episode :600, avg score : -665.2, optmization step: 12500\n",
      "# of episode :620, avg score : -739.6, optmization step: 13000\n",
      "# of episode :640, avg score : -694.0, optmization step: 13400\n",
      "# of episode :660, avg score : -657.7, optmization step: 13800\n",
      "# of episode :680, avg score : -689.0, optmization step: 14200\n",
      "# of episode :700, avg score : -693.6, optmization step: 14600\n",
      "# of episode :720, avg score : -736.7, optmization step: 15000\n",
      "# of episode :740, avg score : -712.0, optmization step: 15500\n",
      "# of episode :760, avg score : -689.0, optmization step: 15900\n",
      "# of episode :780, avg score : -751.5, optmization step: 16300\n",
      "# of episode :800, avg score : -710.6, optmization step: 16700\n",
      "# of episode :820, avg score : -749.4, optmization step: 17100\n",
      "# of episode :840, avg score : -696.8, optmization step: 17600\n",
      "# of episode :860, avg score : -721.9, optmization step: 18000\n",
      "# of episode :880, avg score : -758.7, optmization step: 18400\n",
      "# of episode :900, avg score : -661.7, optmization step: 18800\n",
      "# of episode :920, avg score : -667.1, optmization step: 19200\n",
      "# of episode :940, avg score : -718.5, optmization step: 19700\n",
      "# of episode :960, avg score : -737.5, optmization step: 20100\n",
      "# of episode :980, avg score : -762.4, optmization step: 20500\n",
      "# of episode :1000, avg score : -731.0, optmization step: 20900\n",
      "# of episode :1020, avg score : -744.2, optmization step: 21300\n",
      "# of episode :1040, avg score : -707.6, optmization step: 21700\n",
      "# of episode :1060, avg score : -704.0, optmization step: 22200\n",
      "# of episode :1080, avg score : -668.3, optmization step: 22600\n",
      "# of episode :1100, avg score : -634.2, optmization step: 23000\n",
      "# of episode :1120, avg score : -645.9, optmization step: 23400\n",
      "# of episode :1140, avg score : -683.5, optmization step: 23800\n",
      "# of episode :1160, avg score : -714.7, optmization step: 24300\n",
      "# of episode :1180, avg score : -711.0, optmization step: 24700\n",
      "# of episode :1200, avg score : -642.7, optmization step: 25100\n",
      "# of episode :1220, avg score : -673.1, optmization step: 25500\n",
      "# of episode :1240, avg score : -626.7, optmization step: 25900\n",
      "# of episode :1260, avg score : -672.2, optmization step: 26400\n",
      "# of episode :1280, avg score : -730.6, optmization step: 26800\n",
      "# of episode :1300, avg score : -636.0, optmization step: 27200\n",
      "# of episode :1320, avg score : -655.6, optmization step: 27600\n",
      "# of episode :1340, avg score : -626.0, optmization step: 28000\n",
      "# of episode :1360, avg score : -685.2, optmization step: 28400\n",
      "# of episode :1380, avg score : -677.6, optmization step: 28900\n",
      "# of episode :1400, avg score : -652.1, optmization step: 29300\n",
      "# of episode :1420, avg score : -628.3, optmization step: 29700\n",
      "# of episode :1440, avg score : -703.7, optmization step: 30100\n",
      "# of episode :1460, avg score : -639.3, optmization step: 30500\n",
      "# of episode :1480, avg score : -715.9, optmization step: 31000\n",
      "# of episode :1500, avg score : -824.7, optmization step: 31400\n",
      "# of episode :1520, avg score : -661.8, optmization step: 31800\n",
      "# of episode :1540, avg score : -755.0, optmization step: 32200\n",
      "# of episode :1560, avg score : -742.9, optmization step: 32600\n",
      "# of episode :1580, avg score : -709.5, optmization step: 33100\n",
      "# of episode :1600, avg score : -699.5, optmization step: 33500\n",
      "# of episode :1620, avg score : -659.8, optmization step: 33900\n",
      "# of episode :1640, avg score : -653.8, optmization step: 34300\n",
      "# of episode :1660, avg score : -642.9, optmization step: 34700\n",
      "# of episode :1680, avg score : -666.4, optmization step: 35100\n",
      "# of episode :1700, avg score : -663.6, optmization step: 35600\n",
      "# of episode :1720, avg score : -700.7, optmization step: 36000\n",
      "# of episode :1740, avg score : -604.8, optmization step: 36400\n",
      "# of episode :1760, avg score : -642.5, optmization step: 36800\n",
      "# of episode :1780, avg score : -638.0, optmization step: 37200\n",
      "# of episode :1800, avg score : -627.1, optmization step: 37700\n",
      "# of episode :1820, avg score : -618.4, optmization step: 38100\n",
      "# of episode :1840, avg score : -656.4, optmization step: 38500\n",
      "# of episode :1860, avg score : -676.4, optmization step: 38900\n",
      "# of episode :1880, avg score : -635.4, optmization step: 39300\n",
      "# of episode :1900, avg score : -628.7, optmization step: 39800\n",
      "# of episode :1920, avg score : -609.8, optmization step: 40200\n",
      "# of episode :1940, avg score : -624.4, optmization step: 40600\n",
      "# of episode :1960, avg score : -635.9, optmization step: 41000\n",
      "# of episode :1980, avg score : -659.3, optmization step: 41400\n",
      "# of episode :2000, avg score : -616.9, optmization step: 41800\n",
      "# of episode :2020, avg score : -622.8, optmization step: 42300\n",
      "# of episode :2040, avg score : -659.1, optmization step: 42700\n",
      "# of episode :2060, avg score : -645.5, optmization step: 43100\n",
      "# of episode :2080, avg score : -582.8, optmization step: 43500\n",
      "# of episode :2100, avg score : -626.6, optmization step: 43900\n",
      "# of episode :2120, avg score : -625.8, optmization step: 44400\n",
      "# of episode :2140, avg score : -607.3, optmization step: 44800\n",
      "# of episode :2160, avg score : -642.8, optmization step: 45200\n",
      "# of episode :2180, avg score : -631.0, optmization step: 45600\n",
      "# of episode :2200, avg score : -589.4, optmization step: 46000\n",
      "# of episode :2220, avg score : -593.3, optmization step: 46500\n",
      "# of episode :2240, avg score : -623.8, optmization step: 46900\n",
      "# of episode :2260, avg score : -596.5, optmization step: 47300\n",
      "# of episode :2280, avg score : -618.8, optmization step: 47700\n",
      "# of episode :2300, avg score : -644.1, optmization step: 48100\n",
      "# of episode :2320, avg score : -583.5, optmization step: 48500\n",
      "# of episode :2340, avg score : -623.3, optmization step: 49000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 151\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    149\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_epi\u001b[38;5;241m%\u001b[39mprint_interval\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n_epi\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# of episode :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, avg score : \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m, optmization step: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_epi, score\u001b[38;5;241m/\u001b[39mprint_interval, model\u001b[38;5;241m.\u001b[39moptimization_step))\n",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m, in \u001b[0;36mPPO.train_net\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmin(surr1, surr2) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv(s) , td_target)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate  = 0.0003\n",
    "gamma           = 0.9\n",
    "lmbda           = 0.9\n",
    "eps_clip        = 0.2\n",
    "K_epoch         = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 10\n",
    "minibatch_size = 32\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(3,128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.fc_v = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = 2.0*torch.tanh(self.fc_mu(x))\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        return mu, std\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "                    \n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "                    \n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "        \n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "\n",
    "                    mu, std = self.pi(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    rollout = []\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while count < 200 and not done:\n",
    "            for t in range(rollout_len):\n",
    "                mu, std = model.pi(torch.from_numpy(s).float())\n",
    "                dist = Normal(mu, std)\n",
    "                a = dist.sample()\n",
    "                log_prob = dist.log_prob(a)\n",
    "                s_prime, r, done, truncated, info = env.step([a.item()])\n",
    "\n",
    "                rollout.append((s, a, r/10.0, s_prime, log_prob.item(), done))\n",
    "                if len(rollout) == rollout_len:\n",
    "                    model.put_data(rollout)\n",
    "                    rollout = []\n",
    "\n",
    "                s = s_prime\n",
    "                score += r\n",
    "                count += 1\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, optmization step: {}\".format(n_epi, score/print_interval, model.optimization_step))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c3cfac-bc6a-43b4-ab97-6ffa4b9caf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 26.4\n",
      "# of episode :40, avg score : 28.9\n",
      "# of episode :60, avg score : 38.4\n",
      "# of episode :80, avg score : 40.0\n",
      "# of episode :100, avg score : 83.1\n",
      "# of episode :120, avg score : 78.6\n",
      "# of episode :140, avg score : 134.9\n",
      "# of episode :160, avg score : 184.7\n",
      "# of episode :180, avg score : 212.9\n",
      "# of episode :200, avg score : 227.5\n",
      "# of episode :220, avg score : 190.0\n",
      "# of episode :240, avg score : 109.5\n",
      "# of episode :260, avg score : 79.8\n",
      "# of episode :280, avg score : 106.2\n",
      "# of episode :300, avg score : 219.2\n",
      "# of episode :320, avg score : 196.3\n",
      "# of episode :340, avg score : 173.8\n",
      "# of episode :360, avg score : 256.8\n",
      "# of episode :380, avg score : 249.8\n",
      "# of episode :400, avg score : 250.0\n",
      "# of episode :420, avg score : 226.4\n",
      "# of episode :440, avg score : 228.9\n",
      "# of episode :460, avg score : 310.2\n",
      "# of episode :480, avg score : 367.4\n",
      "# of episode :500, avg score : 190.6\n",
      "# of episode :520, avg score : 144.6\n",
      "# of episode :540, avg score : 176.8\n",
      "# of episode :560, avg score : 253.9\n",
      "# of episode :580, avg score : 270.8\n",
      "# of episode :600, avg score : 215.2\n",
      "# of episode :620, avg score : 209.3\n",
      "# of episode :640, avg score : 210.9\n",
      "# of episode :660, avg score : 316.1\n",
      "# of episode :680, avg score : 438.4\n",
      "# of episode :700, avg score : 466.5\n",
      "# of episode :720, avg score : 409.0\n",
      "# of episode :740, avg score : 399.6\n",
      "# of episode :760, avg score : 438.9\n",
      "# of episode :780, avg score : 295.4\n",
      "# of episode :800, avg score : 478.6\n",
      "# of episode :820, avg score : 546.8\n",
      "# of episode :840, avg score : 973.6\n",
      "# of episode :860, avg score : 265.8\n",
      "# of episode :880, avg score : 4668.9\n",
      "# of episode :900, avg score : 3976.2\n",
      "# of episode :920, avg score : 15241.4\n",
      "# of episode :940, avg score : 84.7\n",
      "# of episode :960, avg score : 119.6\n",
      "# of episode :980, avg score : 150.5\n",
      "# of episode :1000, avg score : 208.6\n",
      "# of episode :1020, avg score : 292.9\n",
      "# of episode :1040, avg score : 251.3\n",
      "# of episode :1060, avg score : 438.6\n",
      "# of episode :1080, avg score : 320.9\n",
      "# of episode :1100, avg score : 290.8\n",
      "# of episode :1120, avg score : 228.2\n",
      "# of episode :1140, avg score : 236.6\n",
      "# of episode :1160, avg score : 376.1\n",
      "# of episode :1180, avg score : 313.0\n",
      "# of episode :1200, avg score : 317.7\n",
      "# of episode :1220, avg score : 1971.5\n",
      "# of episode :1240, avg score : 288.9\n",
      "# of episode :1260, avg score : 282.1\n",
      "# of episode :1280, avg score : 295.3\n",
      "# of episode :1300, avg score : 398.1\n",
      "# of episode :1320, avg score : 309.1\n",
      "# of episode :1340, avg score : 267.9\n",
      "# of episode :1360, avg score : 574.9\n",
      "# of episode :1380, avg score : 1001.5\n",
      "# of episode :1400, avg score : 351.1\n",
      "# of episode :1420, avg score : 242.7\n",
      "# of episode :1440, avg score : 497.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpi(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     99\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(prob)\n\u001b[0;32m--> 100\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m s_prime, r, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mput_data((s, a, r\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100.0\u001b[39m, s_prime, prob[a]\u001b[38;5;241m.\u001b[39mitem(), done))\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/distributions/categorical.py:119\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    117\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m    118\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs_2d, sample_shape\u001b[38;5;241m.\u001b[39mnumel(), \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msamples_2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "        \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(T_horizon):\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "                model.put_data((s, a, r/10.0, s_prime, prob[a].item(), done))\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707a46b2-f955-48f8-8f17-6758944ff380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Hardcode the arguments as variables\n",
    "exp_name = \"your_experiment_name\"\n",
    "seed = 1\n",
    "torch_deterministic = True\n",
    "cuda = True\n",
    "track = False\n",
    "wandb_project_name = \"cleanRL\"\n",
    "wandb_entity = None\n",
    "capture_video = False\n",
    "env_id = \"Hopper-v4\"\n",
    "total_timesteps = 1000000\n",
    "buffer_size = int(1e6)\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 256\n",
    "learning_starts = 5e3\n",
    "policy_lr = 3e-4\n",
    "q_lr = 1e-3\n",
    "policy_frequency = 2\n",
    "target_network_frequency = 1\n",
    "noise_clip = 0.5\n",
    "alpha = 0.2\n",
    "autotune = True\n",
    "\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9d7521f-408f-4160-b44d-7f270e6312d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e274bc53-6000-4477-a147-19ad9d90b2ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m action, log_prob, mean\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtrack:\n",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m, in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     64\u001b[0m         help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy regularization coefficient.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--autotune\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;28mbool\u001b[39m(strtobool(x)), default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, const\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic tuning of the entropy coefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:1872\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1871\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1872\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45ca7fe-ee00-4031-8695-f39819fa1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--exp-name EXP_NAME] [--seed SEED]\n",
      "                             [--torch-deterministic [TORCH_DETERMINISTIC]]\n",
      "                             [--cuda [CUDA]] [--track [TRACK]]\n",
      "                             [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "                             [--wandb-entity WANDB_ENTITY]\n",
      "                             [--capture-video [CAPTURE_VIDEO]]\n",
      "                             [--env-id ENV_ID]\n",
      "                             [--total-timesteps TOTAL_TIMESTEPS]\n",
      "                             [--buffer-size BUFFER_SIZE] [--gamma GAMMA]\n",
      "                             [--tau TAU] [--batch-size BATCH_SIZE]\n",
      "                             [--learning-starts LEARNING_STARTS]\n",
      "                             [--policy-lr POLICY_LR] [--q-lr Q_LR]\n",
      "                             [--policy-frequency POLICY_FREQUENCY]\n",
      "                             [--target-network-frequency TARGET_NETWORK_FREQUENCY]\n",
      "                             [--noise-clip NOISE_CLIP] [--alpha ALPHA]\n",
      "                             [--autotune [AUTOTUNE]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /econ_share/home/pp712/.local/share/jupyter/runtime/kernel-7fff69b6-23d8-4125-b9b2-cd5d2ec9133c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    \n",
    "    # Initialize WandB if tracking is enabled\n",
    "    if args.track:\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    \n",
    "    # Create a Tensorboard SummaryWriter\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    \n",
    "    # Seed the random number generators\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    \n",
    "    # Set the device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    \n",
    "    # Set up the environment\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "    \n",
    "    max_action = float(envs.single_action_space.high[0])\n",
    "    \n",
    "    actor = Actor(envs).to(device)\n",
    "    qf1 = SoftQNetwork(envs).to(device)\n",
    "    qf2 = SoftQNetwork(envs).to(device)\n",
    "    qf1_target = SoftQNetwork(envs).to(device)\n",
    "    qf2_target = SoftQNetwork(envs).to(device)\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    qf2_target.load_state_dict(qf2.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)\n",
    "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)\n",
    "    \n",
    "    # Automatic entropy tuning\n",
    "    if args.autotune:\n",
    "        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "        log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        alpha = log_alpha.exp().item()\n",
    "        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)\n",
    "    else:\n",
    "        alpha = args.alpha\n",
    "    \n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        envs.single_observation_space,\n",
    "        envs.single_action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=True,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Main training loop\n",
    "    obs = envs.reset()\n",
    "    for global_step in range(args.total_timesteps):\n",
    "        # Action logic here\n",
    "        if global_step < args.learning_starts:\n",
    "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
    "        else:\n",
    "            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n",
    "            actions = actions.detach().cpu().numpy()\n",
    "        \n",
    "        # Execute actions, collect data, and log information\n",
    "        next_obs, rewards, dones, infos = envs.step(actions)\n",
    "        \n",
    "        for info in infos:\n",
    "            if \"episode\" in info.keys():\n",
    "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "                break\n",
    "        \n",
    "        real_next_obs = next_obs.copy()\n",
    "        for idx, d in enumerate(dones):\n",
    "            if d:\n",
    "                real_next_obs[idx] = infos[idx][\"terminal_observation\"]\n",
    "        rb.add(obs, real_next_obs, actions, rewards, dones, infos)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        # Training logic\n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)\n",
    "                qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "                qf2_next_target = qf2_target(data.next_observations, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "            \n",
    "            qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "            qf2_a_values = qf2(data.observations, data.actions).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                for _ in range(args.policy_frequency):\n",
    "                    pi, log_pi, _ = actor.get_action(data.observations)\n",
    "                    qf1_pi = qf1(data.observations, pi)\n",
    "                    qf2_pi = qf2(data.observations, pi)\n",
    "                    min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)\n",
    "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "                    \n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "                    \n",
    "                    if args.autotune:\n",
    "                        with torch.no_grad():\n",
    "                            _, log_pi, _ = actor.get_action(data.observations)\n",
    "                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
    "                        \n",
    "                        a_optimizer.zero_grad()\n",
    "                        alpha_loss.backward()\n",
    "                        a_optimizer.step()\n",
    "                        alpha = log_alpha.exp().item()\n",
    "            \n",
    "            if global_step % args.target_network_frequency == 0:\n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            \n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/alpha\", alpha, global_step)\n",
    "                print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "                if args.autotune:\n",
    "                    writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n",
    "    \n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e4dcf0-b2ea-41b5-a3d5-3c2269850381",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution code\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Initialize WandB if tracking is enabled\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m, in \u001b[0;36mparse_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     64\u001b[0m         help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy regularization coefficient.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--autotune\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[38;5;28mbool\u001b[39m(strtobool(x)), default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, const\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic tuning of the entropy coefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:1872\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1871\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1872\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02413ba1-90cc-453a-9489-88213b9e4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
