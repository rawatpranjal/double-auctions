{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73df2c31-2705-49cf-a485-9bd47c3a9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import *\n",
    "from agents import *\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba30afc9-b9e9-44d1-b4a0-64fc08404772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50.5 71.2 121.7 37.6\n",
      "1 110.6 35.6 146.2 42.8\n",
      "2 132.6 23.7 156.4 46.4\n",
      "3 124.8 29.2 154.0 44.8\n",
      "4 114.1 37.3 151.3 44.3\n",
      "5 95.1 37.9 133.0 42.2\n",
      "6 88.6 39.0 127.6 41.7\n",
      "7 77.5 39.9 117.4 41.1\n",
      "8 80.2 37.4 117.6 40.0\n",
      "9 75.4 35.6 111.0 39.5\n",
      "10 68.6 33.4 101.9 39.2\n",
      "11 62.9 29.9 92.8 38.8\n",
      "12 62.2 30.3 92.5 38.2\n",
      "13 57.8 28.9 86.6 37.7\n",
      "14 60.7 29.6 90.3 37.4\n",
      "15 59.5 27.9 87.4 37.7\n",
      "16 56.0 28.3 84.3 37.5\n",
      "17 54.9 27.3 82.2 37.4\n",
      "18 55.2 28.0 83.2 37.7\n",
      "19 52.5 26.1 78.6 37.4\n",
      "20 55.4 25.7 81.0 37.7\n",
      "21 58.7 24.5 83.2 37.9\n",
      "22 60.3 24.8 85.1 37.9\n",
      "23 57.8 23.6 81.4 38.0\n",
      "24 57.5 22.8 80.3 37.8\n",
      "25 59.0 21.5 80.4 38.0\n",
      "26 60.1 21.1 81.2 38.4\n",
      "27 62.9 20.3 83.2 38.5\n",
      "28 62.7 20.0 82.7 38.6\n",
      "29 61.6 19.8 81.3 38.7\n",
      "30 62.4 19.6 82.0 38.9\n",
      "31 62.5 20.0 82.6 39.0\n",
      "32 64.3 19.4 83.7 39.3\n",
      "33 65.6 18.8 84.4 39.5\n",
      "34 65.9 18.3 84.2 40.1\n",
      "35 67.5 17.8 85.2 40.2\n",
      "36 68.1 17.4 85.5 40.4\n",
      "37 68.0 17.0 85.0 40.7\n",
      "38 68.6 16.3 84.9 40.9\n",
      "39 68.8 15.8 84.6 41.1\n",
      "40 68.3 15.5 83.7 41.4\n",
      "41 68.6 15.7 84.2 41.7\n",
      "42 68.6 15.5 84.1 41.8\n",
      "43 69.8 15.2 85.0 42.1\n",
      "44 69.5 15.0 84.4 42.5\n",
      "45 69.1 14.6 83.7 42.6\n",
      "46 69.5 14.3 83.8 42.8\n",
      "47 69.7 14.0 83.7 43.3\n",
      "48 69.4 13.7 83.2 43.7\n",
      "49 70.0 13.4 83.4 43.8\n",
      "50 70.2 13.0 83.2 44.0\n",
      "51 70.4 12.8 83.2 44.2\n",
      "52 70.2 12.6 82.8 44.3\n",
      "53 69.4 12.3 81.7 44.3\n",
      "54 69.4 12.2 81.5 44.6\n",
      "55 68.7 11.7 80.5 44.7\n",
      "56 68.6 11.5 80.1 44.9\n",
      "57 68.6 11.3 80.0 45.0\n",
      "58 67.9 11.1 79.1 45.2\n",
      "59 67.3 11.0 78.3 45.5\n",
      "60 67.7 10.8 78.5 45.7\n",
      "61 67.6 10.6 78.2 45.9\n",
      "62 67.4 10.4 77.9 46.2\n",
      "63 67.4 10.3 77.6 46.5\n",
      "64 66.7 10.0 76.7 46.6\n",
      "65 66.4 9.7 76.1 46.7\n",
      "66 67.2 9.6 76.8 46.7\n",
      "67 67.2 9.4 76.6 46.9\n",
      "68 67.0 9.1 76.0 47.0\n",
      "69 66.8 9.2 76.0 47.1\n",
      "70 66.9 9.1 76.0 47.3\n",
      "71 66.5 9.0 75.5 47.3\n",
      "72 66.6 8.8 75.4 47.4\n",
      "73 66.4 8.7 75.1 47.6\n",
      "74 66.6 8.6 75.2 47.8\n",
      "75 66.4 8.5 74.9 48.0\n",
      "76 66.4 8.4 74.7 48.2\n",
      "77 65.7 8.4 74.2 48.3\n",
      "78 65.7 8.3 74.0 48.5\n",
      "79 65.9 8.2 74.1 48.5\n",
      "80 65.6 8.1 73.7 48.8\n",
      "81 65.5 8.0 73.5 48.9\n",
      "82 65.5 7.9 73.4 49.0\n",
      "83 65.6 7.8 73.4 49.2\n",
      "84 65.6 7.7 73.3 49.2\n",
      "85 65.8 7.6 73.4 49.3\n",
      "86 65.8 7.5 73.3 49.3\n",
      "87 65.4 7.4 72.9 49.6\n",
      "88 65.5 7.4 72.9 49.8\n",
      "89 65.1 7.3 72.4 50.1\n",
      "90 65.0 7.2 72.2 50.2\n",
      "91 64.9 7.1 72.0 50.5\n",
      "92 64.8 7.0 71.9 50.7\n",
      "93 64.7 7.0 71.7 50.8\n",
      "94 64.7 6.9 71.6 51.0\n",
      "95 64.5 7.1 71.7 51.0\n",
      "96 64.3 7.1 71.4 51.1\n",
      "97 64.4 7.0 71.4 51.2\n",
      "98 64.5 6.9 71.4 51.2\n",
      "99 64.3 6.9 71.2 51.3\n",
      "100 64.6 6.2 70.7 51.5\n",
      "101 63.3 6.2 69.4 51.6\n",
      "102 61.8 6.2 68.0 51.7\n",
      "103 61.3 5.7 67.0 51.9\n",
      "104 61.3 5.0 66.3 52.1\n",
      "105 61.7 4.6 66.3 52.4\n",
      "106 61.7 4.2 65.9 52.7\n",
      "107 62.2 3.7 65.9 52.9\n",
      "108 62.0 3.5 65.6 53.0\n",
      "109 62.3 3.3 65.6 53.2\n",
      "110 63.0 3.2 66.2 53.4\n",
      "111 63.5 3.3 66.8 53.6\n",
      "112 63.3 2.9 66.2 53.9\n",
      "113 63.6 2.8 66.5 54.1\n",
      "114 63.1 2.5 65.5 54.4\n",
      "115 63.6 2.4 66.0 54.5\n",
      "116 64.2 2.1 66.3 54.7\n",
      "117 64.5 2.0 66.5 54.9\n",
      "118 64.7 1.6 66.3 55.1\n",
      "119 65.0 1.6 66.6 55.4\n",
      "120 64.4 1.4 65.9 55.5\n",
      "121 63.9 1.4 65.4 55.7\n",
      "122 63.7 1.1 64.8 55.7\n",
      "123 64.3 1.2 65.5 55.9\n",
      "124 64.6 1.1 65.7 56.0\n",
      "125 64.3 1.2 65.6 56.2\n",
      "126 64.1 1.1 65.2 56.2\n",
      "127 63.3 1.1 64.4 56.4\n",
      "128 63.4 1.0 64.4 56.6\n",
      "129 64.0 0.9 64.9 56.5\n",
      "130 64.2 0.7 65.0 56.6\n",
      "131 64.2 0.4 64.7 56.8\n",
      "132 64.1 0.4 64.5 56.7\n",
      "133 63.9 0.6 64.6 56.8\n",
      "134 63.7 0.6 64.3 56.7\n",
      "135 63.6 0.6 64.2 56.8\n",
      "136 63.2 0.6 63.8 56.9\n",
      "137 63.1 0.6 63.7 56.9\n",
      "138 63.5 0.7 64.2 56.8\n",
      "139 63.4 0.8 64.2 56.8\n",
      "140 63.8 0.9 64.7 56.7\n",
      "141 63.5 0.6 64.1 56.7\n",
      "142 63.2 0.8 64.0 56.7\n",
      "143 62.9 0.8 63.7 56.5\n",
      "144 63.2 0.8 63.9 56.4\n",
      "145 63.4 0.8 64.1 56.4\n",
      "146 63.2 0.8 64.1 56.3\n",
      "147 63.0 0.8 63.8 56.2\n",
      "148 63.2 0.8 64.1 56.1\n",
      "149 63.5 0.8 64.3 56.0\n",
      "150 63.2 0.9 64.1 56.0\n",
      "151 63.4 0.9 64.2 56.0\n",
      "152 63.3 1.0 64.2 55.9\n",
      "153 63.8 1.0 64.8 55.9\n",
      "154 63.4 1.1 64.5 55.8\n",
      "155 63.7 1.2 64.9 55.9\n",
      "156 63.6 1.2 64.8 55.9\n",
      "157 63.5 1.2 64.7 56.0\n",
      "158 63.7 1.2 64.9 56.1\n",
      "159 64.0 1.2 65.2 56.0\n",
      "160 63.5 1.1 64.5 56.0\n",
      "161 63.5 1.1 64.6 55.9\n",
      "162 63.5 1.2 64.7 55.8\n",
      "163 63.4 1.1 64.5 55.6\n",
      "164 64.0 1.1 65.1 55.5\n",
      "165 64.2 1.3 65.4 55.5\n",
      "166 63.8 1.3 65.1 55.6\n",
      "167 63.8 1.3 65.1 55.5\n",
      "168 64.0 1.4 65.4 55.5\n",
      "169 63.8 1.4 65.2 55.5\n",
      "170 63.9 1.4 65.3 55.4\n",
      "171 64.2 1.4 65.6 55.3\n",
      "172 64.3 1.4 65.7 55.3\n",
      "173 64.7 1.4 66.1 55.1\n",
      "174 64.5 1.4 65.9 55.1\n",
      "175 64.7 1.4 66.1 55.0\n",
      "176 64.7 1.4 66.1 54.9\n",
      "177 65.2 1.4 66.6 54.8\n",
      "178 65.5 1.4 66.9 54.7\n",
      "179 65.0 1.5 66.5 54.7\n",
      "180 65.4 1.5 66.9 54.5\n",
      "181 65.5 1.6 67.1 54.4\n",
      "182 65.7 1.6 67.4 54.3\n",
      "183 65.9 1.6 67.5 54.2\n",
      "184 65.4 1.4 66.9 54.3\n",
      "185 65.2 1.4 66.6 54.2\n",
      "186 65.3 1.4 66.7 54.3\n",
      "187 65.6 1.6 67.2 54.1\n",
      "188 65.4 1.8 67.2 53.8\n",
      "189 65.7 1.8 67.5 53.6\n",
      "190 65.8 1.8 67.6 53.5\n",
      "191 66.1 2.0 68.1 53.3\n",
      "192 66.6 2.0 68.6 53.0\n",
      "193 66.8 1.9 68.7 52.9\n",
      "194 66.8 1.9 68.7 52.7\n",
      "195 66.6 1.6 68.2 52.9\n",
      "196 67.0 1.6 68.6 52.8\n",
      "197 67.2 1.6 68.8 52.7\n",
      "198 67.3 1.6 68.9 52.7\n",
      "199 67.6 1.6 69.2 52.6\n",
      "200 67.5 1.6 69.1 52.6\n",
      "201 67.6 1.6 69.2 52.5\n",
      "202 68.0 1.6 69.6 52.4\n",
      "203 68.0 1.6 69.6 52.4\n",
      "204 67.9 1.7 69.5 52.3\n",
      "205 68.0 1.7 69.6 52.3\n",
      "206 68.0 1.8 69.8 52.1\n",
      "207 68.3 1.8 70.1 52.1\n",
      "208 67.9 1.8 69.8 52.3\n",
      "209 67.8 1.8 69.6 52.4\n",
      "210 67.5 1.8 69.4 52.4\n",
      "211 67.9 1.8 69.7 52.3\n",
      "212 68.1 1.9 70.1 52.3\n",
      "213 68.6 1.9 70.6 52.2\n",
      "214 68.8 2.1 70.9 52.1\n",
      "215 68.9 2.1 70.9 52.2\n",
      "216 68.9 2.1 70.9 52.1\n",
      "217 68.6 2.1 70.7 52.1\n",
      "218 68.3 2.3 70.6 52.0\n",
      "219 68.7 2.4 71.0 51.8\n",
      "220 68.4 2.3 70.7 51.9\n",
      "221 68.4 2.3 70.7 51.8\n",
      "222 68.4 2.4 70.7 51.8\n",
      "223 68.4 2.4 70.7 51.9\n",
      "224 67.8 2.4 70.1 52.0\n",
      "225 67.5 2.5 70.0 52.0\n",
      "226 67.3 2.5 69.8 52.0\n",
      "227 67.6 2.5 70.1 52.0\n",
      "228 67.7 2.5 70.2 51.8\n",
      "229 67.5 2.5 70.0 51.9\n",
      "230 67.1 2.5 69.6 51.9\n",
      "231 66.8 2.5 69.3 51.9\n",
      "232 66.4 2.5 69.0 52.0\n",
      "233 66.2 2.4 68.6 52.1\n",
      "234 66.4 2.4 68.8 52.3\n",
      "235 66.2 2.4 68.5 52.2\n",
      "236 66.1 2.4 68.5 52.2\n",
      "237 66.2 2.3 68.6 52.3\n",
      "238 65.5 2.4 67.9 52.5\n",
      "239 65.6 2.4 68.0 52.5\n",
      "240 65.5 2.3 67.8 52.5\n",
      "241 65.7 2.4 68.1 52.5\n",
      "242 66.0 2.2 68.2 52.7\n",
      "243 65.8 2.2 68.1 52.7\n",
      "244 65.4 2.5 67.8 52.7\n",
      "245 65.3 2.7 67.9 52.7\n",
      "246 65.6 2.6 68.2 52.7\n",
      "247 65.7 2.6 68.3 52.7\n",
      "248 65.7 2.8 68.5 52.7\n",
      "249 64.9 2.8 67.8 52.8\n",
      "250 65.0 2.9 67.9 52.8\n",
      "251 64.9 2.9 67.8 52.8\n",
      "252 65.0 3.0 68.0 52.9\n",
      "253 64.7 3.0 67.7 53.0\n",
      "254 64.9 2.8 67.8 53.1\n",
      "255 64.9 2.8 67.7 53.1\n",
      "256 65.1 2.8 68.0 53.1\n",
      "257 65.2 2.9 68.1 53.0\n",
      "258 65.5 2.9 68.5 52.9\n",
      "259 65.4 2.9 68.3 52.9\n",
      "260 65.7 3.2 68.9 52.8\n",
      "261 65.3 3.1 68.4 52.9\n",
      "262 65.0 3.2 68.2 53.0\n",
      "263 65.2 3.2 68.4 53.1\n",
      "264 64.9 3.2 68.0 53.3\n",
      "265 64.8 3.2 68.0 53.4\n",
      "266 64.5 3.2 67.7 53.4\n",
      "267 64.5 3.2 67.7 53.4\n",
      "268 64.3 3.2 67.5 53.5\n",
      "269 64.4 3.0 67.5 53.6\n",
      "270 64.6 3.1 67.7 53.7\n",
      "271 64.1 3.0 67.1 53.8\n",
      "272 63.8 3.1 66.9 54.0\n",
      "273 63.4 3.1 66.5 54.0\n",
      "274 63.3 3.2 66.5 54.1\n",
      "275 63.3 3.2 66.4 54.1\n",
      "276 63.2 3.2 66.3 54.1\n",
      "277 63.0 3.0 66.1 54.2\n",
      "278 62.7 2.9 65.6 54.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numSteps):\n\u001b[1;32m     26\u001b[0m     resetSteps(buyers, sellers)\n\u001b[0;32m---> 27\u001b[0m     bids, asks \u001b[38;5;241m=\u001b[39m \u001b[43mcollectOffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuyers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msellers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     currentAsk, currentAskIdx, currentBid, currentBidIdx \u001b[38;5;241m=\u001b[39m bestOffers(bids, asks)\n\u001b[1;32m     29\u001b[0m     price, buy, sell \u001b[38;5;241m=\u001b[39m trade(buyers, sellers, currentAsk, currentAskIdx, currentBid, currentBidIdx)\n",
      "File \u001b[0;32m/econ_share/home/pp712/double-auctions/code/7_clean/setup.py:117\u001b[0m, in \u001b[0;36mcollectOffers\u001b[0;34m(buyers, sellers)\u001b[0m\n\u001b[1;32m    115\u001b[0m bids, asks \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, buyer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(buyers):\n\u001b[0;32m--> 117\u001b[0m     bids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbuyer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seller \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sellers):\n\u001b[1;32m    119\u001b[0m     asks\u001b[38;5;241m.\u001b[39mappend(seller\u001b[38;5;241m.\u001b[39mask())    \n",
      "Cell \u001b[0;32mIn[116], line 53\u001b[0m, in \u001b[0;36mPPO.bid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbid\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstepBid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstepTokenValue \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m         frac \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[60], line 107\u001b[0m, in \u001b[0;36mPPOModel.select_action\u001b[0;34m(self, state, memory)\u001b[0m\n\u001b[1;32m    105\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()   \u001b[38;5;66;03m# flatten the state\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#return self.old_policy.act(state, memory).cpu().numpy().flatten()\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mold_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 57\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[0;34m(self, state, memory)\u001b[0m\n\u001b[1;32m     55\u001b[0m action_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)                     \u001b[38;5;66;03m# (1,4)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m cov_mat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_var)   \u001b[38;5;66;03m# (4,4)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()                              \u001b[38;5;66;03m# (1,4)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m action_logprob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/distributions/multivariate_normal.py:150\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m    149\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/distributions/distribution.py:60\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     59\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m---> 60\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/distributions/constraints.py:509\u001b[0m, in \u001b[0;36m_PositiveDefinite.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 509\u001b[0m     sym_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sym_check\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sym_check\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/distributions/constraints.py:490\u001b[0m, in \u001b[0;36m_Symmetric.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m square_check\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m square_check\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "gameTypes, numBuyers, numSellers, numTokens, numRounds, numPeriods, numSteps, seed = '1001', 2, 2, 3, 1, 50000, 9, 42\n",
    "disclosure = ['rnd', 'period', 'step','currentBid','currentAsk','buy','sell','price','sale']\n",
    "buyerStrategies = ['PPO', 'PPO']\n",
    "sellerStrategies = ['PPO', 'PPO']\n",
    "gameData = [gameTypes, numBuyers, numSellers, numTokens, numRounds, numPeriods, numSteps, seed]\n",
    "buyers, sellers = generateAgents(gameData,buyerStrategies,sellerStrategies,disclosure)\n",
    "log = Log(gameData, buyerStrategies, sellerStrategies, disclosure)\n",
    "\n",
    "for rnd in range(numRounds):\n",
    "    roundData = roundSetup(*gameData)\n",
    "    redemptionValues, tokenCosts, demand, supply, prices, peq, qeq  = roundData[0:7]\n",
    "    [buyerReservationPrices, sellerReservationPrices, buyerSurplus, sellerSurplus, totalSurplus, buyerSurplusFrac, sellerSurplusFrac] = roundData[7:]\n",
    "    log.addRound([rnd] + roundData)\n",
    "    resetRounds(buyers, sellers, redemptionValues, tokenCosts)\n",
    "    Rlist = []\n",
    "    Rlist2 = []\n",
    "    Rlist3 = []\n",
    "    bids_made = []\n",
    "    for period in range(numPeriods):\n",
    "        resetPeriods(buyers, sellers)\n",
    "        R = 0\n",
    "        R2 = 0\n",
    "        bidsmade = []\n",
    "        for step in range(numSteps):\n",
    "            resetSteps(buyers, sellers)\n",
    "            bids, asks = collectOffers(buyers, sellers)\n",
    "            currentAsk, currentAskIdx, currentBid, currentBidIdx = bestOffers(bids, asks)\n",
    "            price, buy, sell = trade(buyers, sellers, currentAsk, currentAskIdx, currentBid, currentBidIdx)\n",
    "            bprofit, sprofit = 0, 0\n",
    "            bidsmade.append(bids[0])\n",
    "            if price > 0:\n",
    "                buyers[currentBidIdx].transact(price)\n",
    "                sellers[currentAskIdx].transact(price)\n",
    "                bprofit = buyers[currentBidIdx].stepProfits\n",
    "                sprofit = sellers[currentAskIdx].stepProfits\n",
    "            R += buyers[0].stepProfits\n",
    "            R2 += buyers[1].stepProfits\n",
    "            log.addStep([rnd, period, step, bids, asks, currentBid, currentBidIdx, currentAsk, currentAskIdx, buy, sell, price, price>0, bprofit, sprofit])\n",
    "            observe(buyers, sellers, log.disclose()) # observe new state\n",
    "            updateStates(buyers, sellers)\n",
    "        Rlist.append(R)\n",
    "        Rlist2.append(R2)\n",
    "        Rlist3.append(R+R2)\n",
    "        bids_made.append(np.nanmean(bidsmade))\n",
    "        updatePolicy(buyers, sellers)\n",
    "        print(period, np.round(np.mean(Rlist[-100:]),1),  np.round(np.mean(Rlist2[-100:]),1), np.round(np.mean(Rlist3[-100:]),1),np.round(np.nanmean(bids_made[-100:]),1) )\n",
    "    #if period % (numPeriods/100) == 0:\n",
    "    #    print(f'\\n{period}')\n",
    "        #display(log.getPeriod(rnd, period).tail(100)[['bprofit', 'currentBidIdx']].groupby('currentBidIdx').sum())\n",
    "        #display(log.getPeriod(rnd, period).tail(100)[['sprofit', 'currentAskIdx']].groupby('currentAskIdx').sum())\n",
    "        #display(log.getPeriod(rnd,period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "371cc3bd-ae54-4f57-a7ed-289d14ffd4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[100. ,  93. ,  49.1],\n",
       "        [ 66.8,  66.2,  50.8]]),\n",
       " array([[ 1. , 51.3, 59.9],\n",
       "        [34.6, 41.4, 75.1],\n",
       "        [41.6, 50.4, 50.8],\n",
       "        [ 0. , 35. , 43.3],\n",
       "        [28.2, 54.1, 65.9],\n",
       "        [ 2.8, 38.9, 42.1]]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.roundData.buyerValues.item(), log.roundData.sellerCosts.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "026a7af4-66dd-466d-83d7-ef530b248dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peq</th>\n",
       "      <th>qeq</th>\n",
       "      <th>buyerSurplus</th>\n",
       "      <th>sellerSurplus</th>\n",
       "      <th>buyerSurplusFrac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.9</td>\n",
       "      <td>6</td>\n",
       "      <td>204.5</td>\n",
       "      <td>119.8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    peq  qeq  buyerSurplus  sellerSurplus  buyerSurplusFrac\n",
       "0  36.9    6         204.5          119.8               0.6"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.roundData[['peq', 'qeq', 'buyerSurplus', 'sellerSurplus', 'buyerSurplusFrac']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d2aad580-3730-4be4-b945-3e066f58e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnd</th>\n",
       "      <th>period</th>\n",
       "      <th>step</th>\n",
       "      <th>bids</th>\n",
       "      <th>asks</th>\n",
       "      <th>currentBid</th>\n",
       "      <th>currentBidIdx</th>\n",
       "      <th>currentAsk</th>\n",
       "      <th>currentAskIdx</th>\n",
       "      <th>buy</th>\n",
       "      <th>sell</th>\n",
       "      <th>price</th>\n",
       "      <th>sale</th>\n",
       "      <th>bprofit</th>\n",
       "      <th>sprofit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>[73.6, 12.4]</td>\n",
       "      <td>[1.0, 34.6, 41.6, 0.0, 28.2, 2.8]</td>\n",
       "      <td>73.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>36.80</td>\n",
       "      <td>True</td>\n",
       "      <td>63.20</td>\n",
       "      <td>36.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>[73.7, 89.5]</td>\n",
       "      <td>[1.0, 34.6, 41.6, 35.0, 28.2, 2.8]</td>\n",
       "      <td>89.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>45.25</td>\n",
       "      <td>True</td>\n",
       "      <td>21.55</td>\n",
       "      <td>44.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>[74.4, 31.9]</td>\n",
       "      <td>[51.3, 34.6, 41.6, 35.0, 28.2, 2.8]</td>\n",
       "      <td>74.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>38.60</td>\n",
       "      <td>True</td>\n",
       "      <td>54.40</td>\n",
       "      <td>35.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>[60.5, 0.0]</td>\n",
       "      <td>[51.3, 34.6, 41.6, 35.0, 28.2, 38.9]</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>44.35</td>\n",
       "      <td>True</td>\n",
       "      <td>4.75</td>\n",
       "      <td>16.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>[nan, 0.0]</td>\n",
       "      <td>[51.3, 34.6, 41.6, 35.0, 54.1, 38.9]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>34.60</td>\n",
       "      <td>True</td>\n",
       "      <td>31.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>[nan, 0.0]</td>\n",
       "      <td>[51.3, 41.4, 41.6, 35.0, 54.1, 38.9]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>35.00</td>\n",
       "      <td>True</td>\n",
       "      <td>15.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[51.3, 41.4, 41.6, 43.3, 54.1, 38.9]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.9</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>7</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[51.3, 41.4, 41.6, 43.3, 54.1, 38.9]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.9</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[51.3, 41.4, 41.6, 43.3, 54.1, 38.9]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.9</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rnd  period  step          bids                                  asks  \\\n",
       "1152    0     128     0  [73.6, 12.4]     [1.0, 34.6, 41.6, 0.0, 28.2, 2.8]   \n",
       "1153    0     128     1  [73.7, 89.5]    [1.0, 34.6, 41.6, 35.0, 28.2, 2.8]   \n",
       "1154    0     128     2  [74.4, 31.9]   [51.3, 34.6, 41.6, 35.0, 28.2, 2.8]   \n",
       "1155    0     128     3   [60.5, 0.0]  [51.3, 34.6, 41.6, 35.0, 28.2, 38.9]   \n",
       "1156    0     128     4    [nan, 0.0]  [51.3, 34.6, 41.6, 35.0, 54.1, 38.9]   \n",
       "1157    0     128     5    [nan, 0.0]  [51.3, 41.4, 41.6, 35.0, 54.1, 38.9]   \n",
       "1158    0     128     6    [nan, nan]  [51.3, 41.4, 41.6, 43.3, 54.1, 38.9]   \n",
       "1159    0     128     7    [nan, nan]  [51.3, 41.4, 41.6, 43.3, 54.1, 38.9]   \n",
       "1160    0     128     8    [nan, nan]  [51.3, 41.4, 41.6, 43.3, 54.1, 38.9]   \n",
       "\n",
       "      currentBid  currentBidIdx  currentAsk  currentAskIdx   buy   sell  \\\n",
       "1152        73.6            0.0         0.0              3  True   True   \n",
       "1153        89.5            1.0         1.0              0  True   True   \n",
       "1154        74.4            0.0         2.8              5  True   True   \n",
       "1155        60.5            0.0        28.2              4  True   True   \n",
       "1156         0.0            1.0        34.6              1  True  False   \n",
       "1157         0.0            1.0        35.0              3  True  False   \n",
       "1158         NaN            NaN        38.9              5   NaN    NaN   \n",
       "1159         NaN            NaN        38.9              5   NaN    NaN   \n",
       "1160         NaN            NaN        38.9              5   NaN    NaN   \n",
       "\n",
       "      price   sale  bprofit  sprofit  \n",
       "1152  36.80   True    63.20    36.80  \n",
       "1153  45.25   True    21.55    44.25  \n",
       "1154  38.60   True    54.40    35.80  \n",
       "1155  44.35   True     4.75    16.15  \n",
       "1156  34.60   True    31.60     0.00  \n",
       "1157  35.00   True    15.80     0.00  \n",
       "1158    NaN  False     0.00     0.00  \n",
       "1159    NaN  False     0.00     0.00  \n",
       "1160    NaN  False     0.00     0.00  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.getPeriod(0,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "73ac94b4-2755-4eeb-acec-c8433a41cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(Trader):\n",
    "    def __init__(self, gameData, disclosure, index, buyer, reinforcer):\n",
    "        super().__init__(gameData, disclosure, index, buyer, reinforcer)\n",
    "        self.learningRate = 0.0002\n",
    "        self.gamma = 0.99\n",
    "        self.numActions = 1\n",
    "        self.depth = 20\n",
    "        self.numStates = 8 + 6 * self.depth\n",
    "        agentState = [0, -1, -1, -1, -1, -1, -1, -1]\n",
    "        history = [-1, -1, -1, -1, -1, -1] * self.depth\n",
    "        self.state = np.nan_to_num(np.array(agentState + history, dtype = np.float32), nan=-9)\n",
    "\n",
    "        action_std = 0.02\n",
    "        betas = [0.9, 0.990]\n",
    "        max_episodes = 10000000\n",
    "        max_timesteps = 100\n",
    "        update_timesteps = 1\n",
    "        action_std = 0.2\n",
    "        K_epochs = 1\n",
    "        eps_clip = 0.2\n",
    "        gamma = 0.98\n",
    "        lr = 0.0003\n",
    "        seed = 123\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.memory = Memory()\n",
    "        self.ppo = PPOModel(self.numStates, self.numActions, action_std, lr, betas, gamma, K_epochs, eps_clip, restore=False, ckpt=None)\n",
    "        self.done = False\n",
    "        self.time_step = 0\n",
    "        self.update_timestep = 1\n",
    "\n",
    "    def observe(self):\n",
    "        self.time_step += 1\n",
    "        agentState = [self.periodStep, self.stepTokenValue, self.stepBid, self.stepAsk, self.stepTrades, self.stepProfits, self.periodTrades, self.periodProfits]\n",
    "        history = self.df.iloc[-self.depth:][['currentBid', 'currentAsk', 'buy', 'sell', 'price', 'price']].values.reshape(-1,).tolist()\n",
    "        if len(history) != (6 * self.depth):\n",
    "            history = [-1] * (6 * self.depth)\n",
    "        self.newState = np.nan_to_num(np.array(agentState + history, dtype = np.float32), nan=-9)\n",
    "        if self.periodStep == self.numSteps:\n",
    "            self.done = True\n",
    "        self.memory.rewards.append(self.stepProfits)\n",
    "        self.memory.is_terminals.append(self.done)\n",
    "        self.state = self.newState\n",
    "\n",
    "    def train(self):\n",
    "        #if self.time_step % self.update_timestep == 0:\n",
    "        self.ppo.update(self.memory)\n",
    "        self.memory.clear_memory()\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def bid(self):\n",
    "        self.stepBid = np.nan\n",
    "        self.action = self.ppo.select_action(self.state, self.memory)\n",
    "        if self.stepTokenValue > 0:\n",
    "            frac = (np.clip(self.action.item(),-1,1)+1)/2\n",
    "            self.stepBid = np.round(frac * 100,1)\n",
    "        return self.stepBid\n",
    "        \n",
    "    def ask(self):\n",
    "        self.stepAsk = np.nan\n",
    "        self.action = self.ppo.select_action(self.state, self.memory)\n",
    "        if self.stepTokenValue > 0:\n",
    "            frac = self.action.item()\n",
    "            self.stepAsk = np.round(frac * 100,1)\n",
    "        return self.stepAsk\n",
    "\n",
    "def updatePolicy(buyers, sellers):\n",
    "    for buyer in buyers:\n",
    "        if buyer.reinforcer == 1:\n",
    "            buyer.train()\n",
    "    for seller in sellers:\n",
    "        if seller.reinforcer == 1:\n",
    "            seller.train()\n",
    "\n",
    "def updateStates(buyers, sellers):\n",
    "    for buyer in buyers:\n",
    "        if buyer.reinforcer == 1:\n",
    "            buyer.observe()\n",
    "    for seller in sellers:\n",
    "        if seller.reinforcer == 1:\n",
    "            seller.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c502315d-8508-4cc3-b22a-9611b8f2850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a PPO algorithm for multi-dimension continuous action\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "class Memory:   # collected from old policy\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.logprobs = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "        del self.logprobs[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        global device\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.action_var = torch.full((action_dim, ), action_std * action_std)    #(4, )\n",
    "\n",
    "    def act(self, state, memory):       # state (1,24)\n",
    "        action_mean = self.actor(state)                     # (1,4)\n",
    "        cov_mat = torch.diag(self.action_var)   # (4,4)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample()                              # (1,4)\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action.float())\n",
    "        memory.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.detach().float()\n",
    "\n",
    "\n",
    "    def evaluate(self, state, action):      # state (4000, 24); action (4000, 4)\n",
    "        state_value = self.critic(state)    # (4000, 1)\n",
    "        \n",
    "        # to calculate action score(logprobs) and distribution entropy\n",
    "        action_mean = self.actor(state)                     # (4000,4)\n",
    "        action_var = self.action_var.expand_as(action_mean) # (4000,4)\n",
    "        cov_mat = torch.diag_embed(action_var)   # (4000,4,4)\n",
    "\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = action.view(-1, 1) \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "\n",
    "class PPOModel:\n",
    "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, restore=False, ckpt=None):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        # current policy\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std)\n",
    "        if restore:\n",
    "            pretained_model = torch.load(ckpt, map_location=lambda storage, loc: storage)\n",
    "            self.policy.load_state_dict(pretained_model)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        \n",
    "        # old policy: initialize old policy with current policy's parameter\n",
    "        self.old_policy = ActorCritic(state_dim, action_dim, action_std)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).float()   # flatten the state\n",
    "        #return self.old_policy.act(state, memory).cpu().numpy().flatten()\n",
    "        return self.old_policy.act(state, memory).cpu().numpy().flatten().astype(float)\n",
    "\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimation of rewards\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = torch.tensor(rewards).float() \n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # Convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(memory.states)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).detach()\n",
    "\n",
    "        # Train policy for K epochs: sampling and updating\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluate old actions and values using current policy\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Importance ratio: p/q\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach()).float()\n",
    "\n",
    "            # Advantages\n",
    "            advantages = rewards - state_values.detach()\n",
    "\n",
    "            # Actor loss using Surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2)\n",
    "    \n",
    "            # Critic loss: critic loss - entropy\n",
    "\n",
    "            critic_loss = 0.5 * self.MSE_loss(rewards, state_values) - 0.01 * dist_entropy\n",
    "    \n",
    "            # Total loss\n",
    "            loss = actor_loss + critic_loss\n",
    "    \n",
    "            # Backward gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        # Copy new weights to old_policy\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "def train(env_name, env, state_dim, action_dim, render, solved_reward,\n",
    "    max_episodes, max_timesteps, update_timestep, action_std, K_epochs, eps_clip,\n",
    "    gamma, lr, betas, ckpt_folder, restore, tb=False, print_interval=10, save_interval=100):\n",
    "\n",
    "    ckpt = ckpt_folder+'/PPO_continuous_'+env_name+'.pth'\n",
    "    if restore:\n",
    "        print('Load checkpoint from {}'.format(ckpt))\n",
    "\n",
    "    memory = Memory()\n",
    "\n",
    "    ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip, restore=restore, ckpt=ckpt)\n",
    "\n",
    "    running_reward, avg_length, time_step = 0, 0, 0\n",
    "\n",
    "    # training loop\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        for t in range(max_timesteps):\n",
    "            time_step += 1\n",
    "\n",
    "            # Run old policy\n",
    "            action = ppo.select_action(state, memory)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                time_step = 0\n",
    "\n",
    "            running_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        avg_length += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "900aae25-5ae7-4b0e-acdf-137fb9882ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAgents(gameData,buyerStrategies,sellerStrategies,disclosure):\n",
    "    buyers, sellers = [], []\n",
    "    for idx,i in enumerate(buyerStrategies):\n",
    "        if i == 'TruthTeller':\n",
    "            buyers.append(TruthTeller(gameData, disclosure, index=idx, buyer=1, reinforcer=0)) \n",
    "        if i == 'ZeroIntelligence':\n",
    "            buyers.append(ZeroIntelligence(gameData, disclosure, index=idx, buyer=1, reinforcer=0)) \n",
    "        if i == 'REINFORCE':\n",
    "            buyers.append(REINFORCE(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'PPO':\n",
    "            buyers.append(PPO(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'SAC':\n",
    "            buyers.append(SAC(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'DQN':\n",
    "            buyers.append(DQN(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "\n",
    "    for idx,i in enumerate(sellerStrategies):\n",
    "        if i == 'TruthTeller':\n",
    "            sellers.append(TruthTeller(gameData, disclosure, index=idx, buyer=0, reinforcer=0)) \n",
    "        if i == 'ZeroIntelligence':\n",
    "            sellers.append(ZeroIntelligence(gameData, disclosure, index=idx, buyer=0, reinforcer=0)) \n",
    "        if i == 'REINFORCE':\n",
    "            sellers.append(REINFORCE(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'PPO':\n",
    "            sellers.append(PPO(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'SAC':\n",
    "            sellers.append(SAC(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'DQN':\n",
    "            sellers.append(DQN(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "    return buyers, sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08302c57-3911-4859-8b8b-533e7597c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db658a5-49c4-4c77-a587-78e8239dab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a34a2e-c5fa-4151-9695-9c031815780c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cb7cb-a859-4023-b285-a524d59a3050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0e464-ab41-4f93-b215-02f82de6747a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16b4e2-b278-4fe5-acfd-eca326cd5861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c8435-5be0-41b6-b8a3-595552ed8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Beta\n",
    "\n",
    "learning_rate  = 0.0003\n",
    "gamma          = 0.9\n",
    "lmbda          = 0.9\n",
    "eps_clip       = 0.2\n",
    "K_epoch        = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 10\n",
    "minibatch_size = 32\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, numStates):\n",
    "        super(PPOModel, self).__init__()\n",
    "        self.numStates = numStates\n",
    "        self.data = []\n",
    "        self.fc1   = nn.Linear(self.numStates,128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.fc_v = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = torch.sigmoid(self.fc_mu(x))\n",
    "        std = 0.4*F.sigmoid(self.fc_std(x))\n",
    "        return mu, std\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition      \n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "        return data_with_adv\n",
    "\n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "                    mu, std = self.pi(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e4d96-bb37-408f-80bd-015bcd25b434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
