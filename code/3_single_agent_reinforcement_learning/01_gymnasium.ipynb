{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23cdbdb",
   "metadata": {},
   "source": [
    "### Gymnasium\n",
    "- API for single agent RL environments\n",
    "- contains common environments (MDPs)\n",
    "- 5 functions: \n",
    "    - make: creates the environment, inbuilt or custom\n",
    "    - reset: resets the game to the initial state. (e.g. Chess at the start)\n",
    "    - step: given the state, playing an action, observing the reward and transitioning to new state\n",
    "        - begins from initial state, until termination / truncation i.e game completion\n",
    "        - then reset() moves back to initial state\n",
    "    - render: how env should be visualized\n",
    "    - close: delete memory\n",
    "- Environment objects: expected input and output\n",
    "    - action spaces: \n",
    "        - types: Box (n-d continous, bounded), Discrete (0,1...N), Dict, Tuple, MultiBinary, Multidiscrete \n",
    "        - .sample() gets you a random action\n",
    "    - observation space (observed state)\n",
    "        - types: Box (n-d continous, bounded), Discrete (0,1...N), Dict, Tuple, MultiBinary, Multidiscrete \n",
    "    - reward_range\n",
    "- wrappers: modifies state. \n",
    "    - FlattenObservation, TimeLimit (to steps), ClipAction, RescaleAction, TimeAwareObs (ensure Markov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a373673",
   "metadata": {},
   "source": [
    "### Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194db063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539ebd6",
   "metadata": {},
   "source": [
    "#### Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d60202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2324599",
   "metadata": {},
   "source": [
    "#### Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305291a1",
   "metadata": {},
   "source": [
    "#### Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa7efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62db2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving logs to visulise in Tensorboard, saving models\n",
    "models_dir = f\"models/Mountain-{time.time()}\"\n",
    "logdir = f\"logs/Mountain-{time.time()}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe86c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Parallel environments\n",
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1)\n",
    "\n",
    "# The learning agent and hyperparameters\n",
    "model = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=256,\n",
    "    ent_coef=0.00429,\n",
    "    learning_rate=7.77e-05,\n",
    "    n_epochs=10,\n",
    "    n_steps=8,\n",
    "    gae_lambda=0.9,\n",
    "    gamma=0.9999,\n",
    "    clip_range=0.1,\n",
    "    max_grad_norm =5,\n",
    "    vf_coef=0.19,\n",
    "    use_sde=True,\n",
    "    policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a76adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/Mountain-1694368241.423321/PPO_0\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 385 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 8   |\n",
      "----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 145          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 16           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.599592e-07 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | -0.158       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00516      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 5.52e-05     |\n",
      "|    std                  | 0.0373       |\n",
      "|    value_loss           | 4.43e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 24            |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047791004 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.39          |\n",
      "|    explained_variance   | -2.11         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00601       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 5.08e-05      |\n",
      "|    std                  | 0.0373        |\n",
      "|    value_loss           | 6.46e-06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 32           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017271712 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | -0.624       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00497      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    std                  | 0.0372       |\n",
      "|    value_loss           | 2.86e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 40            |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074899197 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.41          |\n",
      "|    explained_variance   | -8.55         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00629       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 0.000193      |\n",
      "|    std                  | 0.0372        |\n",
      "|    value_loss           | 5.01e-06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 48           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007907748 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.41         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00584      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0001      |\n",
      "|    std                  | 0.0373       |\n",
      "|    value_loss           | 4.16e-08     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 56          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004907176 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00499     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    std                  | 0.0373      |\n",
      "|    value_loss           | 2.76e-05    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 117           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 64            |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040002167 |\n",
      "|    clip_fraction        | 0.1           |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.39          |\n",
      "|    explained_variance   | 0.275         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00281       |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.0013       |\n",
      "|    std                  | 0.0373        |\n",
      "|    value_loss           | 3.05e-05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 118          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 72           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023679435 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | 0.193        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00495      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000529    |\n",
      "|    std                  | 0.0373       |\n",
      "|    value_loss           | 1.77e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 80           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059157163 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | -0.0601      |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00803      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    std                  | 0.0373       |\n",
      "|    value_loss           | 0.000126     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 88            |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0487776e-05 |\n",
      "|    clip_fraction        | 0.1           |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.39          |\n",
      "|    explained_variance   | -0.232        |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00331       |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000983     |\n",
      "|    std                  | 0.0373        |\n",
      "|    value_loss           | 7.84e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 96          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005145505 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | -11.8       |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00751     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.000172    |\n",
      "|    std                  | 0.0373      |\n",
      "|    value_loss           | 2.34e-06    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 104           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1915646e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.41          |\n",
      "|    explained_variance   | -0.366        |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00623       |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | 0.000128      |\n",
      "|    std                  | 0.0373        |\n",
      "|    value_loss           | 2.76e-06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 112          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035731941 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.41         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00518      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000324    |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 4.57e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 121         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 120         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003563054 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | -0.0762     |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | -0.00149    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 3.99e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 128          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026894212 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | 0.276        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.000656     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 2.98e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 136          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010929331 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.000504     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000334    |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 4.17e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 144          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033248663 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | -0.0793      |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00582      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -3.5e-05     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 1.82e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 152          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021432638 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | -0.567       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00317      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000737    |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 2.2e-05      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 116           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 160           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.6307526e-05 |\n",
      "|    clip_fraction        | 0.2           |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.39          |\n",
      "|    explained_variance   | -0.451        |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00244       |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | 0.00114       |\n",
      "|    std                  | 0.0374        |\n",
      "|    value_loss           | 4.06e-05      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 113            |\n",
      "|    iterations           | 21             |\n",
      "|    time_elapsed         | 1              |\n",
      "|    total_timesteps      | 168            |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000109642744 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | 1.41           |\n",
      "|    explained_variance   | -1.37          |\n",
      "|    learning_rate        | 7.77e-05       |\n",
      "|    loss                 | 0.00109        |\n",
      "|    n_updates            | 200            |\n",
      "|    policy_gradient_loss | -0.00216       |\n",
      "|    std                  | 0.0374         |\n",
      "|    value_loss           | 8.79e-06       |\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 176         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003798671 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.42        |\n",
      "|    explained_variance   | -0.0832     |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00569     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 3.11e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 184         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004424922 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.41        |\n",
      "|    explained_variance   | -2.74       |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00455     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.000437    |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 6.38e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 111          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 192          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005430132 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | -0.561       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.0028      |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00367     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 1.96e-06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 110           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 200           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025408715 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.38          |\n",
      "|    explained_variance   | 0.943         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | -0.00494      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.0053       |\n",
      "|    std                  | 0.0374        |\n",
      "|    value_loss           | 1.82e-08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 110           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 208           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032803416 |\n",
      "|    clip_fraction        | 0.1           |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.36          |\n",
      "|    explained_variance   | 0.0566        |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.000164      |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00251      |\n",
      "|    std                  | 0.0374        |\n",
      "|    value_loss           | 0.000225      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 110         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 216         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004476741 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.36        |\n",
      "|    explained_variance   | -0.082      |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00612     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.000804    |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 0.00025     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 109          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 224          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018929541 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | -0.229       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.0016      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00409     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 6.98e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 108         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 232         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008824185 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | -0.792      |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.0149      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    std                  | 0.0374      |\n",
      "|    value_loss           | 5.26e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 108          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 240          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015979335 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | -0.0679      |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00747      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | 0.00116      |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 3.91e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 107           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 248           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2187829e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.42          |\n",
      "|    explained_variance   | 0.159         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00526       |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000393     |\n",
      "|    std                  | 0.0374        |\n",
      "|    value_loss           | 6.67e-06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 107           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021189451 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.41          |\n",
      "|    explained_variance   | 0.194         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00506       |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000132     |\n",
      "|    std                  | 0.0374        |\n",
      "|    value_loss           | 1.37e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 107          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 264          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018309355 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.39         |\n",
      "|    explained_variance   | 0.0998       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    std                  | 0.0374       |\n",
      "|    value_loss           | 7.42e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 106          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 272          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.063631e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | 0.0871       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00698      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.000751     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 2.77e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 280         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000500232 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.36        |\n",
      "|    explained_variance   | -0.00713    |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00586     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 1.08e-05    |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 0.000156    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 105          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 288          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.937151e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | -0.0298      |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00554      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000144    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 3.07e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 104          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 296          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.137656e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.39         |\n",
      "|    explained_variance   | 0.036        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00571      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000132    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 1.19e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 104          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 304          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069932267 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.41         |\n",
      "|    explained_variance   | 0.0649       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000207    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 2.32e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 103          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 312          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.839633e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00602      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -5.78e-05    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 3.09e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 102          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 320          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030259192 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00432      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 1.25e-07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 328         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002313532 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.4         |\n",
      "|    explained_variance   | -0.0709     |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | -0.00211    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 5.76e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 336         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001293771 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.38        |\n",
      "|    explained_variance   | -0.161      |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00347     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.000894   |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 2.3e-05     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 344          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026135445 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | -0.0361      |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.0055       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000203    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 8.97e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 352          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035120696 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.36         |\n",
      "|    explained_variance   | 0.0176       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00553      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.000123    |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 0.000194     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 360          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.374585e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00245      |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 0.000134     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 368          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037855208 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | 0.248        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.00152     |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 5.77e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 376          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002721101 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | 0.217        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.000807     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 4.47e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 99            |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075124204 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.43          |\n",
      "|    explained_variance   | -0.0276       |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00635       |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | 0.000132      |\n",
      "|    std                  | 0.0375        |\n",
      "|    value_loss           | 2.31e-05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 392         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005448386 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.42        |\n",
      "|    explained_variance   | -5.7        |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.0035      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 5.55e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 400          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027465522 |\n",
      "|    clip_fraction        | 0.163        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | -0.19        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.00841     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 0.000384     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 408          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029684752 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.37         |\n",
      "|    explained_variance   | -0.211       |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.0015      |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    std                  | 0.0375       |\n",
      "|    value_loss           | 0.000211     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 100           |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 416           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024235249 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.36          |\n",
      "|    explained_variance   | -0.216        |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00666       |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | 0.000534      |\n",
      "|    std                  | 0.0375        |\n",
      "|    value_loss           | 1.87e-05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 424         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006169498 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 1.36        |\n",
      "|    explained_variance   | 0.0659      |\n",
      "|    learning_rate        | 7.77e-05    |\n",
      "|    loss                 | 0.00535     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    std                  | 0.0375      |\n",
      "|    value_loss           | 0.000786    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 432          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003886968 |\n",
      "|    clip_fraction        | 0.3          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00506      |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 0.00296      |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 2.33e-06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 440          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015937537 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.0047       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | 0.000167     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 1.73e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 448          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.834766e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00584      |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | 0.000109     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 1.2e-05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 456          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026856884 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.42         |\n",
      "|    explained_variance   | -1.56        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00519      |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.000602    |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 1.18e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 464          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044120103 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.4          |\n",
      "|    explained_variance   | -3.23        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.00619     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00789     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 5.18e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 472          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011390746 |\n",
      "|    clip_fraction        | 0.3          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.38         |\n",
      "|    explained_variance   | -10          |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.0174      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 1.91e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 480          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.863475e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.35         |\n",
      "|    explained_variance   | -0.19        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00869      |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | 0.00227      |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 0.000371     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 488          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021771342 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.35         |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00556      |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000106    |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 7.97e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 101           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 496           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6866794e-05 |\n",
      "|    clip_fraction        | 0.163         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.36          |\n",
      "|    explained_variance   | 0.747         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.0031        |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | -0.00158      |\n",
      "|    std                  | 0.0376        |\n",
      "|    value_loss           | 6.96e-06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 504          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016322881 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.39         |\n",
      "|    explained_variance   | 0.528        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00473      |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | 1.63e-05     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 7.56e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046690106 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.41         |\n",
      "|    explained_variance   | 0.426        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | -0.00152     |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 0.000132     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 520          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019071847 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 1.43         |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 7.77e-05     |\n",
      "|    loss                 | 0.00475      |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000705    |\n",
      "|    std                  | 0.0376       |\n",
      "|    value_loss           | 2.36e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 100           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 528           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021889806 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 1.43          |\n",
      "|    explained_variance   | -7.38         |\n",
      "|    learning_rate        | 7.77e-05      |\n",
      "|    loss                 | 0.00554       |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -0.000272     |\n",
      "|    std                  | 0.0376        |\n",
      "|    value_loss           | 2.44e-05      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIMESTEPS\u001b[38;5;241m*\u001b[39mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 272\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    274\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training and saving models along the way\n",
    "TIMESTEPS = 20000\n",
    "for i in range(10): \n",
    "    model.learn(total_timesteps=TIMESTEPS,reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15655f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Discrete(3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    133\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m--> 134\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes)], loss)\n\u001b[1;32m    136\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn [7], line 107\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(episode)\u001b[0m\n\u001b[1;32m    105\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m    106\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m--> 107\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, episode, score))\n",
      "Cell \u001b[0;32mIn [7], line 68\u001b[0m, in \u001b[0;36mDQN.replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m next_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(next_states)\n\u001b[1;32m     67\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict_on_batch(next_states), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdones)\n\u001b[0;32m---> 68\u001b[0m targets_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)])\n\u001b[1;32m     71\u001b[0m targets_full[[ind], [actions]] \u001b[38;5;241m=\u001b[39m targets\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/engine/training.py:2567\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2565\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_on_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2567\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_predict_function()\n\u001b[1;32m   2571\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(iterator)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/engine/data_adapter.py:1820\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle_batch_iterator\u001b[39m(\n\u001b[1;32m   1817\u001b[0m     strategy, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m ):\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a single-batch dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1820\u001b[0m     x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1822\u001b[0m         data \u001b[38;5;241m=\u001b[39m (x,)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/engine/data_adapter.py:1139\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1139\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/engine/data_adapter.py:1134\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1133\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "#env.seed(110)\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = .95\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.lr = 0.001\n",
    "        self.epsilon_decay = .995\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_space, activation=relu))\n",
    "        model.add(Dense(25, activation=relu))\n",
    "        model.add(Dense(self.action_space, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def get_reward(state):\n",
    "    if state[0] >= 0.5:\n",
    "        print(\"Car has reached the goal\")\n",
    "        return 10\n",
    "    if state[0] > -0.4:\n",
    "        return (1+state[0])**2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def train_dqn(episode):\n",
    "    loss = []\n",
    "    agent = DQN(env.action_space.n, env.observation_space.shape[0])\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, (1, 2))\n",
    "        score = 0\n",
    "        max_steps = 1000\n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            env.render()\n",
    "            #print(env.step(action))\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = get_reward(next_state)\n",
    "            score += reward\n",
    "            next_state = np.reshape(next_state, (1, 2))\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        loss.append(score)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def random_policy(episode, step):\n",
    "    for i_episode in range(episode):\n",
    "        env.reset()\n",
    "        for t in range(step):\n",
    "            env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "            print(\"Starting next episode\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(env.observation_space)\n",
    "    print(env.action_space)\n",
    "    episodes = 60\n",
    "    loss = train_dqn(episodes)\n",
    "    plt.plot([i+1 for i in range(episodes)], loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7276c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:07] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Env Space Information:                                                                                                   <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#236\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Env Space Information:                                                                                                   \u001b]8;id=403958;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=933488;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#236\u001b\\\u001b[2m236\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:07] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>         Observation Space: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Box</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.1415927</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-5</span>.                               <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#237\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">237</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0</span>.       <span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1415927</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.       <span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,<span style=\"font-weight: bold\">)</span>, float32<span style=\"font-weight: bold\">)</span>                                                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         Observation Space: \u001b[1;35mBox\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.5\u001b[0m       \u001b[1;36m-1.5\u001b[0m       \u001b[1;36m-5\u001b[0m.        \u001b[1;36m-5\u001b[0m.        \u001b[1;36m-3.1415927\u001b[0m \u001b[1;36m-5\u001b[0m.                               \u001b]8;id=441001;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=271493;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#237\u001b\\\u001b[2m237\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m          \u001b[1;36m-0\u001b[0m.        \u001b[1;36m-0\u001b[0m.       \u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m1.5\u001b[0m       \u001b[1;36m1.5\u001b[0m       \u001b[1;36m5\u001b[0m.        \u001b[1;36m5\u001b[0m.        \u001b[1;36m3.1415927\u001b[0m \u001b[1;36m5\u001b[0m.        \u001b[1;36m1\u001b[0m.                                 \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m          \u001b[1;36m1\u001b[0m.       \u001b[1m]\u001b[0m, \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m,\u001b[1m)\u001b[0m, float32\u001b[1m)\u001b[0m                                                                                              \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:07] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>         Action Space: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Discrete</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>                                                                                        <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#238\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         Action Space: \u001b[1;35mDiscrete\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m                                                                                        \u001b]8;id=536110;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=424604;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#238\u001b\\\u001b[2m238\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:07] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span>         Reward Space: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Box</span><span style=\"font-weight: bold\">(</span>-inf, inf, <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"font-weight: bold\">)</span>, float32<span style=\"font-weight: bold\">)</span>                                                                      <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#239\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m         Reward Space: \u001b[1;35mBox\u001b[0m\u001b[1m(\u001b[0m-inf, inf, \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m,\u001b[1m)\u001b[0m, float32\u001b[1m)\u001b[0m                                                                      \u001b]8;id=962838;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=870163;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#239\u001b\\\u001b[2m239\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n",
      "/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/pranjal/Desktop/AI_Double_Auctions/code/3_deep_reinforcement_learning/lunarlander_dqn_eval/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  def __init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=318046;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=375441;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=611720;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=934973;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=952225;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=229053;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=529202;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146039;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=295528;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146534;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=792518;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99437;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=648406;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=838234;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> please don't reset a unfinished env when you enable save replay, we just skip it                                         <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m please don't reset a unfinished env when you enable save replay, we just skip it                                         \u001b]8;id=262674;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=953938;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#287\u001b\\\u001b[2m287\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09-20 06:30:08] </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Env <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> step has exceeded max <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">retries</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>                                                                                   <a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base_env_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#375\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">375</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09-20 06:30:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Env \u001b[1;36m0\u001b[0m step has exceeded max \u001b[1;35mretries\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m                                                                                   \u001b]8;id=558433;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py\u001b\\\u001b[2mbase_env_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=849574;file:///usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py#375\u001b\\\u001b[2m375\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Env 0 step has exceeded max retries(1), and the latest exception is: too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     evaluator\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 60\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfg, seed)\u001b[0m\n\u001b[1;32m     56\u001b[0m tb_logger \u001b[38;5;241m=\u001b[39m SummaryWriter(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/log/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cfg\u001b[38;5;241m.\u001b[39mexp_name), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserial\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     57\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m InteractionSerialEvaluator(\n\u001b[1;32m     58\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39meval\u001b[38;5;241m.\u001b[39mevaluator, evaluator_env, policy\u001b[38;5;241m.\u001b[39meval_mode, tb_logger, exp_name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mexp_name\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/worker/collector/interaction_serial_evaluator.py:239\u001b[0m, in \u001b[0;36mInteractionSerialEvaluator.eval\u001b[0;34m(self, save_ckpt_fn, train_iter, envstep, n_episode, force_render, policy_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m actions \u001b[38;5;241m=\u001b[39m {i: a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m policy_output\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    238\u001b[0m actions \u001b[38;5;241m=\u001b[39m to_ndarray(actions)\n\u001b[0;32m--> 239\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m to_tensor(timesteps, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_id, t \u001b[38;5;129;01min\u001b[39;00m timesteps\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py:348\u001b[0m, in \u001b[0;36mBaseEnvManager.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    346\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_id, act \u001b[38;5;129;01min\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 348\u001b[0m     timesteps[env_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timesteps[env_id]\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_episode_count[env_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py:382\u001b[0m, in \u001b[0;36mBaseEnvManager._step\u001b[0;34m(self, env_id, act)\u001b[0m\n\u001b[1;32m    376\u001b[0m runtime_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m step has exceeded max retries(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m), and the latest exception is: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    378\u001b[0m         env_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_retry, \u001b[38;5;28mstr\u001b[39m(exceptions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m runtime_error\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;241m=\u001b[39m exceptions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m runtime_error\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py:371\u001b[0m, in \u001b[0;36mBaseEnvManager._step\u001b[0;34m(self, env_id, act)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_retry):\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    373\u001b[0m         exceptions\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env_manager/base_env_manager.py:366\u001b[0m, in \u001b[0;36mBaseEnvManager._step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;129m@timeout_wrapper\u001b[39m(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_timeout)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m():\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_envs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env/ding_env_wrapper.py:126\u001b[0m, in \u001b[0;36mDingEnvWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cfg\u001b[38;5;241m.\u001b[39mact_scale:\n\u001b[1;32m    125\u001b[0m     action \u001b[38;5;241m=\u001b[39m affine_transform(action, min_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, max_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 126\u001b[0m obs, rew, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    128\u001b[0m     obs \u001b[38;5;241m=\u001b[39m to_ndarray(obs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gym/wrappers/record_video.py:86\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix \u001b[38;5;241m=\u001b[39m name_prefix\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length \u001b[38;5;241m=\u001b[39m video_length\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gym/core.py:280\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the base non-wrapped environment.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;124;03m        Env: The base non-wrapped gym.Env instance\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ding/envs/env_wrappers/env_wrappers.py:367\u001b[0m, in \u001b[0;36mEvalEpisodeReturnEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 367\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_episode_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTimeLimit\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mWrapper):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124;03m\"\"\"This wrapper will issue a `truncated` signal if a maximum number of timesteps is exceeded.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    If a truncation is not defined inside the environment itself, this is the only place that the truncation signal is issued.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Critically, this is different from the `terminated` signal that originates from the underlying environment as part of the MDP.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    (deprecated)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    This information is passed through ``info`` that is returned when `done`-signal was issued.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    The done-signal originates from the time limit (i.e. it signifies a *truncation*) if and only if\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;124;03m    the key `\"TimeLimit.truncated\"` exists in ``info`` and the corresponding value is ``True``. This will be removed in favour\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    of only issuing a `truncated` signal in future versions.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m       >>> from gym.envs.classic_control import CartPoleEnv\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m       >>> from gym.wrappers import TimeLimit\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m       >>> env = CartPoleEnv()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m       >>> env = TimeLimit(env, max_episode_steps=1000)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m         env: gym\u001b[38;5;241m.\u001b[39mEnv,\n\u001b[1;32m     30\u001b[0m         max_episode_steps: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m         new_step_api: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m     ):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124;03m\"\"\"Initializes the :class:`TimeLimit` wrapper with an environment and the number of steps after which truncation will occur.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m            new_step_api (bool): Whether the wrapper's step method outputs two booleans (new API) or one boolean (old API)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOrderEnforcing\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mWrapper):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124;03m\"\"\"A wrapper that will produce an error if :meth:`step` is called before an initial :meth:`reset`.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        >>> from gym.envs.classic_control import CartPoleEnv\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        >>> env = CartPoleEnv()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        >>> env = OrderEnforcing(env)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;124;03m        >>> env.step(0)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        ResetNeeded: Cannot call env.step() before calling env.reset()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m        >>> env.render()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        ResetNeeded: Cannot call env.render() before calling env.reset()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        >>> env.reset()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m        >>> env.render()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m        >>> env.step(0)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: gym\u001b[38;5;241m.\u001b[39mEnv, disable_render_order_enforcing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124;03m\"\"\"A wrapper that will produce an error if :meth:`step` is called before an initial :meth:`reset`.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m            env: The environment to wrap\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m            disable_render_order_enforcing: If to disable render order enforcing\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Env 0 step has exceeded max retries(1), and the latest exception is: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from easydict import EasyDict\n",
    "\n",
    "from ding.config import compile_config\n",
    "from ding.worker import BaseLearner, SampleSerialCollector, InteractionSerialEvaluator, AdvancedReplayBuffer\n",
    "from ding.envs import BaseEnvManager, DingEnvWrapper\n",
    "from ding.policy import DQNPolicy\n",
    "from ding.model import DQN\n",
    "from ding.utils import set_pkg_seed\n",
    "from ding.rl_utils import get_epsilon_greedy_fn\n",
    "from dizoo.box2d.lunarlander.config.lunarlander_dqn_config import main_config, create_config\n",
    "\n",
    "# Get DI-engine form env class\n",
    "def wrapped_cartpole_env():\n",
    "    return DingEnvWrapper(\n",
    "        gym.make(main_config['env']['env_id']),\n",
    "        EasyDict(env_wrapper='default'),\n",
    "    )\n",
    "\n",
    "\n",
    "def main(cfg, seed=0):\n",
    "    cfg['exp_name'] = 'lunarlander_dqn_eval'\n",
    "    cfg = compile_config(\n",
    "        cfg,\n",
    "        BaseEnvManager,\n",
    "        DQNPolicy,\n",
    "        BaseLearner,\n",
    "        SampleSerialCollector,\n",
    "        InteractionSerialEvaluator,\n",
    "        AdvancedReplayBuffer,\n",
    "        save_cfg=True\n",
    "    )\n",
    "    cfg.policy.load_path = './final.pth.tar'\n",
    "\n",
    "    # build multiple environments and use env_manager to manage them\n",
    "    evaluator_env_num = cfg.env.evaluator_env_num\n",
    "    evaluator_env = BaseEnvManager(env_fn=[wrapped_cartpole_env for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n",
    "\n",
    "    # switch save replay interface\n",
    "    # evaluator_env.enable_save_replay(cfg.env.replay_path)\n",
    "    evaluator_env.enable_save_replay(replay_path='./lunarlander_dqn_eval/video')\n",
    "\n",
    "    # Set random seed for all package and instance\n",
    "    evaluator_env.seed(seed, dynamic_seed=False)\n",
    "    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n",
    "\n",
    "    # Set up RL Policy\n",
    "    model = DQN(**cfg.policy.model)\n",
    "    policy = DQNPolicy(cfg.policy, model=model)\n",
    "    policy.eval_mode.load_state_dict(torch.load(cfg.policy.load_path, map_location='cpu'))\n",
    "\n",
    "    # Evaluate\n",
    "    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n",
    "    evaluator = InteractionSerialEvaluator(\n",
    "        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name\n",
    "    )\n",
    "    evaluator.eval()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(main_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16c8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
