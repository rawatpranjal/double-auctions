{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2298e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1e1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "buyer_strategies = ['Honest']\n",
    "seller_strategies = ['Honest','Honest','Honest','Honest','Honest','Honest','Honest','Honest']\n",
    "nbuyers, nsellers = len(buyer_strategies), len(seller_strategies)\n",
    "nrounds, nperiods, ntokens, nsteps, gametype, nbuyers, nsellers = 1, 10000, 10, 10, '1234', len(buyer_strategies), len(seller_strategies)\n",
    "R1, R2, R3, R4 = gametype_to_ran(gametype)\n",
    "game_metadata = [nrounds, nperiods, ntokens, nbuyers, nsellers, nsteps, R1, R2, R3, R4]\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "rnd = 0\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "period = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac652b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<functions.Honest at 0x11e836590>,\n",
       " <functions.Honest at 0x11e835540>,\n",
       " <functions.Honest at 0x11e835510>,\n",
       " <functions.Honest at 0x11e8355a0>,\n",
       " <functions.Honest at 0x11e8354b0>,\n",
       " <functions.Honest at 0x11e835750>,\n",
       " <functions.Honest at 0x11e8344f0>,\n",
       " <functions.Honest at 0x11e835570>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304478f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, db, nsteps, render_mode = None):\n",
    "        self.db = db\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float)\n",
    "        self.observation_space = spaces.Box(low=0, high=nsteps, shape=(1,), dtype=int)\n",
    "\n",
    "    def reset(self):\n",
    "        self.db.reset_period(rnd)\n",
    "        observation = np.array([0])\n",
    "        return observation, None\n",
    "\n",
    "    def step(self, action, timestep, seed = None, options = None):\n",
    "        # convert action to bid\n",
    "        self.db.buyers[0].next_token()\n",
    "        min_bid = self.db.buyers[0].value*0.6\n",
    "        max_bid = self.db.buyers[0].value*1.4\n",
    "        bid = min_bid * action.item() + (1-action.item())*max_bid\n",
    "        \n",
    "        # simulate market\n",
    "        bids = [bid]\n",
    "        asks = [seller.ask(self.db) for seller in self.db.sellers]\n",
    "        current_ask, current_ask_idx, current_bid, current_bid_idx = current_bid_ask(bids, asks) \n",
    "        sale, price, bprofit, sprofit, buy, sell = buy_sell(db, current_bid, current_bid_idx, current_ask, current_ask_idx)\n",
    "        step_data = [rnd,period,timestep,bids,asks,current_bid,current_bid_idx,current_ask,current_ask_idx,buy,sell,price,sale,bprofit,sprofit]\n",
    "        self.db.add_step(step_data)\n",
    "        \n",
    "        # compute reward, new state\n",
    "        reward = 0\n",
    "        if (sale == 1) and (current_bid_idx == 0):\n",
    "            reward = bprofit\n",
    "        observation = np.array([timestep + 1])\n",
    "        \n",
    "        # check termination \n",
    "        if timestep == nsteps-1:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        return observation, reward, terminated, False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2655b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 0, New State: 1, Action:0.0, Reward: 26.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 2, Action:0.8, Reward: 55.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 3, Action:0.3, Reward: 26.8, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 4, Action:0.7, Reward: 41.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 5, Action:0.2, Reward: 12.5, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 6, Action:0.6, Reward: 18.9, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 7, Action:1.0, Reward: 21.7, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 8, Action:0.6, Reward: 11.8, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 9, Action:1.0, Reward: 14.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 10, Action:0.9, Reward: 0, Period End: True\n",
      "done\n",
      "Rnd: 0, Period: 0, New State: 1, Action:0.6, Reward: 49.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 2, Action:0.4, Reward: 39.7, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 3, Action:0.0, Reward: 19.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 4, Action:0.2, Reward: 23.6, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 5, Action:0.6, Reward: 20.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 6, Action:0.5, Reward: 17.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 7, Action:0.1, Reward: 5.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 8, Action:0.9, Reward: 16.2, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 9, Action:0.2, Reward: 3.1, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 10, Action:0.0, Reward: 0, Period End: True\n",
      "done\n",
      "Rnd: 0, Period: 0, New State: 1, Action:0.6, Reward: 51.4, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 2, Action:0.1, Reward: 27.7, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 3, Action:0.6, Reward: 38.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 4, Action:1.0, Reward: 49.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 5, Action:0.4, Reward: 16.8, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 6, Action:0.1, Reward: 8.7, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 7, Action:0.4, Reward: 11.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 8, Action:0.2, Reward: 4.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 9, Action:0.2, Reward: 2.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: 10, Action:1.0, Reward: 0, Period End: True\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnv(db, nsteps)\n",
    "observation, info = env.reset()\n",
    "timestep = 0\n",
    "\n",
    "for _ in range(30): \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info, reset_info = env.step(action, timestep)\n",
    "    print(f\"Rnd: {rnd}, Period: {period}, New State: {observation.item()}, Action:{np.round(action.item(),1)}, Reward: {np.round(reward,1)}, Period End: {done}\")\n",
    "\n",
    "    if done:\n",
    "        # If the episode is done, reset the environment\n",
    "        print('done')\n",
    "        observation, info = env.reset()\n",
    "        timestep = 0\n",
    "    else:\n",
    "        timestep += 1\n",
    "\n",
    "# Close the environment when done\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44f7ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "import time\n",
    "# Saving logs to visulise in Tensorboard, saving models\n",
    "models_dir = f\"models/Mountain-{time.time()}\"\n",
    "logdir = f\"logs/Mountain-{time.time()}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "env = make_vec_env(\"MountainCarContinuous-v0\", n_envs=1)\n",
    "\n",
    "# The learning agent and hyperparameters\n",
    "model = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=256,\n",
    "    ent_coef=0.00429,\n",
    "    learning_rate=7.77e-05,\n",
    "    n_epochs=10,\n",
    "    n_steps=8,\n",
    "    gae_lambda=0.9,\n",
    "    gamma=0.9999,\n",
    "    clip_range=0.1,\n",
    "    max_grad_norm =5,\n",
    "    vf_coef=0.19,\n",
    "    use_sde=True,\n",
    "    policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
