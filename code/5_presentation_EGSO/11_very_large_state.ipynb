{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from itertools import count\n",
    "buyer_strategies = ['Honest', 'Random', 'Random']\n",
    "seller_strategies = ['Random', 'Random', 'Random']\n",
    "nbuyers, nsellers = len(buyer_strategies), len(seller_strategies)\n",
    "nrounds, nperiods, ntokens, nsteps, gametype, nbuyers, nsellers = 10, 10, 8, 50, '1234', len(buyer_strategies), len(seller_strategies)\n",
    "R1, R2, R3, R4 = gametype_to_ran(gametype)\n",
    "game_metadata = [nrounds, nperiods, ntokens, nbuyers, nsellers, nsteps, R1, R2, R3, R4]\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "rnd = 0\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aeeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, db, nsteps, render_mode = None):\n",
    "        self.rnd = 0\n",
    "        self.period = -1\n",
    "        self.nperiods = nperiods\n",
    "        self.db = db\n",
    "        self.action_space = spaces.Box(0,1,(1,),dtype=np.float)\n",
    "        self.observation_space = spaces.Box(-1,200,(13,),dtype=np.float32)\n",
    "\n",
    "    def reset(self,seed=None):\n",
    "        #self.db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "        self.db.reset_period(self.rnd)\n",
    "        self.timestep = 0\n",
    "        self.period += 1\n",
    "        self.db.buyers[0].next_token()\n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([0,-1,-1,-1,-1,-1,-1,-1,agent.value,-1,-1,-1,agent.num_tokens_traded], dtype = np.float32)\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action, seed=None, options=None):\n",
    "        [buyer.next_token() for buyer in self.db.buyers]\n",
    "        [seller.next_token() for seller in self.db.sellers]\n",
    "        bid_frac = action.item()\n",
    "        # convert action to bid\n",
    "        self.db.buyers[0].next_token()\n",
    "        min_bid = self.db.buyers[0].value * min_frac\n",
    "        max_bid = self.db.buyers[0].value * max_frac\n",
    "        bid = np.round(max_bid * bid_frac + (1 - bid_frac) * min_bid, 2)\n",
    "\n",
    "        # simulate market\n",
    "        bids = [buyer.bid(self.db) for buyer in self.db.buyers]\n",
    "        bids[0] = bid\n",
    "        asks = [seller.ask(self.db) for seller in self.db.sellers]\n",
    "        current_ask, current_ask_idx, current_bid, current_bid_idx = current_bid_ask(bids, asks)\n",
    "        sale, price, bprofit, sprofit, buy, sell = buy_sell(self.db, current_bid, current_bid_idx, current_ask, current_ask_idx)\n",
    "        step_data = [self.rnd, self.period, self.timestep, bids, asks, current_bid, current_bid_idx, current_ask, current_ask_idx, buy, sell, price, sale, bprofit, sprofit]\n",
    "        self.db.add_step(step_data)\n",
    "\n",
    "        # compute reward, new state\n",
    "        reward = 0.0\n",
    "        if sale == 1 and current_bid_idx == 0:\n",
    "            reward = bprofit\n",
    "            \n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([self.timestep + 1, current_ask, current_ask_idx, current_bid, current_bid_idx,\n",
    "                                sale, price, buy, sell, agent.value, agent.step_profit,\n",
    "                                agent.sale, agent.num_tokens_traded],dtype = np.float32)\n",
    "        idx = np.isnan(observation)\n",
    "        observation[idx] = -1.0\n",
    "        # check termination\n",
    "        self.timestep += 1\n",
    "        if self.timestep == nsteps:\n",
    "            terminated = True\n",
    "            self.timestep = 0\n",
    "        else:\n",
    "            terminated = False\n",
    "        infos = {\"TimeLimit.truncated\":True}\n",
    "        truncated = False\n",
    "        return observation, reward, terminated, truncated, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = 0\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "observation, info = env.reset()\n",
    "for period in count():\n",
    "    for timestep in count(): \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        print(f\"Rnd: {rnd}, Period: {period}, New State: {observation}, Action:{np.round(action,1)}, Reward: {np.round(reward,1)}, Period End: {done}\")\n",
    "        if done:\n",
    "            # If the episode is done, reset the environment\n",
    "            #print('done')\n",
    "            observation, info = env.reset()\n",
    "            break\n",
    "    if period == nperiods:\n",
    "        period = 0\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(1000).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b34a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your environment and parameters (replace with your actual environment setup)\n",
    "rnd = 0\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.5\n",
    "eval_steps = 1000\n",
    "training_step = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29040eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad71ec",
   "metadata": {},
   "source": [
    "### Continous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856dcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, DDPG, TD3, A2C, PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "model = SAC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.round_data.redemption_values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf995d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9122b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_period(env.db, 0, 2236)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eaf961",
   "metadata": {},
   "source": [
    "### Discrete Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "env.action_space = spaces.Discrete(51)\n",
    "#policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1,)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98f33",
   "metadata": {},
   "source": [
    "## ON POLICY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75639662",
   "metadata": {},
   "source": [
    "### DDPG - Deterministic Deep Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acec3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a34e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfc29c",
   "metadata": {},
   "source": [
    "### PPO - Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbafb0",
   "metadata": {},
   "source": [
    "### A2C - Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc92acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A2C model\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "a2c_model = A2C(MlpPolicy, env, verbose=0)\n",
    "\n",
    "# Train the A2C agent for 10000 steps\n",
    "a2c_model.learn(total_timesteps=training_step, progress_bar = True)\n",
    "\n",
    "# Evaluate the trained A2C agent\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model, env, n_eval_episodes=eval_steps)\n",
    "print(f\"A2C mean_reward: {mean_reward_a2c:.2f} +/- {std_reward_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e53392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
