{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce0ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from itertools import count\n",
    "buyer_strategies = ['Honest', 'Random', 'Random', 'Random']\n",
    "seller_strategies = ['Honest', 'Honest', 'Honest', 'Honest', 'Honest','Honest', 'Honest', 'Honest', 'Honest', 'Honest']\n",
    "nbuyers, nsellers = len(buyer_strategies), len(seller_strategies)\n",
    "nrounds, nperiods, ntokens, nsteps, gametype, nbuyers, nsellers = 10, 10, 8, 50, '1234', len(buyer_strategies), len(seller_strategies)\n",
    "R1, R2, R3, R4 = gametype_to_ran(gametype)\n",
    "game_metadata = [nrounds, nperiods, ntokens, nbuyers, nsellers, nsteps, R1, R2, R3, R4]\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "rnd = 0\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aeeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, db, nsteps, render_mode = None):\n",
    "        self.rnd = 0\n",
    "        self.period = -1\n",
    "        self.nperiods = nperiods\n",
    "        self.db = db\n",
    "        self.action_space = spaces.Box(0,1,(1,),dtype=np.float)\n",
    "        self.observation_space = spaces.Box(-1,200,(13,),dtype=np.float32)\n",
    "\n",
    "    def reset(self,seed=None):\n",
    "        #self.db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "        self.db.reset_period(self.rnd)\n",
    "        self.timestep = 0\n",
    "        self.period += 1\n",
    "        self.db.buyers[0].next_token()\n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([0,-1,-1,-1,-1,-1,-1,-1,agent.value,-1,-1,-1,agent.num_tokens_traded], dtype = np.float32)\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action, seed=None, options=None):\n",
    "        [buyer.next_token() for buyer in self.db.buyers]\n",
    "        [seller.next_token() for seller in self.db.sellers]\n",
    "        bid_frac = action.item()\n",
    "        # convert action to bid\n",
    "        self.db.buyers[0].next_token()\n",
    "        min_bid = self.db.buyers[0].value * min_frac\n",
    "        max_bid = self.db.buyers[0].value * max_frac\n",
    "        bid = np.round(max_bid * bid_frac + (1 - bid_frac) * min_bid, 2)\n",
    "\n",
    "        # simulate market\n",
    "        bids = [buyer.bid(self.db) for buyer in self.db.buyers]\n",
    "        bids[0] = bid\n",
    "        asks = [seller.ask(self.db) for seller in self.db.sellers]\n",
    "        current_ask, current_ask_idx, current_bid, current_bid_idx = current_bid_ask(bids, asks)\n",
    "        sale, price, bprofit, sprofit, buy, sell = buy_sell(self.db, current_bid, current_bid_idx, current_ask, current_ask_idx)\n",
    "        step_data = [self.rnd, self.period, self.timestep, bids, asks, current_bid, current_bid_idx, current_ask, current_ask_idx, buy, sell, price, sale, bprofit, sprofit]\n",
    "        self.db.add_step(step_data)\n",
    "\n",
    "        # compute reward, new state\n",
    "        reward = 0.0\n",
    "        if sale == 1 and current_bid_idx == 0:\n",
    "            reward = bprofit\n",
    "            \n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([self.timestep + 1, current_ask, current_ask_idx, current_bid, current_bid_idx,\n",
    "                                sale, price, buy, sell, agent.value, agent.step_profit,\n",
    "                                agent.sale, agent.num_tokens_traded],dtype = np.float32)\n",
    "        idx = np.isnan(observation)\n",
    "        observation[idx] = -1.0\n",
    "        # check termination\n",
    "        self.timestep += 1\n",
    "        if self.timestep == nsteps:\n",
    "            terminated = True\n",
    "            self.timestep = 0\n",
    "        else:\n",
    "            terminated = False\n",
    "        infos = {\"TimeLimit.truncated\":True}\n",
    "        truncated = False\n",
    "        return observation, reward, terminated, truncated, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = 0\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "observation, info = env.reset()\n",
    "for period in count():\n",
    "    for timestep in count(): \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        print(f\"Rnd: {rnd}, Period: {period}, New State: {observation}, Action:{np.round(action,1)}, Reward: {np.round(reward,1)}, Period End: {done}\")\n",
    "        if done:\n",
    "            # If the episode is done, reset the environment\n",
    "            #print('done')\n",
    "            observation, info = env.reset()\n",
    "            break\n",
    "    if period == nperiods:\n",
    "        period = 0\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(1000).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b34a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your environment and parameters (replace with your actual environment setup)\n",
    "rnd = 0\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.5\n",
    "eval_steps = 1000\n",
    "training_step = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29040eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad71ec",
   "metadata": {},
   "source": [
    "### Continous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856dcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, DDPG, TD3, A2C, PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "model = SAC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 123      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 120      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.4    |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | 0.0238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66929    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 177      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 240      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.6    |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.226    |\n",
      "|    ent_coef_loss   | -0.0185  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67049    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 195      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 360      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.4    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67169    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.8    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | 0.0585   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67289    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 211      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -0.0296  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67409    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 720      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.2    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67529    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71.6    |\n",
      "|    critic_loss     | 2.96     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | 0.0565   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67649    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 220      |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 960      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.8    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -0.0958  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67769    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 221      |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.9    |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67889    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 222      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.9    |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -0.00875 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68009    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 223      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 1320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.3    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.0797  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68129    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 1440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.8    |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.011   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68249    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 1560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.2    |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | 0.0518   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68369    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 1680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.4    |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.154   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68489    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 226      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56      |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | 0.00245  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68609    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.round_data.redemption_values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf995d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9122b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_period(env.db, 0, 2236)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eaf961",
   "metadata": {},
   "source": [
    "### Discrete Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "env.action_space = spaces.Discrete(51)\n",
    "#policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1,)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98f33",
   "metadata": {},
   "source": [
    "## ON POLICY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75639662",
   "metadata": {},
   "source": [
    "### DDPG - Deterministic Deep Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acec3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a34e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfc29c",
   "metadata": {},
   "source": [
    "### PPO - Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbafb0",
   "metadata": {},
   "source": [
    "### A2C - Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc92acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A2C model\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "a2c_model = A2C(MlpPolicy, env, verbose=0)\n",
    "\n",
    "# Train the A2C agent for 10000 steps\n",
    "a2c_model.learn(total_timesteps=training_step, progress_bar = True)\n",
    "\n",
    "# Evaluate the trained A2C agent\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model, env, n_eval_episodes=eval_steps)\n",
    "print(f\"A2C mean_reward: {mean_reward_a2c:.2f} +/- {std_reward_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e53392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
