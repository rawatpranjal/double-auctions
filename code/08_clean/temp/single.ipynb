{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3dfec947-c3f7-4059-a271-e34e352f80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "# import dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import itertools\n",
    "from collections import deque\n",
    "\n",
    "def loadAlgo(algo, numStates, numActions, algoArgs=[]):\n",
    "    if algo=='VPG':\n",
    "        return VPG(numStates, numActions, *algoArgs)\n",
    "    if algo=='DQN':\n",
    "        return DQN(numStates, numActions, *algoArgs)\n",
    "    if algo=='PPO':\n",
    "        return PPO(numStates, numActions, *algoArgs)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, numStates, numActions=10):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(numStates, 256)\n",
    "        self.fc2 = nn.Linear(256, numActions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "class VPG:\n",
    "    def __init__(self, numStates, episodeLength = 10, numActions=10):\n",
    "        super(VPG, self).__init__()\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.numActions = numActions\n",
    "        self.numStates = numStates\n",
    "        self.gamma = 0.99\n",
    "        self.episodeLength = episodeLength\n",
    "        self.policy = PolicyNetwork(numStates, numActions)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters())\n",
    "        self.done = False\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state).unsqueeze(0).float()\n",
    "        probs = self.policy.forward(state)\n",
    "        multinomial = Categorical(probs)\n",
    "        action = multinomial.sample()\n",
    "        self.log_probs.append(multinomial.log_prob(action))\n",
    "        return action.item()\n",
    "        \n",
    "    def observe(self, state, action, reward, newState, done):\n",
    "        self.rewards.append(reward)\n",
    "        self.done = done\n",
    "    \n",
    "    def train(self, state, action, reward, newState, done):\n",
    "        if done and len(self.rewards)>=self.episodeLength:\n",
    "            R = 0\n",
    "            rewards = []\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                rewards.insert(0, R)\n",
    "            rewards = torch.tensor(rewards)\n",
    "            #rewards = (rewards - rewards.mean()) / (rewards.std() + self.eps)\n",
    "            policy_loss = []  \n",
    "            for log_prob, reward in zip(self.log_probs, rewards):\n",
    "                policy_loss.append(-log_prob * reward)\n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss = torch.cat(policy_loss).sum()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            del self.rewards[:]\n",
    "            del self.log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13dfe481-4457-4d06-97cd-cb4b609ad027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "#from algorithms import * \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class Trader:\n",
    "    def __init__(self, gameData, disclosure, index, buyer, reinforcer):\n",
    "        self.gameType, self.numBuyers, self.numSellers, self.numTokens, self.numRounds, self.numPeriods, self.numSteps, self.seed = gameData\n",
    "        self.index = index\n",
    "        self.buyer = buyer\n",
    "        self.reinforcer = reinforcer\n",
    "        self.df = pd.DataFrame(columns=disclosure)\n",
    "        self.disclosure = disclosure\n",
    "        self.gameTokens = []\n",
    "        self.gameTrades = 0\n",
    "        self.gameProfits = 0\n",
    "        self.gameRounds = 0\n",
    "        self.gameSteps = 0\n",
    "        self.roundTrades = 0\n",
    "        self.roundProfits = 0\n",
    "        self.roundPeriods = 0\n",
    "        self.periodTrades = 0\n",
    "        self.periodProfits = 0\n",
    "        self.periodSteps = 0\n",
    "        self.stepTrades = 0\n",
    "        self.stepProfits = 0\n",
    "        self.stepTokenValue = 0\n",
    "        self.periodSteps = 0\n",
    "        self.stepTrades = 0\n",
    "        self.stepProfits = 0\n",
    "        self.stepTokenValue = 0\n",
    "        \n",
    "    def startRound(self, tokenValues):\n",
    "        self.roundTokens = tokenValues\n",
    "        self.roundTrades = 0\n",
    "        self.roundProfits = 0\n",
    "        self.roundPeriods = 0\n",
    "\n",
    "    def endRound(self):\n",
    "        self.gameTokens.append(self.roundTokens)\n",
    "        self.gameTrades += self.roundTrades\n",
    "        self.gameProfits += self.roundProfits\n",
    "        self.gameRounds += 1\n",
    "           \n",
    "    def startPeriod(self):\n",
    "        self.periodTokens = self.roundTokens\n",
    "        self.periodTrades = 0\n",
    "        self.periodProfits = 0\n",
    "        self.periodSteps = 0\n",
    "\n",
    "    def endPeriod(self):\n",
    "        self.roundProfits += self.periodProfits\n",
    "        self.roundTrades += self.periodTrades\n",
    "        self.roundPeriods += 1\n",
    "\n",
    "    def startStep(self):\n",
    "        self.stepProfits = 0\n",
    "        self.stepTrades = 0\n",
    "        self.stepTokenValue = np.nan\n",
    "        if self.periodTrades < self.numTokens:\n",
    "            self.stepTokenValue = self.periodTokens[self.periodTrades]\n",
    "\n",
    "    def endStep(self):\n",
    "        self.gameSteps += 1\n",
    "        self.periodSteps += 1\n",
    "        self.periodProfits += self.stepProfits\n",
    "        self.periodTrades += self.stepTrades  \n",
    "\n",
    "    def buy(self, currentBid, currentAsk):\n",
    "        self.acceptSale = False\n",
    "        if self.stepTokenValue >= currentAsk:\n",
    "            self.acceptSale = True\n",
    "        return self.acceptSale\n",
    "\n",
    "    def sell(self, currentBid, currentAsk):\n",
    "        self.acceptSale = False\n",
    "        if self.stepTokenValue <= currentBid:\n",
    "            self.acceptSale = True\n",
    "        return self.acceptSale\n",
    "    \n",
    "    def transact(self, price):\n",
    "        self.stepTrades = 1\n",
    "        self.stepProfits = profit(self.stepTokenValue,price,self.buyer)\n",
    "\n",
    "class TruthTeller(Trader):\n",
    "    def __init__(self, gameData, disclosure, index, buyer, reinforcer):\n",
    "        super().__init__(gameData, disclosure, index, buyer, reinforcer)\n",
    "    \n",
    "    def bid(self):\n",
    "        self.stepBid = self.stepTokenValue\n",
    "        return self.stepBid\n",
    "    \n",
    "    def ask(self):\n",
    "        self.stepAsk = self.stepTokenValue\n",
    "        return self.stepAsk\n",
    "\n",
    "class ZeroIntelligence(Trader):\n",
    "    def __init__(self, gameData, disclosure, index, buyer, reinforcer):\n",
    "        super().__init__(gameData, disclosure, index, buyer, reinforcer)\n",
    "    \n",
    "    def bid(self):\n",
    "        self.stepBid = np.nan\n",
    "        if self.stepTokenValue >= 0:\n",
    "            self.stepBid = np.random.uniform(self.stepTokenValue*0.1,self.stepTokenValue, 1).item()\n",
    "            self.stepBid = np.round(self.stepBid, 1)\n",
    "        return np.round(self.stepBid,1)\n",
    "        \n",
    "    def ask(self):\n",
    "        self.stepAsk = np.nan\n",
    "        if self.stepTokenValue >= 0:\n",
    "            self.stepAsk = np.random.uniform(self.stepTokenValue,self.stepTokenValue*1.9, 1).item()\n",
    "            self.stepAsk = np.round(self.stepAsk, 1)\n",
    "        return self.stepAsk\n",
    "\n",
    "def generateAgents(gameData,buyerStrategies,sellerStrategies,disclosure):\n",
    "    buyers, sellers = [], []\n",
    "    for idx,i in enumerate(buyerStrategies):\n",
    "        if i == 'TruthTeller':\n",
    "            buyers.append(TruthTeller(gameData, disclosure, index=idx, buyer=1, reinforer=0)) \n",
    "        if i == 'ZeroIntelligence':\n",
    "            buyers.append(ZeroIntelligence(gameData, disclosure, index=idx, buyer=1, reinforer=0)) \n",
    "        if i == 'VPG':\n",
    "            buyers.append(VPG(gameData, disclosure, index=idx, buyer=1, reinforcer=1, episodeLength = gameData[7])) \n",
    "        if i == 'PPO':\n",
    "            buyers.append(PPO(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'SAC':\n",
    "            buyers.append(SAC(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'DQN':\n",
    "            buyers.append(DQN(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "        if i == 'DDPG':\n",
    "            buyers.append(DDPG(gameData, disclosure, index=idx, buyer=1, reinforcer=1)) \n",
    "\n",
    "    for idx,i in enumerate(sellerStrategies):\n",
    "        if i == 'TruthTeller':\n",
    "            sellers.append(TruthTeller(gameData, disclosure, index=idx, buyer=0, reinforcer=0)) \n",
    "        if i == 'ZeroIntelligence':\n",
    "            sellers.append(ZeroIntelligence(gameData, disclosure, index=idx, buyer=0, reinforcer=0)) \n",
    "        if i == 'VPG':\n",
    "            sellers.append(VPG(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'PPO':\n",
    "            sellers.append(PPO(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'SAC':\n",
    "            sellers.append(SAC(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'DQN':\n",
    "            sellers.append(DQN(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "        if i == 'DDPG':\n",
    "            sellers.append(DDPG(gameData, disclosure, index=idx, buyer=0, reinforcer=1)) \n",
    "    return buyers, sellers\n",
    "\n",
    "    \n",
    "def generateState(agent):\n",
    "    counters = [agent.periodSteps] #agent.stepTrades, agent.stepProfits, agent.stepTokenValue]\n",
    "    disclosureLength = len(agent.disclosure)\n",
    "    if (disclosureLength == 0) | (agent.depth == 0):\n",
    "        activityLog = []\n",
    "    else:   \n",
    "        if agent.gameSteps >= agent.depth:\n",
    "            agent.disclosureCopy = deepcopy(agent.disclosure)\n",
    "            bidsDisclose, asksDisclose = False, False\n",
    "            if 'bids' in agent.disclosure:\n",
    "                agent.disclosureCopy.remove('bids')\n",
    "                bidsDisclose = True\n",
    "            if 'asks' in agent.disclosure:\n",
    "                agent.disclosureCopy.remove('asks')\n",
    "                asksDisclose = True\n",
    "            \n",
    "            activityLog = [[]]\n",
    "            for i in range(1, agent.depth+1):\n",
    "                activityLog[0] += agent.df.iloc[-i][agent.disclosureCopy].tolist()           \n",
    "                if bidsDisclose:\n",
    "                    activityLog[0] += agent.df.iloc[-i].bids\n",
    "                if asksDisclose:\n",
    "                    activityLog[0] += agent.df.iloc[-i].asks\n",
    "            activityLog = activityLog[0]\n",
    "        else:\n",
    "            bidsDisclose, asksDisclose = False, False\n",
    "            if 'bids' in agent.disclosure:\n",
    "                disclosureLength -= 1\n",
    "                bidsDisclose = True\n",
    "            if 'asks' in agent.disclosure:\n",
    "                disclosureLength -= 1\n",
    "                asksDisclose = True\n",
    "            activityLog = [-9] * (disclosureLength*agent.depth + bidsDisclose*agent.depth*agent.numBuyers+asksDisclose*agent.depth*agent.numSellers)\n",
    "        \n",
    "    state = counters + activityLog\n",
    "    cleanState = [-9 if np.isnan(x) else x for x in state]\n",
    "    return cleanState\n",
    "\n",
    "class Reinforcer(Trader):\n",
    "    def __init__(self, gameData, disclosure=['currentBid', 'currentAsk', 'buy', 'sell', 'price', 'price'], \n",
    "                 index=0, buyer=1, reinforcer=1, numActions=10, algo='VPG', algoArgs=[], depth = 0, verbose = 0):\n",
    "        super().__init__(gameData, disclosure, index, buyer, reinforcer)\n",
    "        self.depth = depth\n",
    "        self.disclosure = disclosure\n",
    "        print(generateState(self))\n",
    "        self.state = generateState(self)\n",
    "        print(self.state)\n",
    "        self.numStates = len(self.state)\n",
    "        self.numActions = numActions\n",
    "        self.state = [-1]*self.numStates\n",
    "        self.algo = loadAlgo(algo, self.numStates, self.numActions, *algoArgs)\n",
    "        self.done = False\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def observe(self):\n",
    "        self.newState = generateState(self)\n",
    "        self.algo.observe(self.state, self.action, self.stepProfits, self.newState, self.done)\n",
    "        if self.verbose == 1:\n",
    "            print(self.state, self.action, self.stepProfits, self.newState, self.done)\n",
    "        self.state = self.newState\n",
    "        if (self.periodSteps == self.numSteps-1): # & (self.roundPeriods == self.numPeriods-1):\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.done = False\n",
    "\n",
    "    def train(self):\n",
    "        self.algo.train(self.state, self.action, self.stepProfits, self.newState, self.done)\n",
    "        \n",
    "    def bid(self):\n",
    "        self.stepBid = np.nan\n",
    "        self.action = self.algo.act(self.state)\n",
    "        if self.stepTokenValue >= 0:\n",
    "            self.stepBid = (self.action/(self.numActions-1)) * 100\n",
    "        return self.stepBid\n",
    "        \n",
    "    def ask(self):\n",
    "        self.stepAsk = np.nan\n",
    "        self.action = self.algo.act(self.state)\n",
    "        if self.stepTokenValue >= 0:\n",
    "            self.stepAsk = (self.action/(self.numActions-1)) * 100\n",
    "        return self.stepAsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84ee5c4a-56f3-471c-86e5-59679bbf43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9]\n",
      "[0, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9]\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class GymEnv(gym.Env):\n",
    "    def __init__(self, metaData, buyers, sellers, log):\n",
    "        self.gameData = metaData[0:8]\n",
    "        self.gameTypes, self.numBuyers, self.numSellers, self.numTokens, self.numRounds, self.numPeriods, self.numSteps, self.seed = self.gameData\n",
    "        self.disclosure, self.buyers, self.sellers = metaData[8:]\n",
    "        self.log = log\n",
    "        self.rnd = 0\n",
    "        self.period = 0\n",
    "        self.Step = 0\n",
    "        self.action_space = spaces.Box(-1,1,(1,),dtype=np.float32)\n",
    "        self.numStates = len(generateState(self.buyers[0]))\n",
    "        self.observation_space = spaces.Box(-1000,1000,(self.numStates,),dtype=np.float32)\n",
    "        startRounds(self.gameData, self.log, self.buyers, self.sellers, self.rnd)\n",
    "    \n",
    "    def reset(self, seed = None):\n",
    "        print('\\nreset')\n",
    "        startPeriods(self.buyers, self.sellers)\n",
    "        generateState(self.buyers[0])\n",
    "        return self.buyers[0].state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        print('\\nstep')\n",
    "        startSteps(self.buyers, self.sellers)\n",
    "        bids, asks = collectOffers(self.buyers, self.sellers)\n",
    "        bids[0] = action.item() * 100\n",
    "        currentAsk, currentAskIdx, currentBid, currentBidIdx = bestOffers(bids, asks)\n",
    "        price, buy, sell = trade(buyers, sellers, currentAsk, currentAskIdx, currentBid, currentBidIdx)\n",
    "        bprofit, sprofit = 0, 0\n",
    "        if price > 0:\n",
    "            self.buyers[currentBidIdx].transact(price)\n",
    "            self.sellers[currentAskIdx].transact(price)\n",
    "            bprofit = self.buyers[currentBidIdx].stepProfits\n",
    "            sprofit = self.sellers[currentAskIdx].stepProfits\n",
    "        self.log.addStep([self.rnd, self.period, self.Step, bids, asks, currentBid, currentBidIdx, currentAsk, currentAskIdx, buy, sell, price, price>0, bprofit, sprofit])\n",
    "        observe(self.buyers, self.sellers, log.disclose())\n",
    "        reward = 0.0\n",
    "        if price > 0 and currentBidIdx == 0:\n",
    "            reward = np.nan_to_num(bprofit,nan=0)\n",
    "        updateStates(self.buyers, self.sellers)\n",
    "        newState = self.buyers[0].state\n",
    "        print(self.buyers[0].state)\n",
    "        done = self.buyers[0].done\n",
    "        updatePolicy(self.buyers, self.sellers)\n",
    "        endSteps(self.buyers, self.sellers)\n",
    "        print('step:', self.Step, 'periodtrades:', self.buyers[0].periodTrades)\n",
    "        self.Step += 1\n",
    "        if done:\n",
    "            print('\\t', self.period)\n",
    "            endPeriods(self.buyers, self.sellers)\n",
    "            self.period += 1\n",
    "            self.Step = 0\n",
    "        return newState, reward, done, False, {}\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import torch as th\n",
    "th.autograd.set_detect_anomaly(True)\n",
    "numRounds, numPeriods, numSteps = 1, 10000, 9\n",
    "numBuyers, numSellers, numTokens = 3, 3, 3\n",
    "gameTypes, seed = '1001', 42\n",
    "disclosure = ['asks','currentBid','currentBidIdx','currentAsk',\t'currentAskIdx','buy','sell','price','sale']\n",
    "#disclosure = []\n",
    "depth = 1\n",
    "# ZeroIntelligence, TruthTeller\n",
    "\n",
    "gameData = [gameTypes, numBuyers, numSellers, numTokens, numRounds, numPeriods, numSteps, seed]\n",
    "\n",
    "buyers = [\n",
    "    Reinforcer(gameData, disclosure, index=0, buyer=1, reinforcer=1, numActions = 20, verbose = 1, depth = depth),\n",
    "    ZeroIntelligence(gameData, disclosure, index=0, buyer=1, reinforcer=0),\n",
    "    ZeroIntelligence(gameData, disclosure, index=0, buyer=1, reinforcer=0),\n",
    "]\n",
    "sellers = [\n",
    "    ZeroIntelligence(gameData, disclosure, index=0, buyer=0, reinforcer=0),\n",
    "    ZeroIntelligence(gameData, disclosure, index=0, buyer=0, reinforcer=0),\n",
    "    ZeroIntelligence(gameData, disclosure, index=0, buyer=0, reinforcer=0),\n",
    "          ]\n",
    "log = Log(gameData, disclosure)\n",
    "metaData = [gameTypes, numBuyers, numSellers, numTokens, numRounds, numPeriods, numSteps, seed, disclosure, buyers, sellers]\n",
    "from stable_baselines3 import PPO, SAC, DDPG\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "env = GymEnv(metaData, buyers, sellers, log)\n",
    "#print(env.log.roundData.iloc[0].buyerValues.item())\n",
    "#policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,)\n",
    "#model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ed2e888-254d-4040-ac30-5fe86320c687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyers[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa31ca90-408d-4e05-a2c5-d2fb634a1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reset\n",
      "\n",
      "step\n",
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] 4 0 [0, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9] False\n",
      "[0, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9]\n",
      "step: 0 periodtrades: 0\n",
      "\n",
      "step\n",
      "[0, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9] 6 0 [1, 47.4, 1, 14.7, 0, True, True, 31.049999999999997, True, 14.7, 49.6, 23.0] False\n",
      "[1, 47.4, 1, 14.7, 0, True, True, 31.049999999999997, True, 14.7, 49.6, 23.0]\n",
      "step: 1 periodtrades: 0\n",
      "\n",
      "step\n",
      "[1, 47.4, 1, 14.7, 0, True, True, 31.049999999999997, True, 14.7, 49.6, 23.0] 3 0 [2, 44.5, 2, 20.3, 2, True, True, 32.4, True, 32.4, 55.8, 20.3] False\n",
      "[2, 44.5, 2, 20.3, 2, True, True, 32.4, True, 32.4, 55.8, 20.3]\n",
      "step: 2 periodtrades: 0\n",
      "\n",
      "step\n",
      "[2, 44.5, 2, 20.3, 2, True, True, 32.4, True, 32.4, 55.8, 20.3] 3 0 [3, 19.6, 1, 24.3, 2, True, False, 24.3, True, 46.1, 45.8, 24.3] False\n",
      "[3, 19.6, 1, 24.3, 2, True, False, 24.3, True, 46.1, 45.8, 24.3]\n",
      "step: 3 periodtrades: 0\n",
      "\n",
      "step\n",
      "[3, 19.6, 1, 24.3, 2, True, False, 24.3, True, 46.1, 45.8, 24.3] 3 0 [4, 38.0, 2, 53.3, 0, False, True, 38.0, True, 53.3, 56.7, 70.2] False\n",
      "[4, 38.0, 2, 53.3, 0, False, True, 38.0, True, 53.3, 56.7, 70.2]\n",
      "step: 4 periodtrades: 0\n",
      "\n",
      "step\n",
      "[4, 38.0, 2, 53.3, 0, False, True, 38.0, True, 53.3, 56.7, 70.2] 3 62.183843564987185 [5, 42.532312870025635, 0, 33.1, 1, True, True, 37.816156435012815, True, 57.8, 33.1, 45.8] False\n",
      "[5, 42.532312870025635, 0, 33.1, 1, True, True, 37.816156435012815, True, 57.8, 33.1, 45.8]\n",
      "step: 5 periodtrades: 1\n",
      "\n",
      "step\n",
      "[5, 42.532312870025635, 0, 33.1, 1, True, True, 37.816156435012815, True, 57.8, 33.1, 45.8] 3 0 [6, 3.5, 2, 40.9, 0, False, False, -9, False, 40.9, 61.0, 48.4] False\n",
      "[6, 3.5, 2, 40.9, 0, False, False, -9, False, 40.9, 61.0, 48.4]\n",
      "step: 6 periodtrades: 1\n",
      "\n",
      "step\n",
      "[6, 3.5, 2, 40.9, 0, False, False, -9, False, 40.9, 61.0, 48.4] 1 23.24786820411682 [7, 31.804263591766357, 0, 41.7, 0, True, True, 36.75213179588318, True, 41.7, 56.6, 57.9] False\n",
      "[7, 31.804263591766357, 0, 41.7, 0, True, True, 36.75213179588318, True, 41.7, 56.6, 57.9]\n",
      "step: 7 periodtrades: 2\n",
      "\n",
      "step\n",
      "[7, 31.804263591766357, 0, 41.7, 0, True, True, 36.75213179588318, True, 41.7, 56.6, 57.9] 3 0 [8, 5.6, 2, 41.5, 2, False, False, -9, False, -9, 77.8, 41.5] False\n",
      "[8, 5.6, 2, 41.5, 2, False, False, -9, False, -9, 77.8, 41.5]\n",
      "step: 8 periodtrades: 2\n",
      "\t 0\n",
      "\n",
      "reset\n",
      "\n",
      "step\n",
      "[8, 5.6, 2, 41.5, 2, False, False, -9, False, -9, 77.8, 41.5] 1 0 [0, 64.8, 2, 0.0, 1, True, True, 32.4, True, 15.9, 0.0, 28.6] True\n",
      "[0, 64.8, 2, 0.0, 1, True, True, 32.4, True, 15.9, 0.0, 28.6]\n",
      "step: 0 periodtrades: 0\n",
      "\n",
      "step\n",
      "[0, 64.8, 2, 0.0, 1, True, True, 32.4, True, 15.9, 0.0, 28.6] 3 0 [1, 46.4, 1, 21.8, 2, True, True, 34.1, True, 22.9, 32.7, 21.8] False\n",
      "[1, 46.4, 1, 21.8, 2, True, True, 34.1, True, 22.9, 32.7, 21.8]\n",
      "step: 1 periodtrades: 0\n",
      "\n",
      "step\n",
      "[1, 46.4, 1, 21.8, 2, True, True, 34.1, True, 22.9, 32.7, 21.8] 3 0 [2, 34.3, 2, 21.1, 0, True, True, 27.7, True, 21.1, 39.8, 22.0] False\n",
      "[2, 34.3, 2, 21.1, 0, True, True, 27.7, True, 21.1, 39.8, 22.0]\n",
      "step: 2 periodtrades: 0\n",
      "\n",
      "step\n",
      "[2, 34.3, 2, 21.1, 0, True, True, 27.7, True, 21.1, 39.8, 22.0] 3 62.18675737380981 [3, 38.22648525238037, 0, 37.4, 2, True, True, 37.81324262619019, True, 52.0, 48.3, 37.4] False\n",
      "[3, 38.22648525238037, 0, 37.4, 2, True, True, 37.81324262619019, True, 52.0, 48.3, 37.4]\n",
      "step: 3 periodtrades: 1\n",
      "\n",
      "step\n",
      "[3, 38.22648525238037, 0, 37.4, 2, True, True, 37.81324262619019, True, 52.0, 48.3, 37.4] 3 0 [4, 30.1, 1, 51.6, 0, True, False, 51.6, True, 51.6, 51.7, 58.6] False\n",
      "[4, 30.1, 1, 51.6, 0, True, False, 51.6, True, 51.6, 51.7, 58.6]\n",
      "step: 4 periodtrades: 1\n",
      "\n",
      "step\n",
      "[4, 30.1, 1, 51.6, 0, True, False, 51.6, True, 51.6, 51.7, 58.6] 3 14.603231048583986 [5, 50.99353790283203, 0, 39.8, 2, True, True, 45.396768951416014, True, 46.5, 42.5, 39.8] False\n",
      "[5, 50.99353790283203, 0, 39.8, 2, True, True, 45.396768951416014, True, 46.5, 42.5, 39.8]\n",
      "step: 5 periodtrades: 2\n",
      "\n",
      "step\n",
      "[5, 50.99353790283203, 0, 39.8, 2, True, True, 45.396768951416014, True, 46.5, 42.5, 39.8] 3 11.389142465591426 [6, 47.62171506881714, 0, 39.4, 1, True, True, 43.51085753440857, True, 49.7, 39.4, -9] False\n",
      "[6, 47.62171506881714, 0, 39.4, 1, True, True, 43.51085753440857, True, 49.7, 39.4, -9]\n",
      "step: 6 periodtrades: 3\n",
      "\n",
      "step\n",
      "[6, 47.62171506881714, 0, 39.4, 1, True, True, 43.51085753440857, True, 49.7, 39.4, -9] 3 0 [7, 22.7, 2, 38.7, 0, False, False, -9, False, 38.7, 61.9, -9] False\n",
      "[7, 22.7, 2, 38.7, 0, False, False, -9, False, 38.7, 61.9, -9]\n",
      "step: 7 periodtrades: 3\n",
      "\n",
      "step\n",
      "[7, 22.7, 2, 38.7, 0, False, False, -9, False, 38.7, 61.9, -9] 7 nan [8, 32.727718353271484, 0, 33.8, 0, False, True, 32.727718353271484, True, 33.8, 57.0, -9] False\n",
      "[8, 32.727718353271484, 0, 33.8, 0, False, True, 32.727718353271484, True, 33.8, 57.0, -9]\n",
      "step: 8 periodtrades: 4\n",
      "\t 1\n",
      "\n",
      "reset\n",
      "\n",
      "step\n",
      "[8, 32.727718353271484, 0, 33.8, 0, False, True, 32.727718353271484, True, 33.8, 57.0, -9] 7 0 [0, 76.3, 2, 0.0, 1, True, True, 38.15, True, 23.3, 0.0, 29.4] True\n",
      "[0, 76.3, 2, 0.0, 1, True, True, 38.15, True, 23.3, 0.0, 29.4]\n",
      "step: 0 periodtrades: 0\n",
      "\n",
      "step\n",
      "[0, 76.3, 2, 0.0, 1, True, True, 38.15, True, 23.3, 0.0, 29.4] 3 0 [1, 51.9, 1, 24.3, 0, True, True, 38.1, True, 24.3, 45.6, 28.5] False\n",
      "[1, 51.9, 1, 24.3, 0, True, True, 38.1, True, 24.3, 45.6, 28.5]\n",
      "step: 1 periodtrades: 0\n",
      "\n",
      "step\n",
      "[1, 51.9, 1, 24.3, 0, True, True, 38.1, True, 24.3, 45.6, 28.5] 3 0 [2, 51.9, 1, 22.8, 2, True, True, 37.35, True, 34.5, 37.0, 22.8] False\n",
      "[2, 51.9, 1, 22.8, 2, True, True, 37.35, True, 34.5, 37.0, 22.8]\n",
      "step: 2 periodtrades: 0\n",
      "\n",
      "step\n",
      "[2, 51.9, 1, 22.8, 2, True, True, 37.35, True, 34.5, 37.0, 22.8] 3 0 [3, 34.2, 2, 28.6, 2, True, True, 31.400000000000002, True, 31.6, 44.8, 28.6] False\n",
      "[3, 34.2, 2, 28.6, 2, True, True, 31.400000000000002, True, 31.6, 44.8, 28.6]\n",
      "step: 3 periodtrades: 0\n",
      "\n",
      "step\n",
      "[3, 34.2, 2, 28.6, 2, True, True, 31.400000000000002, True, 31.6, 44.8, 28.6] 3 59.1 [4, 25.02570152282715, 0, 40.9, 0, True, False, 40.9, True, 40.9, 56.8, 50.2] False\n",
      "[4, 25.02570152282715, 0, 40.9, 0, True, False, 40.9, True, 40.9, 56.8, 50.2]\n",
      "step: 4 periodtrades: 1\n",
      "\n",
      "step\n",
      "[4, 25.02570152282715, 0, 40.9, 0, True, False, 40.9, True, 40.9, 56.8, 50.2] 3 0 [5, 18.2, 2, 41.9, 0, False, False, -9, False, 41.9, 57.6, 72.6] False\n",
      "[5, 18.2, 2, 41.9, 0, False, False, -9, False, 41.9, 57.6, 72.6]\n",
      "step: 5 periodtrades: 1\n",
      "\n",
      "step\n",
      "[5, 18.2, 2, 41.9, 0, False, False, -9, False, 41.9, 57.6, 72.6] 1 7.072281312942508 [6, 67.25543737411499, 0, 38.6, 1, True, True, 52.92771868705749, True, 40.2, 38.6, 40.2] False\n",
      "[6, 67.25543737411499, 0, 38.6, 1, True, True, 52.92771868705749, True, 40.2, 38.6, 40.2]\n",
      "step: 6 periodtrades: 2\n",
      "\n",
      "step\n",
      "[6, 67.25543737411499, 0, 38.6, 1, True, True, 52.92771868705749, True, 40.2, 38.6, 40.2] 3 0 [7, 18.3, 1, 33.1, 0, False, False, -9, False, 33.1, 56.5, 70.7] False\n",
      "[7, 18.3, 1, 33.1, 0, False, False, -9, False, 33.1, 56.5, 70.7]\n",
      "step: 7 periodtrades: 2\n",
      "\n",
      "step\n",
      "[7, 18.3, 1, 33.1, 0, False, False, -9, False, 33.1, 56.5, 70.7] 1 -7.3019090652465835 [8, 78.90381813049316, 0, 45.5, 0, True, True, 62.20190906524658, True, 45.5, 85.3, 47.4] False\n",
      "[8, 78.90381813049316, 0, 45.5, 0, True, True, 62.20190906524658, True, 45.5, 85.3, 47.4]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MulBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 312\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    541\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    547\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[72], line 46\u001b[0m, in \u001b[0;36mGymEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     45\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m---> 46\u001b[0m \u001b[43mupdatePolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuyers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msellers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m endSteps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msellers)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mStep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiodtrades:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mperiodTrades)\n",
      "File \u001b[0;32m/econ_share/home/pp712/double-auctions/code/8_clean/utils.py:183\u001b[0m, in \u001b[0;36mupdatePolicy\u001b[0;34m(buyers, sellers)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buyer \u001b[38;5;129;01min\u001b[39;00m buyers:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buyer\u001b[38;5;241m.\u001b[39mreinforcer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[43mbuyer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seller \u001b[38;5;129;01min\u001b[39;00m sellers:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seller\u001b[38;5;241m.\u001b[39mreinforcer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[71], line 217\u001b[0m, in \u001b[0;36mReinforcer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepProfits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewState\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 74\u001b[0m, in \u001b[0;36mVPG.train\u001b[0;34m(self, state, action, reward, newState, done)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     73\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(policy_loss)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 74\u001b[0m \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[:]\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'MulBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "model.learn(1000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c37dc-e014-423a-b6fa-050dac7419b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.stepData.tail(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97564a1a-6723-4f0b-b77b-6014336f5271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "reset\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[-1] 0 0 [0] False\n",
      "[0]\n",
      "step: 0 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[0] 6 0 [1] False\n",
      "[1]\n",
      "step: 1 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[1] 11 0 [2] False\n",
      "[2]\n",
      "step: 2 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[2] 10 0 [3] False\n",
      "[3]\n",
      "step: 3 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[3] 4 45.38761405944824 [4] False\n",
      "[4]\n",
      "step: 4 periodtrades: 1\n",
      "\n",
      "step\n",
      "periodtrades 1 numtokens 3\n",
      "[4] 19 23.102818965911865 [5] False\n",
      "[5]\n",
      "step: 5 periodtrades: 2\n",
      "\n",
      "step\n",
      "periodtrades 2 numtokens 3\n",
      "[5] 6 0 [6] False\n",
      "[6]\n",
      "step: 6 periodtrades: 2\n",
      "\n",
      "step\n",
      "periodtrades 2 numtokens 3\n",
      "[6] 3 0 [7] False\n",
      "[7]\n",
      "step: 7 periodtrades: 2\n",
      "\n",
      "step\n",
      "periodtrades 2 numtokens 3\n",
      "[7] 1 -10.949999999999996 [8] False\n",
      "[8]\n",
      "step: 8 periodtrades: 3\n",
      "\t 0\n",
      "\n",
      "reset\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[8] 2 0 [0] True\n",
      "[0]\n",
      "step: 0 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[0] 14 0 [1] False\n",
      "[1]\n",
      "step: 1 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[1] 9 0 [2] False\n",
      "[2]\n",
      "step: 2 periodtrades: 0\n",
      "\n",
      "step\n",
      "periodtrades 0 numtokens 3\n",
      "[2] 10 35.599999999999994 [3] False\n",
      "[3]\n",
      "step: 3 periodtrades: 1\n",
      "\n",
      "step\n",
      "periodtrades 1 numtokens 3\n",
      "[3] 0 0 [4] False\n",
      "[4]\n",
      "step: 4 periodtrades: 1\n",
      "\n",
      "step\n",
      "periodtrades 1 numtokens 3\n",
      "[4] 8 23.700000000000003 [5] False\n",
      "[5]\n",
      "step: 5 periodtrades: 2\n",
      "\n",
      "step\n",
      "periodtrades 2 numtokens 3\n",
      "[5] 11 -15.300000000000004 [6] False\n",
      "[6]\n",
      "step: 6 periodtrades: 3\n",
      "\n",
      "step\n",
      "[6] 15 nan [7] False\n",
      "[7]\n",
      "step: 7 periodtrades: 4\n",
      "\n",
      "step\n",
      "[7] 10 0 [8] False\n",
      "[8]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MulBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_policy\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RecurrentPPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpLstmPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:469\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    466\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 469\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:254\u001b[0m, in \u001b[0;36mRecurrentPPO.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    252\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 254\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[37], line 46\u001b[0m, in \u001b[0;36mGymEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     45\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m---> 46\u001b[0m \u001b[43mupdatePolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuyers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msellers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m endSteps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msellers)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mStep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiodtrades:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuyers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mperiodTrades)\n",
      "File \u001b[0;32m/econ_share/home/pp712/double-auctions/code/8_clean/utils.py:183\u001b[0m, in \u001b[0;36mupdatePolicy\u001b[0;34m(buyers, sellers)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buyer \u001b[38;5;129;01min\u001b[39;00m buyers:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buyer\u001b[38;5;241m.\u001b[39mreinforcer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[43mbuyer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seller \u001b[38;5;129;01min\u001b[39;00m sellers:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seller\u001b[38;5;241m.\u001b[39mreinforcer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 200\u001b[0m, in \u001b[0;36mReinforcer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepProfits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewState\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mVPG.train\u001b[0;34m(self, state, action, reward, newState, done)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     69\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(policy_loss)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 70\u001b[0m \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[:]\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'MulBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", env, verbose=1)\n",
    "model.learn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2189e-23a0-47b6-9995-9593ed177a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
