{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa515781-0144-40c2-8886-ae0152cf68a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#comment \n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Hardcoded values\n",
    "gamma = 0.99\n",
    "seed = 543\n",
    "render = False\n",
    "log_interval = 10\n",
    "\n",
    "# Create the Gym environment\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = running_reward * 0.99 + t * 0.01\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                i_episode, t, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold*2:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99fdbec8-bf8e-479c-a443-2924af2361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You appear to be missing MuJoCo.  We expected to find the file here: /home/pp712/.mujoco/mujoco210\n",
      "\n",
      "This package only provides python bindings, the library must be installed separately.\n",
      "\n",
      "Please follow the instructions on the README to install MuJoCo\n",
      "\n",
      "    https://github.com/openai/mujoco-py#install-mujoco\n",
      "\n",
      "Which can be downloaded from the website\n",
      "\n",
      "    https://www.roboti.us/index.html\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "\nYou appear to be missing MuJoCo.  We expected to find the file here: /home/pp712/.mujoco/mujoco210\n\nThis package only provides python bindings, the library must be installed separately.\n\nPlease follow the instructions on the README to install MuJoCo\n\n    https://github.com/openai/mujoco-py#install-mujoco\n\nWhich can be downloaded from the website\n\n    https://www.roboti.us/index.html\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m ckpt_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    170\u001b[0m display \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space) \u001b[38;5;241m!=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mdiscrete\u001b[38;5;241m.\u001b[39mDiscrete:\n\u001b[1;32m    175\u001b[0m     env \u001b[38;5;241m=\u001b[39m NormalizedActions(gym\u001b[38;5;241m.\u001b[39mmake(env_name))\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/econ_share/apps/miniforge3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1126\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/gym/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MujocoEnv, MuJocoPyEnv  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AntEnv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhalf_cheetah\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HalfCheetahEnv\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/gym/envs/mujoco/mujoco_env.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Space\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     MUJOCO_PY_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m const\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmjrenderpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MjRenderPool\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/mujoco_py/builder.py:503\u001b[0m\n\u001b[1;32m    499\u001b[0m     build_fn_cleanup(name)\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m__fun\n\u001b[0;32m--> 503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m \u001b[43mdiscover_mujoco\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m load_cython_ext(mujoco_path)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n",
      "File \u001b[0;32m/econ_share/home/pp712/myenv/lib/python3.11/site-packages/mujoco_py/utils.py:78\u001b[0m, in \u001b[0;36mdiscover_mujoco\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     message \u001b[38;5;241m=\u001b[39m MISSING_MUJOCO_MESSAGE\u001b[38;5;241m.\u001b[39mformat(mujoco_path)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(message, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(message)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mujoco_path\n",
      "\u001b[0;31mException\u001b[0m: \nYou appear to be missing MuJoCo.  We expected to find the file here: /home/pp712/.mujoco/mujoco210\n\nThis package only provides python bindings, the library must be installed separately.\n\nPlease follow the instructions on the README to install MuJoCo\n\n    https://github.com/openai/mujoco-py#install-mujoco\n\nWhich can be downloaded from the website\n\n    https://www.roboti.us/index.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "pi = Variable(torch.FloatTensor([math.pi]))\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state))\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "\n",
    "        eps = torch.randn(mu.size())\n",
    "        # calculate the probability\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps)).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
    "\n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i]))).sum() - (0.0001*entropies[i]).sum()\n",
    "        loss = loss / len(rewards)\n",
    "\t\t\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        action = action * 2 - 1\n",
    "        return actions\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.n\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        probs = self.model(Variable(state))       \n",
    "        action = probs.multinomial(1).data\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "        entropy = - (probs*probs.log()).sum()\n",
    "\n",
    "        return action[0], log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i]))).sum() - (0.0001*entropies[i]).sum()\n",
    "        loss = loss / len(rewards)\n",
    "\t\t\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "# Hardcoded parameters\n",
    "env_name = 'InvertedPendulum-v4'\n",
    "gamma = 0.99\n",
    "exploration_end = 100\n",
    "seed = 123\n",
    "num_steps = 1000\n",
    "num_episodes = 2000\n",
    "hidden_size = 128\n",
    "render = False\n",
    "ckpt_freq = 100\n",
    "display = False\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "if type(env.action_space) != gym.spaces.discrete.Discrete:\n",
    "    env = NormalizedActions(gym.make(env_name))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if display:\n",
    "    env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = REINFORCE(hidden_size, env.observation_space.shape[0], env.action_space)\n",
    "\n",
    "dir = 'ckpt_' + env_name\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = torch.Tensor([env.reset()[0]])\n",
    "    entropies = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    for t in range(num_steps):\n",
    "        action, log_prob, entropy = agent.select_action(state)\n",
    "        action = action.cpu()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action.numpy()[0])\n",
    "\n",
    "        entropies.append(entropy)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = torch.Tensor([next_state])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.update_parameters(rewards, log_probs, entropies, gamma)\n",
    "\n",
    "    if i_episode % ckpt_freq == 0:\n",
    "        torch.save(agent.model.state_dict(), os.path.join(dir, 'reinforce-'+str(i_episode)+'.pkl'))\n",
    "\n",
    "    print(\"Episode: {}, reward: {}\".format(i_episode, np.sum(rewards)))\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efe1b2-7215-46e2-9261-805909ff21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = RecurrentPPO(\"MlpLstmPolicy\", \"CartPole-v1\", verbose=1)\n",
    "model.learn(5000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=20, warn=False)\n",
    "print(mean_reward)\n",
    "\n",
    "model.save(\"ppo_recurrent\")\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = RecurrentPPO.load(\"ppo_recurrent\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "# cell and hidden state of the LSTM\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "while True:\n",
    "    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    episode_starts = dones\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef961e-8ec8-4639-a76b-74f7b4139c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
