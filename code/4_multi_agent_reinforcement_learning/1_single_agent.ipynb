{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce0ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from itertools import count\n",
    "buyer_strategies = ['Honest', 'Random', 'Random']\n",
    "seller_strategies = ['Random', 'Random', 'Random']\n",
    "nbuyers, nsellers = len(buyer_strategies), len(seller_strategies)\n",
    "nrounds, nperiods, ntokens, nsteps, gametype, nbuyers, nsellers = 10, 10, 8, 50, '1234', len(buyer_strategies), len(seller_strategies)\n",
    "R1, R2, R3, R4 = gametype_to_ran(gametype)\n",
    "game_metadata = [nrounds, nperiods, ntokens, nbuyers, nsellers, nsteps, R1, R2, R3, R4]\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "rnd = 0\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87aeeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, db, nsteps, render_mode = None):\n",
    "        self.rnd = 0\n",
    "        self.period = -1\n",
    "        self.nperiods = nperiods\n",
    "        self.db = db\n",
    "        self.action_space = spaces.Box(0,1,(1,),dtype=np.float)\n",
    "        self.observation_space = spaces.Box(-1,200,(13,),dtype=np.float32)\n",
    "\n",
    "    def reset(self,seed=None):\n",
    "        #self.db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "        self.db.reset_period(self.rnd)\n",
    "        self.timestep = 0\n",
    "        self.period += 1\n",
    "        self.db.buyers[0].next_token()\n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([0,-1,-1,-1,-1,-1,-1,-1,agent.value,-1,-1,-1,agent.num_tokens_traded], dtype = np.float32)\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action, seed=None, options=None):\n",
    "        [buyer.next_token() for buyer in self.db.buyers]\n",
    "        [seller.next_token() for seller in self.db.sellers]\n",
    "        bid_frac = action.item()\n",
    "        # convert action to bid\n",
    "        self.db.buyers[0].next_token()\n",
    "        min_bid = self.db.buyers[0].value * min_frac\n",
    "        max_bid = self.db.buyers[0].value * max_frac\n",
    "        bid = np.round(max_bid * bid_frac + (1 - bid_frac) * min_bid, 2)\n",
    "\n",
    "        # simulate market\n",
    "        bids = [buyer.bid(self.db) for buyer in self.db.buyers]\n",
    "        bids[0] = bid\n",
    "        asks = [seller.ask(self.db) for seller in self.db.sellers]\n",
    "        current_ask, current_ask_idx, current_bid, current_bid_idx = current_bid_ask(bids, asks)\n",
    "        sale, price, bprofit, sprofit, buy, sell = buy_sell(self.db, current_bid, current_bid_idx, current_ask, current_ask_idx)\n",
    "        step_data = [self.rnd, self.period, self.timestep, bids, asks, current_bid, current_bid_idx, current_ask, current_ask_idx, buy, sell, price, sale, bprofit, sprofit]\n",
    "        self.db.add_step(step_data)\n",
    "\n",
    "        # compute reward, new state\n",
    "        reward = 0.0\n",
    "        if sale == 1 and current_bid_idx == 0:\n",
    "            reward = bprofit\n",
    "            \n",
    "        agent = self.db.buyers[0]\n",
    "        observation = np.array([self.timestep + 1, current_ask, current_ask_idx, current_bid, current_bid_idx,\n",
    "                                sale, price, buy, sell, agent.value, agent.step_profit,\n",
    "                                agent.sale, agent.num_tokens_traded],dtype = np.float32)\n",
    "        idx = np.isnan(observation)\n",
    "        observation[idx] = -1.0\n",
    "        # check termination\n",
    "        self.timestep += 1\n",
    "        if self.timestep == nsteps:\n",
    "            terminated = True\n",
    "            self.timestep = 0\n",
    "        else:\n",
    "            terminated = False\n",
    "        infos = {\"TimeLimit.truncated\":True}\n",
    "        truncated = False\n",
    "        return observation, reward, terminated, truncated, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313e5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d7618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 0, New State: [ 1.    31.2    1.    83.93   0.     1.    57.565  1.     1.    87.6\n",
      " 30.035  1.     1.   ], Action:[1.], Reward: 30.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 2.    30.5    0.    78.5    1.     1.    54.5    1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 3.    33.     1.    94.8    2.     1.    63.9    1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 4.    40.3    2.    73.5    1.     1.    56.9    1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 5.    44.6    2.    75.8    2.     1.    60.2    1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 6.    39.7    2.    75.3    1.     1.    57.5    1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 7.    51.6    0.    52.3    1.     1.    51.95   1.     1.    73.1\n",
      " 30.035  1.     1.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 8.    54.9    2.    51.35   0.     1.    53.125  1.     1.    73.1\n",
      " 19.975  1.     2.   ], Action:[0.7], Reward: 20.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [ 9.    49.9    1.    55.1    2.     1.    52.5    1.     1.    65.1\n",
      " 19.975  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [10.    59.2    1.    58.47   0.     1.    58.835  1.     1.    65.1\n",
      "  6.265  1.     3.   ], Action:[0.9], Reward: 6.3, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [11.    57.4    0.    42.2    2.     0.    -1.     0.     0.    60.9\n",
      "  6.265  1.     3.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [12.    60.6    0.    55.59   0.     1.    58.095  1.     1.    60.9\n",
      "  2.805  1.     4.   ], Action:[0.9], Reward: 2.8, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [13.    63.8    0.    54.8    2.     1.    54.8    0.     1.    53.4\n",
      "  2.805  1.     4.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [14.    68.3    0.    47.7    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  1.     4.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [15.    66.6    1.    41.03   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [16.    60.     1.    52.42   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [17.    68.1    0.    43.92   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [18.    68.1    0.    47.3    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [19.    68.1    2.    40.13   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [20.    85.3    0.    27.8    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [21.    70.3    1.    44.2    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [22.    76.     1.    48.03   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [23.    60.8    2.    44.1    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [24.    68.9    2.    52.7    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [25.    62.5    2.    48.6    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [26.    78.2    0.    41.7    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [27.    66.3    1.    32.7    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [28.    66.9    1.    46.1    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [29.    66.8    1.    46.9    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [30.    62.7    2.    45.07   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [31.    64.8    1.    30.3    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [32.    68.8    1.    42.45   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [33.    73.4    0.    49.     2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [34.    62.2    1.    45.65   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [35.    63.4    1.    37.37   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [36.    65.3    1.    29.8    1.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [37.    64.7    2.    52.9    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [38.    63.4    2.    49.5    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [39.    80.9    1.    53.     2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [40.    63.7    2.    41.5    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [41.    68.9    2.    50.53   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [42.    64.6    2.    50.5    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [43.    70.6    2.    51.83   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [44.    62.2    1.    48.43   0.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [45.    62.5    2.    53.1    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [46.    61.7    2.    43.1    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [47.    67.7    2.    48.     2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [48.    67.3    1.    51.1    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [49.    66.3    2.    32.6    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 0, New State: [50.    65.8    1.    42.2    2.     0.    -1.     0.     0.    53.4\n",
      "  2.805  0.     4.   ], Action:[0.5], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 1, New State: [ 1.    26.3    1.    93.     2.     1.    59.65   1.     1.    87.6\n",
      "  2.805  0.     0.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 2.    30.4    0.    75.     2.     1.    52.7    1.     1.    87.6\n",
      "  2.805  0.     0.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 3.    38.     1.    61.6    2.     1.    49.8    1.     1.    87.6\n",
      "  2.805  0.     0.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 4.    45.4    2.    46.8    1.     1.    46.1    1.     1.    87.6\n",
      "  2.805  0.     0.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 5.  35.5  2.  82.7  0.   1.  59.1  1.   1.  87.6 28.5  1.   1. ], Action:[0.9], Reward: 28.5, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 6.   41.5   2.   74.4   1.    1.   57.95  1.    1.   73.1  28.5   1.\n",
      "  1.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 7.   49.5   0.   65.4   1.    1.   57.45  1.    1.   73.1  28.5   1.\n",
      "  1.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 8.   46.8   1.   50.9   1.    1.   48.85  1.    1.   73.1  28.5   1.\n",
      "  1.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [ 9.   44.9   2.   53.8   2.    1.   49.35  1.    1.   73.1  28.5   1.\n",
      "  1.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [10.    44.2    0.    51.51   0.     1.    47.855  1.     1.    73.1\n",
      " 25.245  1.     2.   ], Action:[0.7], Reward: 25.2, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [11.    50.7    0.    51.29   0.     1.    50.995  1.     1.    65.1\n",
      " 14.105  1.     3.   ], Action:[0.8], Reward: 14.1, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [12.   57.9   1.   41.52  0.    1.   57.9   1.    0.   60.9   3.    1.\n",
      "  4.  ], Action:[0.7], Reward: 3.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [13.  84.3  0.  52.9  2.   0.  -1.   0.   0.  53.4  3.   1.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [14.   64.1   1.   51.29  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [15.  80.3  2.  34.7  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [16.  71.9  1.  34.9  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [17.  73.4  1.  32.3  1.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [18.  62.5  2.  36.8  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [19.   68.9   2.   43.37  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [20.   76.2   1.   53.29  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [21.   75.2   2.   46.41  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [22.  61.1  2.  39.3  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [23.   74.8   1.   43.97  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [24.  61.5  2.  52.6  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [25.  70.1  1.  51.4  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [26.  59.7  1.  32.4  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [27.  66.6  1.  38.9  1.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [28.  80.8  1.  34.3  1.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [29.  71.8  2.  50.8  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [30.  69.2  1.  51.6  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [31.  67.4  1.  39.1  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [32.  71.1  1.  37.7  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [33.   72.1   2.   52.19  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [34.  80.4  2.  51.5  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [35.  85.7  0.  51.3  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [36.  68.8  2.  34.2  1.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [37.  71.2  2.  46.2  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [38.  67.6  1.  46.   2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [39.  74.8  1.  43.3  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [40.  77.8  1.  47.8  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 1, New State: [41.  63.4  1.  33.   2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [42.  60.9  1.  53.4  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [43.  63.9  1.  43.2  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [44.  69.3  0.  43.4  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [45.   69.6   1.   51.58  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [46.  60.5  1.  39.4  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [47.   77.8   1.   38.71  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [48.  66.5  2.  38.5  2.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [49.  82.5  2.  50.4  0.   0.  -1.   0.   0.  53.4  3.   0.   4. ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 1, New State: [50.   59.5   1.   44.11  0.    0.   -1.    0.    0.   53.4   3.    0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 2, New State: [ 1.   30.6   1.   77.9   1.    1.   54.25  1.    1.   87.6   3.    0.\n",
      "  0.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 2.  32.7  2.  96.5  2.   1.  64.6  1.   1.  87.6  3.   0.   0. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 3.   34.2   0.   72.3   2.    1.   53.25  1.    1.   87.6   3.    0.\n",
      "  0.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 4.  32.3  1.  51.9  1.   1.  42.1  1.   1.  87.6  3.   0.   0. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 5.   40.5   2.   69.82  0.    1.   55.16  1.    1.   87.6  32.44  1.\n",
      "  1.  ], Action:[0.8], Reward: 32.4, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 6.   44.5   2.   60.2   1.    1.   52.35  1.    1.   73.1  32.44  1.\n",
      "  1.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 7.   41.5   2.   62.6   2.    1.   52.05  1.    1.   73.1  32.44  1.\n",
      "  1.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 8.    51.7    1.    50.93   0.     1.    51.315  1.     1.    73.1\n",
      " 21.785  1.     2.   ], Action:[0.7], Reward: 21.8, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [ 9.    47.7    0.    60.8    1.     1.    54.25   1.     1.    65.1\n",
      " 21.785  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [10.    65.8    0.    34.4    2.     0.    -1.     0.     0.    65.1\n",
      " 21.785  1.     2.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [11.    57.3    1.    35.8    1.     0.    -1.     0.     0.    65.1\n",
      " 21.785  1.     2.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [12.    59.4    0.    52.05   0.     1.    55.725  1.     1.    65.1\n",
      "  9.375  1.     3.   ], Action:[0.8], Reward: 9.4, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [13.    45.4    0.    48.6    2.     1.    47.     1.     1.    60.9\n",
      "  9.375  1.     3.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [14.    80.2    2.    52.6    2.     0.    -1.     0.     0.    60.9\n",
      "  9.375  1.     3.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [15.    68.3    1.    41.7    2.     0.    -1.     0.     0.    60.9\n",
      "  9.375  1.     3.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [16.  61.7  1.  55.6  0.   1.  55.6  0.   1.  60.9  5.3  0.   4. ], Action:[0.9], Reward: 5.3, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [17.   59.7   1.   44.47  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [18.   70.    2.   46.64  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [19.  68.6  2.  46.2  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [20.  68.7  1.  51.2  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [21.  67.2  1.  38.2  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [22.  76.4  1.  50.5  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [23.  71.9  1.  37.8  1.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [24.  75.5  1.  40.9  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [25.  75.8  2.  33.4  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [26.  77.2  2.  42.7  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [27.  66.8  1.  47.5  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [28.  62.5  2.  39.9  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [29.  74.9  2.  33.4  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [30.  71.8  1.  43.8  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [31.   66.1   2.   45.03  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [32.  63.7  2.  42.   2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [33.   64.8   2.   51.39  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [34.  70.7  1.  51.9  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [35.  63.9  1.  40.7  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [36.  69.   0.  33.6  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [37.   69.2   2.   53.38  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [38.  70.   2.  52.1  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [39.  78.   0.  52.7  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [40.   65.    2.   51.75  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [41.  81.8  1.  52.8  0.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [42.   73.5   1.   50.55  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [43.   63.2   1.   34.84  0.    0.   -1.    0.    0.   53.4   5.3   0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [44.  59.8  1.  48.4  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [45.  74.2  1.  35.2  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [46.  61.8  1.  33.6  1.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [47.  60.4  1.  40.9  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [48.  65.4  1.  38.3  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [49.  63.8  2.  41.2  2.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 2, New State: [50.  66.9  1.  39.5  1.   0.  -1.   0.   0.  53.4  5.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 3, New State: [ 1.   25.5   1.   73.4   2.    1.   49.45  1.    1.   87.6   5.3   0.\n",
      "  0.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 2.  36.1  0.  81.1  2.   1.  58.6  1.   1.  87.6  5.3  0.   0. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 3.  38.6  2.  80.   1.   1.  59.3  1.   1.  87.6  5.3  0.   0. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 4.   38.    1.   57.5   2.    1.   47.75  1.    1.   87.6   5.3   0.\n",
      "  0.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 5.  41.7  2.  47.1  1.   1.  44.4  1.   1.  87.6  5.3  0.   0. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 6.   38.6   0.   59.3   1.    1.   48.95  1.    1.   87.6   5.3   0.\n",
      "  0.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 7.   45.7   0.   58.2   1.    1.   51.95  1.    1.   87.6   5.3   0.\n",
      "  0.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 8.   46.6   0.   54.3   2.    1.   50.45  1.    1.   87.6   5.3   0.\n",
      "  0.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [ 9.   39.9   2.   80.28  0.    1.   60.09  1.    1.   87.6  27.51  1.\n",
      "  1.  ], Action:[0.9], Reward: 27.5, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [10.    55.3    2.    46.43   0.     1.    50.865  1.     1.    73.1\n",
      " 22.235  1.     2.   ], Action:[0.6], Reward: 22.2, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [11.    64.7    1.    46.6    2.     1.    46.6    0.     1.    65.1\n",
      " 22.235  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [12.   70.1   1.   55.58  0.    1.   55.58  0.    1.   65.1   9.52  0.\n",
      "  3.  ], Action:[0.9], Reward: 9.5, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [13.   62.4   1.   43.4   2.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [14.   60.1   1.   32.9   1.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [15.   62.4   1.   36.3   2.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [16.   71.7   0.   37.1   1.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [17.   69.4   1.   41.3   2.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [18.   66.3   2.   41.26  0.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [19.   72.    0.   43.84  0.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [20.   73.2   1.   26.4   1.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [21.   69.7   1.   48.12  0.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [22.   75.2   1.   56.39  0.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [23.   62.7   1.   54.12  0.    0.   -1.    0.    0.   60.9   9.52  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [24.   80.1   1.   59.35  0.    1.   59.35  0.    1.   60.9   1.55  0.\n",
      "  4.  ], Action:[1.], Reward: 1.5, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [25.   70.8   2.   37.2   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [26.   78.4   2.   39.2   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 3, New State: [27.   68.4   0.   42.3   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [28.   68.9   0.   26.8   1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [29.   74.3   2.   50.94  0.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [30.   69.1   1.   43.3   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [31.   67.1   2.   42.3   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [32.   63.5   2.   38.4   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [33.   71.4   1.   41.4   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [34.   65.    2.   38.3   1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [35.   75.8   2.   33.03  0.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [36.   84.7   2.   36.1   1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [37.   68.    2.   35.7   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [38.   72.9   1.   37.3   1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [39.   68.9   2.   51.94  0.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [40.   68.8   1.   25.    2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [41.   75.5   1.   41.1   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [42.   79.    0.   28.2   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [43.   64.5   2.   29.    1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [44.   64.6   2.   39.4   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [45.   79.7   1.   43.18  0.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [46.   74.3   1.   43.4   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [47.   71.6   1.   38.4   1.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [48.   67.9   1.   28.43  0.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [49.   72.1   0.   40.2   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 3, New State: [50.   69.2   2.   35.1   2.    0.   -1.    0.    0.   53.4   1.55  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 4, New State: [ 1.   26.6   1.   78.6   2.    1.   52.6   1.    1.   87.6   1.55  0.\n",
      "  0.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 2.    30.9    1.    58.83   0.     1.    44.865  1.     1.    87.6\n",
      " 42.735  1.     1.   ], Action:[0.7], Reward: 42.7, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 3.    32.7    2.    67.7    2.     1.    50.2    1.     1.    73.1\n",
      " 42.735  1.     1.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 4.    40.     0.    62.5    2.     1.    51.25   1.     1.    73.1\n",
      " 42.735  1.     1.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 5.    44.4    2.    69.03   0.     1.    56.715  1.     1.    73.1\n",
      " 16.385  1.     2.   ], Action:[0.9], Reward: 16.4, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 6.    35.4    2.    54.9    1.     1.    45.15   1.     1.    65.1\n",
      " 16.385  1.     2.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 7.    39.9    0.    50.7    2.     1.    45.3    1.     1.    65.1\n",
      " 16.385  1.     2.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 8.    49.2    1.    41.2    1.     1.    49.2    1.     0.    65.1\n",
      " 16.385  1.     2.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [ 9.  40.6  2.  56.2  0.   1.  48.4  1.   1.  65.1 16.7  1.   3. ], Action:[0.9], Reward: 16.7, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [10.   49.8   0.   49.5   2.    1.   49.65  1.    1.   60.9  16.7   1.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [11.  49.   0.  64.2  1.   1.  56.6  1.   1.  60.9 16.7  1.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [12.  56.4  1.  55.1  1.   1.  56.4  1.   0.  60.9 16.7  1.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [13.  61.1  1.  30.5  1.   0.  -1.   0.   0.  60.9 16.7  1.   3. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [14.  71.5  2.  34.6  1.   0.  -1.   0.   0.  60.9 16.7  1.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [15.  64.3  1.  42.7  2.   0.  -1.   0.   0.  60.9 16.7  1.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [16.  60.5  1.  32.2  2.   0.  -1.   0.   0.  60.9 16.7  1.   3. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [17.  69.7  2.  36.   2.   0.  -1.   0.   0.  60.9 16.7  1.   3. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [18.   61.8   2.   36.94  0.    0.   -1.    0.    0.   60.9  16.7   0.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [19.  78.7  2.  37.5  1.   0.  -1.   0.   0.  60.9 16.7  0.   3. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [20.  78.7  1.  35.2  1.   0.  -1.   0.   0.  60.9 16.7  0.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [21.  63.1  1.  39.4  2.   0.  -1.   0.   0.  60.9 16.7  0.   3. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [22.   68.5   0.   59.23  0.    0.   -1.    0.    0.   60.9  16.7   0.\n",
      "  3.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [23.   59.6   1.   54.87  0.    1.   59.6   1.    0.   60.9   1.3   1.\n",
      "  4.  ], Action:[0.9], Reward: 1.3, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [24.  83.8  2.  36.3  2.   0.  -1.   0.   0.  53.4  1.3  1.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [25.  68.   1.  36.8  1.   0.  -1.   0.   0.  53.4  1.3  1.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [26.   76.3   2.   44.32  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [27.  81.2  0.  31.9  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [28.   64.9   1.   49.67  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [29.   72.3   2.   50.41  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [30.  68.   0.  40.8  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [31.  72.7  2.  30.2  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [32.  63.9  2.  36.2  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [33.   70.8   1.   45.23  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [34.  87.4  2.  37.8  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [35.  68.9  0.  37.6  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [36.   83.2   2.   49.15  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [37.  68.3  0.  40.2  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [38.   72.2   2.   48.94  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [39.  70.9  2.  33.9  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [40.  68.9  1.  36.2  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [41.  69.4  0.  34.7  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [42.  80.1  0.  37.5  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [43.  71.3  0.  38.7  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [44.  78.2  2.  41.7  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [45.  66.4  2.  32.7  1.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [46.   73.7   0.   52.24  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [47.   71.    1.   34.61  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [48.  67.   1.  36.3  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [49.  70.7  0.  33.8  2.   0.  -1.   0.   0.  53.4  1.3  0.   4. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 4, New State: [50.   79.3   2.   51.82  0.    0.   -1.    0.    0.   53.4   1.3   0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 5, New State: [ 1.   29.3   1.   87.2   2.    1.   58.25  1.    1.   87.6   1.3   0.\n",
      "  0.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 2.   29.5   0.   54.6   2.    1.   42.05  1.    1.   87.6   1.3   0.\n",
      "  0.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 3.    33.4    2.    55.55   0.     1.    44.475  1.     1.    87.6\n",
      " 43.125  1.     1.   ], Action:[0.6], Reward: 43.1, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 5, New State: [ 4.    34.3    2.    79.9    1.     1.    57.1    1.     1.    73.1\n",
      " 43.125  1.     1.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 5.    38.6    1.    71.8    1.     1.    55.2    1.     1.    73.1\n",
      " 43.125  1.     1.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 6.    38.4    2.    52.7    2.     1.    45.55   1.     1.    73.1\n",
      " 43.125  1.     1.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 7.   46.8   1.   63.48  0.    1.   55.14  1.    1.   73.1  17.96  1.\n",
      "  2.  ], Action:[0.9], Reward: 18.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 8.   45.5   0.   47.7   2.    1.   46.6   1.    1.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [ 9.   46.4   2.   46.3   1.    1.   46.35  1.    1.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [10.   46.    0.   33.5   1.    1.   46.    1.    0.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [11.   51.6   0.   48.2   2.    1.   49.9   1.    1.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [12.   60.7   1.   36.    1.    0.   -1.    0.    0.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [13.   70.1   1.   33.5   1.    0.   -1.    0.    0.   65.1  17.96  1.\n",
      "  2.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [14.   69.    1.   62.03  0.    1.   62.03  0.    1.   65.1   3.07  0.\n",
      "  3.  ], Action:[1.], Reward: 3.1, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [15.   64.3   1.   41.32  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [16.   63.8   2.   41.17  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [17.   59.2   1.   37.8   1.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [18.   59.9   1.   35.7   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [19.   75.3   1.   53.5   0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [20.   64.4   2.   42.21  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [21.   72.    0.   38.9   1.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [22.   70.2   1.   38.3   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [23.   73.1   2.   25.5   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [24.   64.6   2.   24.1   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [25.   69.    2.   49.17  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [26.   66.5   2.   33.5   1.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [27.   67.    1.   43.2   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [28.   82.5   1.   38.01  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [29.   63.5   2.   48.09  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [30.   71.7   1.   42.08  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [31.   71.6   0.   43.97  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [32.   69.9   1.   47.43  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [33.   75.9   1.   32.2   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [34.   69.3   2.   40.12  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [35.   70.2   2.   27.4   1.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [36.   63.6   2.   53.39  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [37.   64.3   2.   40.4   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [38.   72.4   0.   42.5   2.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [39.   68.    2.   36.26  0.    0.   -1.    0.    0.   60.9   3.07  0.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [40.   60.9   1.   39.11  0.    1.   60.9   1.    0.   60.9   0.    1.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [41.  70.   1.  39.8  2.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [42.  63.4  2.  36.7  1.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [43.  68.1  0.  42.1  2.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [44.  76.2  2.  42.4  2.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [45.  76.5  1.  38.1  2.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [46.  72.5  2.  38.8  2.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [47.  68.2  0.  38.7  1.   0.  -1.   0.   0.  53.4  0.   1.   4. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [48.   74.9   2.   32.05  0.    0.   -1.    0.    0.   53.4   0.    0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [49.  70.8  0.  28.4  2.   0.  -1.   0.   0.  53.4  0.   0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 5, New State: [50.  73.6  1.  38.3  2.   0.  -1.   0.   0.  53.4  0.   0.   4. ], Action:[0.6], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 6, New State: [ 1.  29.4  1.  74.8  1.   1.  52.1  1.   1.  87.6  0.   0.   0. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 2.  29.1  0.  86.5  2.   1.  57.8  1.   1.  87.6  0.   0.   0. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 3.  40.4  1.  70.4  2.   1.  55.4  1.   1.  87.6  0.   0.   0. ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 4.   35.1   2.   60.    2.    1.   47.55  1.    1.   87.6   0.    0.\n",
      "  0.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 5.   35.    2.   54.9   1.    1.   44.95  1.    1.   87.6   0.    0.\n",
      "  0.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 6.  41.3  0.  43.9  1.   1.  42.6  1.   1.  87.6  0.   0.   0. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 7.  40.8  2.  57.2  1.   1.  49.   1.   1.  87.6  0.   0.   0. ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 8.  54.   2.  52.   2.   1.  53.   1.   1.  87.6  0.   0.   0. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [ 9.  47.8  1.  50.4  2.   1.  49.1  1.   1.  87.6  0.   0.   0. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [10.   50.4   0.   61.48  0.    1.   55.94  1.    1.   87.6  31.66  1.\n",
      "  1.  ], Action:[0.7], Reward: 31.7, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [11.   62.8   1.   29.2   1.    0.   -1.    0.    0.   73.1  31.66  1.\n",
      "  1.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [12.   58.3   0.   58.86  0.    1.   58.58  1.    1.   73.1  14.52  1.\n",
      "  2.  ], Action:[0.8], Reward: 14.5, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [13.   67.    1.   40.8   2.    0.   -1.    0.    0.   65.1  14.52  1.\n",
      "  2.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [14.   75.1   1.   33.16  0.    0.   -1.    0.    0.   65.1  14.52  0.\n",
      "  2.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [15.   76.    2.   35.1   2.    0.   -1.    0.    0.   65.1  14.52  0.\n",
      "  2.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [16.   73.5   1.   48.46  0.    0.   -1.    0.    0.   65.1  14.52  0.\n",
      "  2.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [17.   72.2   0.   42.6   2.    0.   -1.    0.    0.   65.1  14.52  0.\n",
      "  2.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [18.   58.6   1.   51.41  0.    1.   58.6   1.    0.   65.1   6.5   1.\n",
      "  3.  ], Action:[0.8], Reward: 6.5, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [19.   74.    1.   46.21  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [20.  67.1  1.  35.6  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [21.   73.7   2.   53.74  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [22.  63.9  1.  32.7  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [23.  67.7  2.  34.7  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [24.   70.5   2.   60.33  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [25.   70.7   2.   45.65  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [26.   74.1   2.   31.82  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [27.  60.1  1.  37.7  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [28.   68.4   0.   58.89  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [29.  64.2  1.  38.9  1.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [30.  69.5  2.  42.7  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [31.  62.1  2.  25.5  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [32.  74.7  2.  41.5  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [33.  68.8  2.  31.6  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [34.   64.9   2.   48.69  0.    0.   -1.    0.    0.   60.9   6.5   0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [35.  62.3  1.  38.9  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [36.  61.4  2.  34.8  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [37.  68.2  0.  30.9  2.   0.  -1.   0.   0.  60.9  6.5  0.   3. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [38.  60.5  1.  34.3  0.   1.  60.5  1.   0.  60.9  0.4  1.   4. ], Action:[0.6], Reward: 0.4, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [39.   68.1   0.   47.17  0.    0.   -1.    0.    0.   53.4   0.4   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [40.  73.   2.  39.2  1.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [41.   73.3   0.   47.53  0.    0.   -1.    0.    0.   53.4   0.4   0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [42.  64.3  1.  37.2  1.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [43.  68.3  1.  37.7  2.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [44.  66.1  2.  26.8  1.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [45.  63.7  2.  34.   2.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [46.  77.2  2.  37.5  2.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [47.  72.5  1.  40.8  2.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.3], Reward: 0.0, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 6, New State: [48.  72.6  2.  51.6  0.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [49.  68.2  0.  38.7  1.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 6, New State: [50.  79.1  2.  39.8  0.   0.  -1.   0.   0.  53.4  0.4  0.   4. ], Action:[0.7], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 7, New State: [ 1.  23.9  1.  98.1  2.   1.  61.   1.   1.  87.6  0.4  0.   0. ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 2.  35.5  1.  78.5  2.   1.  57.   1.   1.  87.6  0.4  0.   0. ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 3.   31.4   0.   78.1   1.    1.   54.75  1.    1.   87.6   0.4   0.\n",
      "  0.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 4.   43.3   0.   84.06  0.    1.   63.68  1.    1.   87.6  23.92  1.\n",
      "  1.  ], Action:[1.], Reward: 23.9, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 5.   37.    2.   55.4   2.    1.   46.2   1.    1.   73.1  23.92  1.\n",
      "  1.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 6.   38.4   2.   73.7   1.    1.   56.05  1.    1.   73.1  23.92  1.\n",
      "  1.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 7.   36.1   2.   53.7   1.    1.   44.9   1.    1.   73.1  23.92  1.\n",
      "  1.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 8.   42.5   2.   51.8   1.    1.   47.15  1.    1.   73.1  23.92  1.\n",
      "  1.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [ 9.   45.9   0.   62.98  0.    1.   54.44  1.    1.   73.1  18.66  1.\n",
      "  2.  ], Action:[0.9], Reward: 18.7, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [10.   50.5   0.   54.8   2.    1.   52.65  1.    1.   65.1  18.66  1.\n",
      "  2.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [11.   45.7   1.   50.3   2.    1.   48.    1.    1.   65.1  18.66  1.\n",
      "  2.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [12.   59.2   1.   32.1   1.    0.   -1.    0.    0.   65.1  18.66  1.\n",
      "  2.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [13.   67.    2.   41.1   2.    0.   -1.    0.    0.   65.1  18.66  1.\n",
      "  2.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [14.   67.3   1.   38.61  0.    0.   -1.    0.    0.   65.1  18.66  0.\n",
      "  2.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [15.   67.1   2.   44.    0.    0.   -1.    0.    0.   65.1  18.66  0.\n",
      "  2.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [16.   70.6   1.   57.66  0.    1.   57.66  0.    1.   65.1   7.44  0.\n",
      "  3.  ], Action:[0.9], Reward: 7.4, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [17.   65.9   2.   40.3   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [18.   69.4   1.   41.88  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [19.   67.9   0.   32.5   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [20.   70.8   2.   39.    2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [21.   85.8   2.   37.9   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [22.   64.6   2.   60.08  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [23.   64.8   2.   56.36  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [24.   64.8   1.   41.1   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [25.   61.8   2.   50.93  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [26.   65.4   1.   54.4   0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [27.   73.6   2.   39.4   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [28.   73.2   2.   42.81  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [29.   67.3   1.   37.8   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [30.   66.9   2.   50.22  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [31.   76.8   0.   30.7   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [32.   85.3   0.   32.1   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [33.   62.1   1.   35.6   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [34.   64.3   1.   49.1   0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [35.   67.5   2.   28.7   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [36.   64.8   2.   52.48  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [37.   63.    1.   36.2   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [38.   61.3   2.   38.6   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [39.   67.6   1.   45.63  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [40.   59.7   1.   35.6   1.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [41.   73.7   2.   49.51  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [42.   65.9   2.   44.82  0.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [43.   74.5   2.   33.8   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [44.   64.2   1.   23.7   2.    0.   -1.    0.    0.   60.9   7.44  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [45.    60.8    1.    60.07   0.     1.    60.435  1.     1.    60.9\n",
      "  0.465  1.     4.   ], Action:[1.], Reward: 0.5, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [46.    73.5    2.    36.5    1.     0.    -1.     0.     0.    53.4\n",
      "  0.465  1.     4.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [47.    71.9    0.    38.9    2.     0.    -1.     0.     0.    53.4\n",
      "  0.465  1.     4.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [48.    70.     2.    38.4    2.     0.    -1.     0.     0.    53.4\n",
      "  0.465  1.     4.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [49.    64.5    2.    45.08   0.     0.    -1.     0.     0.    53.4\n",
      "  0.465  0.     4.   ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 7, New State: [50.    74.3    1.    34.1    1.     0.    -1.     0.     0.    53.4\n",
      "  0.465  0.     4.   ], Action:[0.2], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 8, New State: [ 1.    27.4    1.    98.     2.     1.    62.7    1.     1.    87.6\n",
      "  0.465  0.     0.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 2.    32.8    0.    59.9    1.     1.    46.35   1.     1.    87.6\n",
      "  0.465  0.     0.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 3.    34.3    1.    58.     1.     1.    46.15   1.     1.    87.6\n",
      "  0.465  0.     0.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 4.    39.2    0.    81.57   0.     1.    60.385  1.     1.    87.6\n",
      " 27.215  1.     1.   ], Action:[0.9], Reward: 27.2, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 5.    40.4    2.    83.4    2.     1.    61.9    1.     1.    73.1\n",
      " 27.215  1.     1.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 6.    35.6    2.    53.7    1.     1.    44.65   1.     1.    73.1\n",
      " 27.215  1.     1.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 7.    50.7    2.    56.8    2.     1.    53.75   1.     1.    73.1\n",
      " 27.215  1.     1.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 8.    48.1    2.    49.2    1.     1.    48.65   1.     1.    73.1\n",
      " 27.215  1.     1.   ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [ 9.    47.7    1.    58.71   0.     1.    53.205  1.     1.    73.1\n",
      " 19.895  1.     2.   ], Action:[0.8], Reward: 19.9, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [10.    46.5    0.    47.5    2.     1.    47.     1.     1.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [11.    51.1    0.    53.2    2.     1.    52.15   1.     1.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [12.    81.7    1.    31.2    2.     0.    -1.     0.     0.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [13.    63.6    1.    39.8    2.     0.    -1.     0.     0.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [14.    72.9    2.    39.5    2.     0.    -1.     0.     0.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [15.    63.6    1.    38.     2.     0.    -1.     0.     0.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [16.    62.4    2.    40.4    2.     0.    -1.     0.     0.    65.1\n",
      " 19.895  1.     2.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [17.    76.7    1.    44.19   0.     0.    -1.     0.     0.    65.1\n",
      " 19.895  0.     2.   ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [18.   78.8   1.   63.78  0.    1.   63.78  0.    1.   65.1   1.32  0.\n",
      "  3.  ], Action:[1.], Reward: 1.3, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [19.   78.4   1.   59.46  0.    1.   59.46  0.    1.   60.9   1.44  0.\n",
      "  4.  ], Action:[1.], Reward: 1.4, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [20.   67.2   1.   38.1   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 8, New State: [21.   65.    2.   49.    0.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [22.   79.5   2.   31.9   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [23.   68.8   2.   32.38  0.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [24.   63.7   2.   40.3   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [25.   64.5   1.   39.    2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [26.   63.4   2.   33.4   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [27.   75.    0.   38.6   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [28.   85.6   2.   38.6   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [29.   63.3   2.   32.9   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [30.   72.3   1.   36.2   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [31.   65.9   2.   42.4   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [32.   79.6   0.   29.6   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [33.   63.8   2.   42.8   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [34.   77.8   0.   36.    2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [35.   85.1   2.   31.7   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [36.   77.8   0.   36.4   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [37.   74.7   0.   37.3   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [38.   64.6   1.   45.28  0.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [39.   66.6   2.   35.1   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [40.   62.    2.   42.4   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [41.   64.8   2.   35.2   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [42.   71.7   2.   24.1   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [43.   73.5   1.   38.8   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [44.   71.8   0.   38.8   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [45.   75.3   2.   42.3   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [46.   75.3   1.   33.7   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [47.   87.1   1.   23.3   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [48.   61.3   2.   41.4   2.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [49.   66.1   2.   37.99  0.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 8, New State: [50.   82.9   2.   34.6   1.    0.   -1.    0.    0.   53.4   1.44  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 9, New State: [ 1.   33.    1.   75.7   1.    1.   54.35  1.    1.   87.6   1.44  0.\n",
      "  0.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 2.   32.7   1.   70.1   1.    1.   51.4   1.    1.   87.6   1.44  0.\n",
      "  0.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 3.   32.    2.   81.14  0.    1.   56.57  1.    1.   87.6  31.03  1.\n",
      "  1.  ], Action:[0.9], Reward: 31.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 4.   30.3   0.   54.1   2.    1.   42.2   1.    1.   73.1  31.03  1.\n",
      "  1.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 5.    43.1    2.    72.75   0.     1.    57.925  1.     1.    73.1\n",
      " 15.175  1.     2.   ], Action:[1.], Reward: 15.2, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 6.    51.3    1.    75.6    1.     1.    63.45   1.     1.    65.1\n",
      " 15.175  1.     2.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 7.    38.2    2.    59.2    2.     1.    48.7    1.     1.    65.1\n",
      " 15.175  1.     2.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 8.   43.9   2.   58.98  0.    1.   51.44  1.    1.   65.1  13.66  1.\n",
      "  3.  ], Action:[0.9], Reward: 13.7, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [ 9.   38.8   0.   60.    1.    1.   49.4   1.    1.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [10.   55.2   0.   39.4   2.    1.   55.2   1.    0.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [11.   46.1   0.   33.5   2.    1.   46.1   1.    0.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [12.   65.8   1.   42.2   2.    0.   -1.    0.    0.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [13.   72.4   1.   50.9   2.    0.   -1.    0.    0.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [14.   65.1   1.   44.5   2.    0.   -1.    0.    0.   60.9  13.66  1.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [15.   66.3   2.   53.35  0.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [16.   58.6   1.   37.2   1.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [17.   55.6   1.   38.5   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [18.   59.    1.   52.5   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [19.   68.2   1.   38.64  0.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [20.   77.7   1.   50.79  0.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [21.   63.4   1.   42.95  0.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [22.   65.8   2.   50.9   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [23.   75.4   0.   40.8   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [24.   61.4   2.   38.2   1.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [25.   70.4   2.   51.05  0.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [26.   61.4   1.   35.3   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [27.   67.3   1.   37.2   1.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [28.   63.3   2.   33.1   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [29.   71.3   1.   35.9   2.    0.   -1.    0.    0.   60.9  13.66  0.\n",
      "  3.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [30.   62.9   1.   57.17  0.    1.   57.17  0.    1.   60.9   3.73  0.\n",
      "  4.  ], Action:[0.9], Reward: 3.7, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [31.   73.2   1.   38.68  0.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [32.   74.7   1.   52.92  0.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [33.   68.4   2.   43.1   0.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [34.   68.2   2.   39.3   1.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [35.   72.4   0.   41.1   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [36.   61.5   1.   47.6   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [37.   67.3   2.   36.1   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [38.   61.3   1.   49.2   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [39.   75.7   0.   35.7   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [40.   84.    0.   37.8   1.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [41.   73.4   2.   44.84  0.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [42.   70.3   2.   42.1   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [43.   62.8   1.   44.2   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [44.   77.4   2.   37.    1.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnd: 0, Period: 9, New State: [45.   62.4   1.   52.6   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [46.   73.3   0.   30.2   1.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [47.   62.1   1.   52.1   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [48.   76.5   2.   43.2   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [49.   63.8   1.   34.8   1.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 9, New State: [50.   65.1   2.   39.1   2.    0.   -1.    0.    0.   53.4   3.73  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: True\n",
      "Rnd: 0, Period: 10, New State: [ 1.   29.2   1.   70.3   1.    1.   49.75  1.    1.   87.6   3.73  0.\n",
      "  0.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 2.   28.8   0.   56.2   2.    1.   42.5   1.    1.   87.6   3.73  0.\n",
      "  0.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 3.   35.8   1.   67.7   2.    1.   51.75  1.    1.   87.6   3.73  0.\n",
      "  0.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 4.   32.9   2.   61.4   1.    1.   47.15  1.    1.   87.6   3.73  0.\n",
      "  0.  ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 5.   38.5   2.   45.9   1.    1.   42.2   1.    1.   87.6   3.73  0.\n",
      "  0.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 6.    36.3    2.    85.03   0.     1.    60.665  1.     1.    87.6\n",
      " 26.935  1.     1.   ], Action:[1.], Reward: 26.9, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 7.    46.     2.    50.4    1.     1.    48.2    1.     1.    73.1\n",
      " 26.935  1.     1.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 8.    43.4    0.    49.1    2.     1.    46.25   1.     1.    73.1\n",
      " 26.935  1.     1.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [ 9.    56.7    1.    45.7    2.     1.    51.2    1.     1.    73.1\n",
      " 26.935  1.     1.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [10.    61.6    0.    37.2    2.     0.    -1.     0.     0.    73.1\n",
      " 26.935  1.     1.   ], Action:[0.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [11.    52.5    0.    28.9    2.     1.    52.5    1.     0.    73.1\n",
      " 26.935  1.     1.   ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [12.   52.7   0.   37.74  0.    1.   52.7   1.    0.   73.1  20.4   1.\n",
      "  2.  ], Action:[0.5], Reward: 20.4, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [13.    59.4    1.    57.21   0.     1.    58.305  1.     1.    65.1\n",
      "  6.795  1.     3.   ], Action:[0.9], Reward: 6.8, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [14.    76.3    0.    43.5    2.     0.    -1.     0.     0.    60.9\n",
      "  6.795  1.     3.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [15.    60.3    1.    32.9    1.     0.    -1.     0.     0.    60.9\n",
      "  6.795  1.     3.   ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [16.    68.4    1.    40.     2.     0.    -1.     0.     0.    60.9\n",
      "  6.795  1.     3.   ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [17.    73.5    2.    32.7    1.     0.    -1.     0.     0.    60.9\n",
      "  6.795  1.     3.   ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [18.    69.1    0.    39.5    2.     0.    -1.     0.     0.    60.9\n",
      "  6.795  1.     3.   ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [19.    65.9    2.    56.87   0.     0.    -1.     0.     0.    60.9\n",
      "  6.795  0.     3.   ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [20.    59.5    1.    40.4    2.     0.    -1.     0.     0.    60.9\n",
      "  6.795  0.     3.   ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [21.   72.1   1.   58.95  0.    1.   58.95  0.    1.   60.9   1.95  0.\n",
      "  4.  ], Action:[1.], Reward: 1.9, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [22.   81.7   2.   44.04  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [23.   69.2   0.   41.1   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [24.   65.1   1.   51.09  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[1.], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [25.   74.2   1.   37.77  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [26.   68.1   0.   36.3   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [27.   80.7   0.   42.8   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [28.   68.6   2.   47.27  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [29.   65.7   2.   42.34  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.8], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [30.   80.8   2.   34.2   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [31.   67.9   1.   34.99  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [32.   64.8   1.   42.3   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [33.   75.7   0.   48.18  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [34.   63.5   2.   41.3   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [35.   75.4   0.   46.13  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [36.   80.4   2.   43.6   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.6], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [37.   77.7   0.   32.5   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [38.   88.    2.   47.66  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [39.   82.7   2.   42.5   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [40.   71.9   1.   26.7   1.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.4], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [41.   62.7   2.   50.5   0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [42.   66.9   1.   41.6   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.7], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [43.   68.7   1.   41.1   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.5], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [44.   65.7   1.   24.1   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [45.   72.2   2.   40.9   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.3], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [46.   67.    1.   39.7   1.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [47.   64.7   1.   42.3   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [48.   66.    2.   37.6   1.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.2], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [49.   73.5   1.   45.91  0.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.9], Reward: 0.0, Period End: False\n",
      "Rnd: 0, Period: 10, New State: [50.   72.    2.   37.6   2.    0.   -1.    0.    0.   53.4   1.95  0.\n",
      "  4.  ], Action:[0.1], Reward: 0.0, Period End: True\n"
     ]
    }
   ],
   "source": [
    "rnd = 0\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "observation, info = env.reset()\n",
    "for period in count():\n",
    "    for timestep in count(): \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        print(f\"Rnd: {rnd}, Period: {period}, New State: {observation}, Action:{np.round(action,1)}, Reward: {np.round(reward,1)}, Period End: {done}\")\n",
    "        if done:\n",
    "            # If the episode is done, reset the environment\n",
    "            #print('done')\n",
    "            observation, info = env.reset()\n",
    "            break\n",
    "    if period == nperiods:\n",
    "        period = 0\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8540088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnd</th>\n",
       "      <th>period</th>\n",
       "      <th>step</th>\n",
       "      <th>current_bid</th>\n",
       "      <th>current_ask</th>\n",
       "      <th>current_ask_idx</th>\n",
       "      <th>buy</th>\n",
       "      <th>sell</th>\n",
       "      <th>price</th>\n",
       "      <th>sale</th>\n",
       "      <th>bprofit</th>\n",
       "      <th>sprofit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current_bid_idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>816</td>\n",
       "      <td>4035</td>\n",
       "      <td>8201.29</td>\n",
       "      <td>10723.9</td>\n",
       "      <td>217</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>2479.67</td>\n",
       "      <td>44</td>\n",
       "      <td>674.03</td>\n",
       "      <td>471.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>651</td>\n",
       "      <td>2597</td>\n",
       "      <td>5343.90</td>\n",
       "      <td>7157.6</td>\n",
       "      <td>140</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>2233.95</td>\n",
       "      <td>44</td>\n",
       "      <td>1117.75</td>\n",
       "      <td>696.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1283</td>\n",
       "      <td>6843</td>\n",
       "      <td>11814.60</td>\n",
       "      <td>16869.4</td>\n",
       "      <td>301</td>\n",
       "      <td>49</td>\n",
       "      <td>48</td>\n",
       "      <td>2645.60</td>\n",
       "      <td>51</td>\n",
       "      <td>1101.10</td>\n",
       "      <td>859.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 rnd  period  step  current_bid  current_ask  current_ask_idx  \\\n",
       "current_bid_idx                                                                 \n",
       "0                  0     816  4035      8201.29      10723.9              217   \n",
       "1                  0     651  2597      5343.90       7157.6              140   \n",
       "2                  0    1283  6843     11814.60      16869.4              301   \n",
       "\n",
       "                 buy  sell    price  sale  bprofit  sprofit  \n",
       "current_bid_idx                                              \n",
       "0                 35    38  2479.67    44   674.03   471.97  \n",
       "1                 44    41  2233.95    44  1117.75   696.55  \n",
       "2                 49    48  2645.60    51  1101.10   859.20  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.step_data.head(1000).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7b34a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your environment and parameters (replace with your actual environment setup)\n",
    "rnd = 0\n",
    "period = 0\n",
    "num_states = nsteps\n",
    "min_frac = 0.01\n",
    "max_frac = 1.5\n",
    "eval_steps = 1000\n",
    "training_step = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29040eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad71ec",
   "metadata": {},
   "source": [
    "### Continous Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e856dcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5244ee4b4c45d5b72a0efd7e77c29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -29      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.249   |\n",
      "|    critic_loss     | 6.65     |\n",
      "|    ent_coef        | 0.915    |\n",
      "|    ent_coef_loss   | -0.147   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -24.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.643    |\n",
      "|    critic_loss     | 8.26     |\n",
      "|    ent_coef        | 0.862    |\n",
      "|    ent_coef_loss   | -0.246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -16.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 21       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.37     |\n",
      "|    critic_loss     | 8.16     |\n",
      "|    ent_coef        | 0.813    |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -15.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 1000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.932    |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.77     |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -12.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 20       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.48     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.732    |\n",
      "|    ent_coef_loss   | -0.375   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -9.36    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 22       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 1400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.36     |\n",
      "|    critic_loss     | 6.65     |\n",
      "|    ent_coef        | 0.698    |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -6.96    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 24       |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.1      |\n",
      "|    critic_loss     | 6.62     |\n",
      "|    ent_coef        | 0.666    |\n",
      "|    ent_coef_loss   | -0.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -4.07    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 26       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.6      |\n",
      "|    critic_loss     | 3.88     |\n",
      "|    ent_coef        | 0.636    |\n",
      "|    ent_coef_loss   | -0.397   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -2.13    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.26     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.608    |\n",
      "|    ent_coef_loss   | -0.451   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -0.556   |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 2200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.15     |\n",
      "|    critic_loss     | 7.14     |\n",
      "|    ent_coef        | 0.581    |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 0.989    |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.74     |\n",
      "|    critic_loss     | 8.58     |\n",
      "|    ent_coef        | 0.555    |\n",
      "|    ent_coef_loss   | -0.353   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 2.75     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 2600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.37     |\n",
      "|    critic_loss     | 6.17     |\n",
      "|    ent_coef        | 0.529    |\n",
      "|    ent_coef_loss   | -0.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 3.45     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 2800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.47     |\n",
      "|    critic_loss     | 7.11     |\n",
      "|    ent_coef        | 0.503    |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 4.46     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.11     |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.478    |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 4.72     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4        |\n",
      "|    critic_loss     | 4.15     |\n",
      "|    ent_coef        | 0.455    |\n",
      "|    ent_coef_loss   | -0.688   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 5.06     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 3400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.4      |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.432    |\n",
      "|    ent_coef_loss   | -0.655   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 5.56     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 3600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.27     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.412    |\n",
      "|    ent_coef_loss   | -0.655   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 6.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 3800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.97     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.392    |\n",
      "|    ent_coef_loss   | -0.719   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 6.49     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.99     |\n",
      "|    critic_loss     | 5.27     |\n",
      "|    ent_coef        | 0.373    |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 7.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 4200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.98     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.355    |\n",
      "|    ent_coef_loss   | -0.899   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 7.59     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 4400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.55     |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    ent_coef        | 0.338    |\n",
      "|    ent_coef_loss   | -0.765   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 7.8      |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 4600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.05     |\n",
      "|    critic_loss     | 3.16     |\n",
      "|    ent_coef        | 0.321    |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 8.21     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 2.89     |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -0.578   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 8.42     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.59     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.29     |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 9.86     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 5200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.15     |\n",
      "|    critic_loss     | 3.21     |\n",
      "|    ent_coef        | 0.276    |\n",
      "|    ent_coef_loss   | -0.849   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 11.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 5400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.66     |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.262    |\n",
      "|    ent_coef_loss   | -0.733   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 13.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.45     |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.248    |\n",
      "|    ent_coef_loss   | -0.902   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 14.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 5800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.95     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.235    |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.86     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 16.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 6200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.18     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.21     |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6099     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.19     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.198    |\n",
      "|    ent_coef_loss   | -0.799   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 6600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.78     |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | -0.995   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 6800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.9      |\n",
      "|    critic_loss     | 5.09     |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.06     |\n",
      "|    critic_loss     | 3.63     |\n",
      "|    ent_coef        | 0.166    |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.81     |\n",
      "|    critic_loss     | 2.67     |\n",
      "|    ent_coef        | 0.156    |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 7400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.43     |\n",
      "|    critic_loss     | 2.75     |\n",
      "|    ent_coef        | 0.147    |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 7600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.4      |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 7800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.87     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.131    |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.42     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | -0.753   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 8200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.03     |\n",
      "|    critic_loss     | 4.07     |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -0.943   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 8400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.3      |\n",
      "|    critic_loss     | 2.77     |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 8600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.14     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.45     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.098    |\n",
      "|    ent_coef_loss   | -1.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.43     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0927   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 9200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.94     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0878   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9099     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 9400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.19     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.083    |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.82     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0786   |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 9800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.06     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0746   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 28.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0707   |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 29.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 10200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.4      |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.067    |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 29.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 300      |\n",
      "|    total_timesteps | 10400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.58     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0635   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 30.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 10600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.77     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0604   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 30.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 10800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.6      |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 31.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.81     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 31.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.9      |\n",
      "|    critic_loss     | 0.738    |\n",
      "|    ent_coef        | 0.0518   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 31.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 321      |\n",
      "|    total_timesteps | 11400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.18     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0492   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 31.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 325      |\n",
      "|    total_timesteps | 11600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.07     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 32.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 11800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.928    |\n",
      "|    critic_loss     | 0.982    |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 32.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.04     |\n",
      "|    critic_loss     | 0.891    |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 32.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 12200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.11     |\n",
      "|    critic_loss     | 0.761    |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 33.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 341      |\n",
      "|    total_timesteps | 12400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.975    |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 33.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 12600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.775    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.866   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 12800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.87     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 353      |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.527    |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 13200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.586    |\n",
      "|    critic_loss     | 0.393    |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 13400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.158   |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -0.749   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 13600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.699    |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.717   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13499    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 50        |\n",
      "|    ep_rew_mean     | 34.9      |\n",
      "| time/              |           |\n",
      "|    episodes        | 276       |\n",
      "|    fps             | 37        |\n",
      "|    time_elapsed    | 372       |\n",
      "|    total_timesteps | 13800     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.000471 |\n",
      "|    critic_loss     | 1.4       |\n",
      "|    ent_coef        | 0.0277    |\n",
      "|    ent_coef_loss   | -1.13     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 13699     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 14000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0678   |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 34.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 382      |\n",
      "|    total_timesteps | 14200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.173   |\n",
      "|    critic_loss     | 0.879    |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 387      |\n",
      "|    total_timesteps | 14400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.22    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -0.988   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35       |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 14600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0748  |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | -0.782   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 396      |\n",
      "|    total_timesteps | 14800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.208    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.553   |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 404      |\n",
      "|    total_timesteps | 15200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.662   |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.935   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 408      |\n",
      "|    total_timesteps | 15400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.357   |\n",
      "|    critic_loss     | 0.997    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 15600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.778   |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | -0.615   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 15800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.603   |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | -0.733   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 420      |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.0556  |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 35.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 16200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.696   |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36       |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 429      |\n",
      "|    total_timesteps | 16400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.53    |\n",
      "|    critic_loss     | 0.98     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -0.706   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 433      |\n",
      "|    total_timesteps | 16600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.693    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 16800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.984   |\n",
      "|    critic_loss     | 0.883    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 442      |\n",
      "|    total_timesteps | 17000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.52    |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.462    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 446      |\n",
      "|    total_timesteps | 17200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.606   |\n",
      "|    critic_loss     | 0.956    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 450      |\n",
      "|    total_timesteps | 17400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.45    |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.847   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 36.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 454      |\n",
      "|    total_timesteps | 17600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.562   |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.523   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 37.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 459      |\n",
      "|    total_timesteps | 17800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.815   |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 37.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 463      |\n",
      "|    total_timesteps | 18000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.864   |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.426   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 37.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 467      |\n",
      "|    total_timesteps | 18200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.902    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | -0.155   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 472      |\n",
      "|    total_timesteps | 18400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.224   |\n",
      "|    critic_loss     | 0.35     |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | -0.684   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 476      |\n",
      "|    total_timesteps | 18600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.934   |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 0.359    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 480      |\n",
      "|    total_timesteps | 18800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -0.843   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 485      |\n",
      "|    total_timesteps | 19000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.662   |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 489      |\n",
      "|    total_timesteps | 19200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 0.637    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 0.737    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 493      |\n",
      "|    total_timesteps | 19400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.756   |\n",
      "|    critic_loss     | 0.659    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 0.888    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 497      |\n",
      "|    total_timesteps | 19600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.69    |\n",
      "|    critic_loss     | 0.948    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 38.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 502      |\n",
      "|    total_timesteps | 19800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 0.714    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39       |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 506      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.02    |\n",
      "|    critic_loss     | 0.587    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 0.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 20200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.65    |\n",
      "|    critic_loss     | 0.746    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -0.382   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 20400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 0.553    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 519      |\n",
      "|    total_timesteps | 20600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.58    |\n",
      "|    critic_loss     | 0.993    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 523      |\n",
      "|    total_timesteps | 20800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 0.566    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 21000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.16    |\n",
      "|    critic_loss     | 0.952    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -0.165   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 532      |\n",
      "|    total_timesteps | 21200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.79     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 536      |\n",
      "|    total_timesteps | 21400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.21    |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 21600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.49    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.924   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 21800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.601   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 549      |\n",
      "|    total_timesteps | 22000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.22    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 0.847    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 553      |\n",
      "|    total_timesteps | 22200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.17    |\n",
      "|    critic_loss     | 0.831    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 558      |\n",
      "|    total_timesteps | 22400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.51    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 562      |\n",
      "|    total_timesteps | 22600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.633    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.416    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 566      |\n",
      "|    total_timesteps | 22800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.31    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 0.243    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 23000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.62    |\n",
      "|    critic_loss     | 0.918    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 0.338    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 464      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 575      |\n",
      "|    total_timesteps | 23200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.32    |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 0.607    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 580      |\n",
      "|    total_timesteps | 23400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.15    |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.197   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 472      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 584      |\n",
      "|    total_timesteps | 23600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 0.265    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 476      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 23800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -0.0908  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 593      |\n",
      "|    total_timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.64    |\n",
      "|    critic_loss     | 0.594    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.389    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 484      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 24200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.17    |\n",
      "|    critic_loss     | 0.673    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -0.0359  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 488      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 602      |\n",
      "|    total_timesteps | 24400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.71    |\n",
      "|    critic_loss     | 0.769    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -0.0184  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 492      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 606      |\n",
      "|    total_timesteps | 24600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.15    |\n",
      "|    critic_loss     | 0.741    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -0.745   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 496      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 611      |\n",
      "|    total_timesteps | 24800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.695    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.163    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.63    |\n",
      "|    critic_loss     | 0.664    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -0.0945  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 504      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 620      |\n",
      "|    total_timesteps | 25200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.44    |\n",
      "|    critic_loss     | 0.97     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -0.0608  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 624      |\n",
      "|    total_timesteps | 25400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.42    |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.672   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 512      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 630      |\n",
      "|    total_timesteps | 25600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.87    |\n",
      "|    critic_loss     | 0.608    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.783   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 516      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 634      |\n",
      "|    total_timesteps | 25800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.938   |\n",
      "|    critic_loss     | 0.729    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -0.0552  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 639      |\n",
      "|    total_timesteps | 26000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.23    |\n",
      "|    critic_loss     | 0.404    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 524      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 643      |\n",
      "|    total_timesteps | 26200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2       |\n",
      "|    critic_loss     | 0.923    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 0.613    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 528      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 648      |\n",
      "|    total_timesteps | 26400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.89    |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -0.0485  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 532      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 652      |\n",
      "|    total_timesteps | 26600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.842   |\n",
      "|    critic_loss     | 0.428    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -0.754   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 536      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 26800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 0.951    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 2        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 662      |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.336   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 544      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 666      |\n",
      "|    total_timesteps | 27200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.77    |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 0.0785   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 548      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 671      |\n",
      "|    total_timesteps | 27400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.0385  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 552      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 676      |\n",
      "|    total_timesteps | 27600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6     |\n",
      "|    critic_loss     | 0.478    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 0.704    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 556      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 680      |\n",
      "|    total_timesteps | 27800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.75    |\n",
      "|    critic_loss     | 0.628    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.474    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 685      |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.12    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.187    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 564      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 690      |\n",
      "|    total_timesteps | 28200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 0.862    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 568      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 696      |\n",
      "|    total_timesteps | 28400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.93    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.0151  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 572      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 701      |\n",
      "|    total_timesteps | 28600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.697    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.924    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 576      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 707      |\n",
      "|    total_timesteps | 28800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.16    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.66     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 713      |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.53    |\n",
      "|    critic_loss     | 0.906    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 584      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 718      |\n",
      "|    total_timesteps | 29200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.52    |\n",
      "|    critic_loss     | 0.836    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -0.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 588      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 722      |\n",
      "|    total_timesteps | 29400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 592      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 727      |\n",
      "|    total_timesteps | 29600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.03    |\n",
      "|    critic_loss     | 0.575    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.554   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 596      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 732      |\n",
      "|    total_timesteps | 29800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.88    |\n",
      "|    critic_loss     | 0.403    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.096    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 736      |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.74    |\n",
      "|    critic_loss     | 0.559    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 604      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 741      |\n",
      "|    total_timesteps | 30200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3       |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.172    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 608      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 30400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 0.757    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 612      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 750      |\n",
      "|    total_timesteps | 30600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.68    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 616      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 755      |\n",
      "|    total_timesteps | 30800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.46    |\n",
      "|    critic_loss     | 0.754    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 760      |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.154   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 624      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 764      |\n",
      "|    total_timesteps | 31200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.24    |\n",
      "|    critic_loss     | 0.523    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.853   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 628      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 769      |\n",
      "|    total_timesteps | 31400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.14    |\n",
      "|    critic_loss     | 0.422    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 632      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 774      |\n",
      "|    total_timesteps | 31600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.754    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 636      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 778      |\n",
      "|    total_timesteps | 31800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.96    |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -0.795   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 783      |\n",
      "|    total_timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.87    |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 644      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 788      |\n",
      "|    total_timesteps | 32200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.19    |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.0815  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 648      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 32400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 0.989    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.446    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 652      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 798      |\n",
      "|    total_timesteps | 32600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.58    |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 656      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 803      |\n",
      "|    total_timesteps | 32800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.34    |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 808      |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 0.101    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 664      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 813      |\n",
      "|    total_timesteps | 33200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2       |\n",
      "|    critic_loss     | 0.751    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 668      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 819      |\n",
      "|    total_timesteps | 33400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 0.909    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 0.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 672      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 824      |\n",
      "|    total_timesteps | 33600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.85    |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.127   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 676      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 829      |\n",
      "|    total_timesteps | 33800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.618    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 835      |\n",
      "|    total_timesteps | 34000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.03    |\n",
      "|    critic_loss     | 0.863    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 684      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 34200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.94    |\n",
      "|    critic_loss     | 0.561    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.858   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 688      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 845      |\n",
      "|    total_timesteps | 34400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.67    |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -0.0334  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 692      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 850      |\n",
      "|    total_timesteps | 34600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 696      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 855      |\n",
      "|    total_timesteps | 34800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.324    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 861      |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.29    |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.492    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 704      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 866      |\n",
      "|    total_timesteps | 35200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 0.748    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.017   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 708      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 871      |\n",
      "|    total_timesteps | 35400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 0.669    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -0.123   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 712      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 876      |\n",
      "|    total_timesteps | 35600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.6     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.845    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 716      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 881      |\n",
      "|    total_timesteps | 35800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.74    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.575   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 887      |\n",
      "|    total_timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.33    |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 724      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 892      |\n",
      "|    total_timesteps | 36200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.12    |\n",
      "|    critic_loss     | 0.462    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 728      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 897      |\n",
      "|    total_timesteps | 36400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.11    |\n",
      "|    critic_loss     | 0.583    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 732      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 902      |\n",
      "|    total_timesteps | 36600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.53    |\n",
      "|    critic_loss     | 0.936    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.264   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 736      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 908      |\n",
      "|    total_timesteps | 36800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 0.469    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.269    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 913      |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.4     |\n",
      "|    critic_loss     | 0.534    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | -0.616   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 744      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 918      |\n",
      "|    total_timesteps | 37200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.81    |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.0837   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 748      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 924      |\n",
      "|    total_timesteps | 37400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.45    |\n",
      "|    critic_loss     | 0.924    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.125   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 752      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 929      |\n",
      "|    total_timesteps | 37600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.64    |\n",
      "|    critic_loss     | 0.684    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.303   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 756      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 934      |\n",
      "|    total_timesteps | 37800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.07    |\n",
      "|    critic_loss     | 0.212    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.606   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 38000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.39    |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.375    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 764      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 945      |\n",
      "|    total_timesteps | 38200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.873   |\n",
      "|    critic_loss     | 0.511    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 768      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 951      |\n",
      "|    total_timesteps | 38400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.95    |\n",
      "|    critic_loss     | 0.577    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 772      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 956      |\n",
      "|    total_timesteps | 38600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.61    |\n",
      "|    critic_loss     | 0.878    |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 0.0206   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 776      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 961      |\n",
      "|    total_timesteps | 38800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.34    |\n",
      "|    critic_loss     | 0.695    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.0938   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 967      |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.56    |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 784      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 972      |\n",
      "|    total_timesteps | 39200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.53    |\n",
      "|    critic_loss     | 0.922    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39099    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 39.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 788      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 977      |\n",
      "|    total_timesteps | 39400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.91    |\n",
      "|    critic_loss     | 0.739    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 792      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 39600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.73    |\n",
      "|    critic_loss     | 0.772    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.0748   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 796      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 988      |\n",
      "|    total_timesteps | 39800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.52    |\n",
      "|    critic_loss     | 0.645    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.427   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 993      |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.48    |\n",
      "|    critic_loss     | 0.449    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.463   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 804      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 999      |\n",
      "|    total_timesteps | 40200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.81    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 808      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1004     |\n",
      "|    total_timesteps | 40400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.99    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 812      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1010     |\n",
      "|    total_timesteps | 40600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.957   |\n",
      "|    critic_loss     | 0.185    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.271    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 816      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1015     |\n",
      "|    total_timesteps | 40800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.27    |\n",
      "|    critic_loss     | 0.458    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1021     |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.2     |\n",
      "|    critic_loss     | 0.808    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.482    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 824      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1026     |\n",
      "|    total_timesteps | 41200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.79    |\n",
      "|    critic_loss     | 0.657    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 828      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1031     |\n",
      "|    total_timesteps | 41400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.76    |\n",
      "|    critic_loss     | 0.79     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 832      |\n",
      "|    fps             | 40       |\n",
      "|    time_elapsed    | 1037     |\n",
      "|    total_timesteps | 41600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.72    |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.552   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1051     |\n",
      "|    total_timesteps | 42000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.23    |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.517   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 844      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1057     |\n",
      "|    total_timesteps | 42200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.587    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.358   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 848      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1063     |\n",
      "|    total_timesteps | 42400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.59    |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42299    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 856      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1075     |\n",
      "|    total_timesteps | 42800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.4     |\n",
      "|    critic_loss     | 0.538    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1080     |\n",
      "|    total_timesteps | 43000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.78    |\n",
      "|    critic_loss     | 0.759    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 864      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1086     |\n",
      "|    total_timesteps | 43200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.28    |\n",
      "|    critic_loss     | 0.58     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 868      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1091     |\n",
      "|    total_timesteps | 43400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.26    |\n",
      "|    critic_loss     | 0.719    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.0219  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 872      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 43600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.83    |\n",
      "|    critic_loss     | 0.595    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 876      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1102     |\n",
      "|    total_timesteps | 43800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.41    |\n",
      "|    critic_loss     | 0.785    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1108     |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.47    |\n",
      "|    critic_loss     | 0.586    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 884      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1113     |\n",
      "|    total_timesteps | 44200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1     |\n",
      "|    critic_loss     | 0.811    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 0.458    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 888      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1119     |\n",
      "|    total_timesteps | 44400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.31    |\n",
      "|    critic_loss     | 0.772    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 896      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1130     |\n",
      "|    total_timesteps | 44800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.93    |\n",
      "|    critic_loss     | 0.664    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.453    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1135     |\n",
      "|    total_timesteps | 45000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.877    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 904      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1141     |\n",
      "|    total_timesteps | 45200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.696    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 908      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1147     |\n",
      "|    total_timesteps | 45400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.07    |\n",
      "|    critic_loss     | 0.869    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.643   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 912      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1152     |\n",
      "|    total_timesteps | 45600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.28    |\n",
      "|    critic_loss     | 0.337    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.954   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 916      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1158     |\n",
      "|    total_timesteps | 45800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.93    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45699    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1163     |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.21    |\n",
      "|    critic_loss     | 0.803    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.313    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 924      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1169     |\n",
      "|    total_timesteps | 46200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.59    |\n",
      "|    critic_loss     | 0.887    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.789   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41       |\n",
      "| time/              |          |\n",
      "|    episodes        | 928      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1175     |\n",
      "|    total_timesteps | 46400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.14    |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -0.0887  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 932      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1180     |\n",
      "|    total_timesteps | 46600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.5     |\n",
      "|    critic_loss     | 0.914    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.147   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 40.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 936      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1186     |\n",
      "|    total_timesteps | 46800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.48    |\n",
      "|    critic_loss     | 0.277    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -0.721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1192     |\n",
      "|    total_timesteps | 47000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.48    |\n",
      "|    critic_loss     | 0.778    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -0.125   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 944      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1197     |\n",
      "|    total_timesteps | 47200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.36    |\n",
      "|    critic_loss     | 0.936    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 948      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1203     |\n",
      "|    total_timesteps | 47400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.04    |\n",
      "|    critic_loss     | 0.782    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -0.346   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 952      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total_timesteps | 47600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.51    |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 956      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1215     |\n",
      "|    total_timesteps | 47800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.06    |\n",
      "|    critic_loss     | 0.925    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -0.0127  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1220     |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.05    |\n",
      "|    critic_loss     | 0.641    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42       |\n",
      "| time/              |          |\n",
      "|    episodes        | 964      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1226     |\n",
      "|    total_timesteps | 48200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.8     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.134   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 41.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 968      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1231     |\n",
      "|    total_timesteps | 48400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.25    |\n",
      "|    critic_loss     | 0.643    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.964   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 972      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1237     |\n",
      "|    total_timesteps | 48600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.54    |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42       |\n",
      "| time/              |          |\n",
      "|    episodes        | 976      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1243     |\n",
      "|    total_timesteps | 48800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.884    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | 0.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48699    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1248     |\n",
      "|    total_timesteps | 49000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.75    |\n",
      "|    critic_loss     | 0.53     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -0.134   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 988      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1260     |\n",
      "|    total_timesteps | 49400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.24    |\n",
      "|    critic_loss     | 0.71     |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 992      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1266     |\n",
      "|    total_timesteps | 49600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.86    |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 996      |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1271     |\n",
      "|    total_timesteps | 49800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.7     |\n",
      "|    critic_loss     | 0.803    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.448   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49699    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 42.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 1277     |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.71    |\n",
      "|    critic_loss     | 0.92     |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -0.251   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x108a137c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC, DDPG, TD3, A2C, PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], qf=[128, 128]))\n",
    "model = SAC(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 123      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 61       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 120      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.4    |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | 0.0238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66929    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 177      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 240      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.6    |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.226    |\n",
      "|    ent_coef_loss   | -0.0185  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67049    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 195      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 360      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.4    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67169    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 480      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.8    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | 0.0585   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67289    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 211      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -0.0296  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67409    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 720      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.2    |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67529    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 840      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71.6    |\n",
      "|    critic_loss     | 2.96     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | 0.0565   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67649    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 220      |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 960      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.8    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -0.0958  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67769    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 221      |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 1080     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.9    |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67889    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 222      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.9    |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | -0.00875 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68009    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 223      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 1320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.3    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.0797  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68129    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 1440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.8    |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.011   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68249    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 1560     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.2    |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | 0.0518   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68369    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 1680     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.4    |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.219    |\n",
      "|    ent_coef_loss   | -0.154   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68489    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 226      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56      |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.221    |\n",
      "|    ent_coef_loss   | 0.00245  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68609    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.round_data.redemption_values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf995d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9122b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_period(env.db, 0, 2236)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eaf961",
   "metadata": {},
   "source": [
    "### Discrete Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "env.action_space = spaces.Discrete(51)\n",
    "#policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1,)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de98f33",
   "metadata": {},
   "source": [
    "## ON POLICY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75639662",
   "metadata": {},
   "source": [
    "### DDPG - Deterministic Deep Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = DDPG(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1,)\n",
    "model.learn(50000, progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acec3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.head(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a34e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.step_data.tail(100).groupby('current_bid_idx').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfc29c",
   "metadata": {},
   "source": [
    "### PPO - Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "db = Database(game_metadata, buyer_strategies, seller_strategies)\n",
    "db.reset_round(rnd, ntokens, nbuyers, nsellers, R1, R2, R3, R4)\n",
    "env = TradingEnv(db, nsteps)\n",
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(50000, progress_bar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbafb0",
   "metadata": {},
   "source": [
    "### A2C - Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc92acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A2C model\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "a2c_model = A2C(MlpPolicy, env, verbose=0)\n",
    "\n",
    "# Train the A2C agent for 10000 steps\n",
    "a2c_model.learn(total_timesteps=training_step, progress_bar = True)\n",
    "\n",
    "# Evaluate the trained A2C agent\n",
    "mean_reward_a2c, std_reward_a2c = evaluate_policy(a2c_model, env, n_eval_episodes=eval_steps)\n",
    "print(f\"A2C mean_reward: {mean_reward_a2c:.2f} +/- {std_reward_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e53392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
